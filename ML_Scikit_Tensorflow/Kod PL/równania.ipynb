{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Równania**\n",
    "\n",
    "*Notatnik ten zawiera wszystkie równania zawarte w książce. Jeśli postanowisz wydrukować je na koszulce, też chcę jeden egzemplarz! ;-)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 1.\n",
    "**Równanie 1.1. Prosty model liniowe**\n",
    "\n",
    "$\n",
    "\\text{satysfakcja_z_życia} = \\theta_0 + \\theta_1 \\times \\text{PKB_per_capita}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 2.\n",
    "**Równanie 2.1. Pierwiastek błędu średniokwadratowego (RMSE))**\n",
    "\n",
    "$\n",
    "\\text{RMSE}(\\mathbf{X}, h) = \\sqrt{\\frac{1}{m}\\sum\\limits_{i=1}^{m}\\left(h(\\mathbf{x}^{(i)}) - y^{(i)}\\right)^2}\n",
    "$\n",
    "\n",
    "\n",
    "**Notacje:**\n",
    "\n",
    "$\n",
    "  \\mathbf{x}^{(1)} = \\begin{pmatrix}\n",
    "  -118.29 \\\\\n",
    "  33.91 \\\\\n",
    "  1,416 \\\\\n",
    "  38,372\n",
    "  \\end{pmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "  y^{(1)}=156,400\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "  \\mathbf{X} = \\begin{pmatrix}\n",
    "  (\\mathbf{x}^{(1)})^T \\\\\n",
    "  (\\mathbf{x}^{(2)})^T\\\\\n",
    "  \\vdots \\\\\n",
    "  (\\mathbf{x}^{(1999)})^T \\\\\n",
    "  (\\mathbf{x}^{(2000)})^T\n",
    "  \\end{pmatrix} = \\begin{pmatrix}\n",
    "  -118.29 & 33.91 & 1,416 & 38,372 \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "  \\end{pmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 2.2. Średni absolutny błąd**\n",
    "\n",
    "$\n",
    "\\text{MAE}(\\mathbf{X}, h) = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\left| h(\\mathbf{x}^{(i)}) - y^{(i)} \\right|\n",
    "$\n",
    "\n",
    "**normy $\\ell_k$:**\n",
    "\n",
    "$ \\left\\| \\mathbf{v} \\right\\| _k = (\\left| v_0 \\right|^k + \\left| v_1 \\right|^k + \\dots + \\left| v_n \\right|^k)^{\\frac{1}{k}} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 3.\n",
    "**Równanie 3.1. Precyzja**\n",
    "\n",
    "$\n",
    "\\text{precyzja} = \\cfrac{PP}{PP + FP}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 3.2. Pełność**\n",
    "\n",
    "$\n",
    "\\text{pełność} = \\cfrac{PP}{PP + FN}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 3.3. Wynik $F_1$**\n",
    "\n",
    "$\n",
    "F_1 = \\cfrac{2}{\\cfrac{1}{\\text{preccyzja}} + \\cfrac{1}{\\text{pełność}}} = 2 \\times \\cfrac{\\text{precyzja}\\, \\times \\, \\text{pełność}}{\\text{precyzja}\\, + \\, \\text{pełność}} = \\cfrac{PP}{PP + \\cfrac{FN + FP}{2}}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 4.\n",
    "**Równanie 4.1. Predykcja za pomocą modelu regresji liniowej**\n",
    "\n",
    "$\n",
    "\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.2. Predykcja za pomocą modelu regresji liniowej (postać wektorowa)**\n",
    "\n",
    "$\n",
    "\\hat{y} = h_{\\mathbf{\\theta}}(\\mathbf{x}) = \\mathbf{\\theta}^T \\cdot \\mathbf{x}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.3. Funkcja kosztu MSE dla modelu regresji liniowej**\n",
    "\n",
    "$\n",
    "\\text{MSE}(\\mathbf{X}, h_{\\mathbf{\\theta}}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{(\\mathbf{\\theta}^T \\cdot \\mathbf{x}^{(i)} - y^{(i)})^2}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.4.Równanie normalne**\n",
    "\n",
    "$\n",
    "\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^T \\cdot \\mathbf{X})^{-1} \\cdot \\mathbf{X}^T \\cdot \\mathbf{y}\n",
    "$\n",
    "\n",
    "\n",
    "** Notacja pochodnych cząstkowych:**\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_j} \\text{MSE}(\\mathbf{\\theta})$\n",
    "\n",
    "\n",
    "**Równanie 4.5. Wzór na pochodne cząstkowe funkcji kosztu**\n",
    "\n",
    "$\n",
    "\\dfrac{\\partial}{\\partial \\theta_j} \\text{MSE}(\\mathbf{\\theta}) = \\dfrac{2}{m}\\sum\\limits_{i=1}^{m}(\\mathbf{\\theta}^T \\cdot \\mathbf{x}^{(i)} - y^{(i)})\\, x_j^{(i)}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.6. Wektor gradientów funkcji kosztu**\n",
    "\n",
    "$\n",
    "\\nabla_{\\mathbf{\\theta}}\\, \\text{MSE}(\\mathbf{\\theta}) =\n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial}{\\partial \\theta_0} \\text{MSE}(\\mathbf{\\theta}) \\\\\n",
    " \\frac{\\partial}{\\partial \\theta_1} \\text{MSE}(\\mathbf{\\theta}) \\\\\n",
    " \\vdots \\\\\n",
    " \\frac{\\partial}{\\partial \\theta_n} \\text{MSE}(\\mathbf{\\theta})\n",
    "\\end{pmatrix}\n",
    " = \\dfrac{2}{m} \\mathbf{X}^T \\cdot (\\mathbf{X} \\cdot \\mathbf{\\theta} - \\mathbf{y})\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.7. Określanie kroku gradientu prostego**\n",
    "\n",
    "$\n",
    "\\mathbf{\\theta}^{(\\text{kolejny krok})} = \\mathbf{\\theta} - \\eta \\nabla_{\\mathbf{\\theta}}\\, \\text{MSE}(\\mathbf{\\theta})\n",
    "$\n",
    "\n",
    "\n",
    "$ O(\\frac{1}{\\text{przebiegi}}) $\n",
    "\n",
    "\n",
    "$ \\hat{y} = 0.56 x_1^2 + 0.93 x_1 + 1.78 $\n",
    "\n",
    "\n",
    "$ y = 0.5 x_1^2 + 1.0 x_1 + 2.0 + \\text{szum gaussowski} $\n",
    "\n",
    "\n",
    "$ \\dfrac{(n+d)!}{d!\\,n!} $\n",
    "\n",
    "\n",
    "$ \\alpha \\sum_{i=1}^{n}{\\theta_i^2}$\n",
    "\n",
    "\n",
    "**Równanie 4.8. Funkcja kosztu regresji grzbietowej**\n",
    "\n",
    "$\n",
    "J(\\mathbf{\\theta}) = \\text{MSE}(\\mathbf{\\theta}) + \\alpha \\dfrac{1}{2}\\sum\\limits_{i=1}^{n}\\theta_i^2\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.9. Jawny wzór regresji grzbietowej**\n",
    "\n",
    "$\n",
    "\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^T \\cdot \\mathbf{X} + \\alpha \\mathbf{A})^{-1} \\cdot \\mathbf{X}^T \\cdot \\mathbf{y}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.10. Funkcja kosztu regresji metodą LASSO**\n",
    "\n",
    "$\n",
    "J(\\mathbf{\\theta}) = \\text{MSE}(\\mathbf{\\theta}) + \\alpha \\sum\\limits_{i=1}^{n}\\left| \\theta_i \\right|\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.11. Wektor podgradientów w regresji metodą LASSO**\n",
    "\n",
    "$\n",
    "g(\\mathbf{\\theta}, J) = \\nabla_{\\mathbf{\\theta}}\\, \\text{MSE}(\\mathbf{\\theta}) + \\alpha\n",
    "\\begin{pmatrix}\n",
    "  \\operatorname{sign}(\\theta_1) \\\\\n",
    "  \\operatorname{sign}(\\theta_2) \\\\\n",
    "  \\vdots \\\\\n",
    "  \\operatorname{sign}(\\theta_n) \\\\\n",
    "\\end{pmatrix} \\quad \\text{gdzie } \\operatorname{sign}(\\theta_i) =\n",
    "\\begin{cases}\n",
    "-1 & \\text{if } \\theta_i < 0 \\\\\n",
    "0 & \\text{if } \\theta_i = 0 \\\\\n",
    "+1 & \\text{if } \\theta_i > 0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.12. Funkcja kosztu metody elastycznej siatki**\n",
    "\n",
    "$\n",
    "J(\\mathbf{\\theta}) = \\text{MSE}(\\mathbf{\\theta}) + r \\alpha \\sum\\limits_{i=1}^{n}\\left| \\theta_i \\right| + \\dfrac{1 - r}{2} \\alpha \\sum\\limits_{i=1}^{n}{\\theta_i^2}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.13. Szacowane prawdopodobieństwo w modelu regresji logistycznej (postać wektorowa)**\n",
    "\n",
    "$\n",
    "\\hat{p} = h_{\\mathbf{\\theta}}(\\mathbf{x}) = \\sigma(\\mathbf{\\theta}^T \\cdot \\mathbf{x})\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.14. Funkcja logistyczna**\n",
    "\n",
    "$\n",
    "\\sigma(t) = \\dfrac{1}{1 + \\exp(-t)}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.15. Prognoza modelu regresji logistycznej**\n",
    "\n",
    "$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "  0 & \\text{jeśli } \\hat{p} < 0.5, \\\\\n",
    "  1 & \\text{jeśli } \\hat{p} \\geq 0.5.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.16. Funkcja kosztu dla pojedynczej próbki uczącej**\n",
    "\n",
    "$\n",
    "c(\\mathbf{\\theta}) =\n",
    "\\begin{cases}\n",
    "  -\\log(\\hat{p}) & \\text{jeśli } y = 1, \\\\\n",
    "  -\\log(1 - \\hat{p}) & \\text{jeśli } y = 0.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.17. Funkcja kosztu regresji logistycznej (logarytmiczna funkcja straty)**\n",
    "\n",
    "$\n",
    "J(\\mathbf{\\theta}) = -\\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{\\left[ y^{(i)} log\\left(\\hat{p}^{(i)}\\right) + (1 - y^{(i)}) log\\left(1 - \\hat{p}^{(i)}\\right)\\right]}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.18. Pochodne cząstkowe logistycznej funkcji kosztu**\n",
    "\n",
    "$\n",
    "\\dfrac{\\partial}{\\partial \\theta_j} \\text{J}(\\mathbf{\\theta}) = \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\left(\\mathbf{\\sigma(\\theta}^T \\cdot \\mathbf{x}^{(i)}) - y^{(i)}\\right)\\, x_j^{(i)}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.19. Wynik funkcji softmax dla klasy k**\n",
    "\n",
    "$\n",
    "s_k(\\mathbf{x}) = ({\\mathbf{\\theta}^{(k)}})^T \\cdot \\mathbf{x}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.20. Funkcja softmax**\n",
    "\n",
    "$\n",
    "\\hat{p}_k = \\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.21. Prognoza klasyfikatora regresji softmax**\n",
    "\n",
    "$\n",
    "\\hat{y} = \\underset{k}{\\operatorname{argmax}} \\, \\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\underset{k}{\\operatorname{argmax}} \\, s_k(\\mathbf{x}) = \\underset{k}{\\operatorname{argmax}} \\, \\left( ({\\mathbf{\\theta}^{(k)}})^T \\cdot \\mathbf{x} \\right)\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 4.22. Funkcja kosztu — entropia krzyżowa**\n",
    "\n",
    "$\n",
    "J(\\mathbf{\\Theta}) = - \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}\n",
    "$\n",
    "\n",
    "**Entropia krzyżowa pomiędzy dwoma rozkładami prawdopodobieństwa  $p$ i $q$:**\n",
    "$ H(p, q) = -\\sum\\limits_{x}p(x) \\log q(x) $\n",
    "\n",
    "\n",
    "**Równanie 4.23. Wektor gradientów entropii krzyżowej dla klasy k**\n",
    "\n",
    "$\n",
    "\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 5.\n",
    "**Równanie 5.1. Gaussowska funkcja RBF**\n",
    "\n",
    "$\n",
    "{\\displaystyle \\phi_{\\gamma}(\\mathbf{x}, \\mathbf{\\ell})} = {\\displaystyle \\exp({\\displaystyle -\\gamma \\left\\| \\mathbf{x} - \\mathbf{\\ell} \\right\\|^2})}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 5.2. Prognoza liniowego klasyfikatora SVM**\n",
    "\n",
    "$\n",
    "\\hat{y} = \\begin{cases}\n",
    " 0 & \\text{jeśli } \\mathbf{w}^T \\cdot \\mathbf{x} + b < 0, \\\\\n",
    " 1 & \\text{jeśli } \\mathbf{w}^T \\cdot \\mathbf{x} + b \\geq 0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 5.3. Cel liniowego klasyfikatora SVM wykorzystującego twardy margines**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "&\\underset{\\mathbf{w}, b}{\\operatorname{minimalizuj}}\\quad{\\frac{1}{2}\\mathbf{w}^T \\cdot \\mathbf{w}} \\\\\n",
    "&\\text{pod warunkiem, że} \\quad t^{(i)}(\\mathbf{w}^T \\cdot \\mathbf{x}^{(i)} + b) \\ge 1 \\quad \\text{dla } i = 1, 2, \\dots, m\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 5.4. Cel liniowego klasyfikatora SVM wykorzystującego miękki margines**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "&\\underset{\\mathbf{w}, b, \\mathbf{\\zeta}}{\\operatorname{minimalizuj}}\\quad{\\dfrac{1}{2}\\mathbf{w}^T \\cdot \\mathbf{w} + C \\sum\\limits_{i=1}^m{\\zeta^{(i)}}}\\\\\n",
    "&\\text{pod warunkiem, że} \\quad t^{(i)}(\\mathbf{w}^T \\cdot \\mathbf{x}^{(i)} + b) \\ge 1 - \\zeta^{(i)} \\quad \\text{i} \\quad \\zeta^{(i)} \\ge 0 \\quad \\text{dla } i = 1, 2, \\dots, m\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 5.5. Problem programowania kwadratowego**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\underset{\\mathbf{p}}{\\text{Minimalizuj}} \\quad & \\dfrac{1}{2} \\mathbf{p}^T \\cdot \\mathbf{H} \\cdot \\mathbf{p} \\quad + \\quad \\mathbf{f}^T \\cdot \\mathbf{p}  \\\\\n",
    "\\text{pod warunkiem, że} \\quad & \\mathbf{A} \\cdot \\mathbf{p} \\le \\mathbf{b} \\\\\n",
    "\\text{gdzie } &\n",
    "\\begin{cases}\n",
    "  \\mathbf{p} & \\text{ jest }n_p\\text{-wymiarowym wektorem (} n_p = \\text{liczba parametrów),}\\\\\n",
    "  \\mathbf{H} & \\text{ jest macierzą }n_p \\times n_p \\text{,}\\\\\n",
    "  \\mathbf{f} & \\text{ jest }n_p\\text{-wymiarowym wektorem,}\\\\\n",
    "  \\mathbf{A} & \\text{ jest macierzą } n_c \\times n_p \\text{(}n_c = \\text{liczba ograniczeń),}\\\\\n",
    "  \\mathbf{b} & \\text{ jest }n_c\\text{-wymiarowym wektorem.}\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 5.6. Postać dualna celu liniowej maszyny SVM**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\underset{\\mathbf{\\alpha}}{\\operatorname{minimalizuj}}\n",
    "\\dfrac{1}{2}\\sum\\limits_{i=1}^{m}{\n",
    "  \\sum\\limits_{j=1}^{m}{\n",
    "  \\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} {\\mathbf{x}^{(i)}}^T \\cdot \\mathbf{x}^{(j)}\n",
    "  }\n",
    "} \\quad - \\quad \\sum\\limits_{i=1}^{m}{\\alpha^{(i)}}\\\\\n",
    "\\text{pod warunkiem, że}\\quad \\alpha^{(i)} \\ge 0 \\quad \\text{dla }i = 1, 2, \\dots, m\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 5.7. Przejście z problemu dualnego do pierwotnego**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "&\\hat{\\mathbf{w}} = \\sum_{i=1}^{m}{\\hat{\\alpha}}^{(i)}t^{(i)}\\mathbf{x}^{(i)}\\\\\n",
    "&\\hat{b} = \\dfrac{1}{n_s}\\sum\\limits_{\\scriptstyle i=1 \\atop {\\scriptstyle {\\hat{\\alpha}}^{(i)} > 0}}^{m}{\\left(1 - t^{(i)}({\\hat{\\mathbf{w}}}^T \\cdot \\mathbf{x}^{(i)})\\right)}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 5.8. Odwzorowanie wielomianowe drugiego stopnia**\n",
    "\n",
    "$\n",
    "\\phi\\left(\\mathbf{x}\\right) = \\phi\\left( \\begin{pmatrix}\n",
    "  x_1 \\\\\n",
    "  x_2\n",
    "\\end{pmatrix} \\right) = \\begin{pmatrix}\n",
    "  {x_1}^2 \\\\\n",
    "  \\sqrt{2} \\, x_1 x_2 \\\\\n",
    "  {x_2}^2\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 5.9. Sztuczka z jądrem dla odwzorowania wielomianowego drugiego stopnia**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\phi(\\mathbf{a})^T \\cdot \\phi(\\mathbf{b}) & \\quad = \\begin{pmatrix}\n",
    "  {a_1}^2 \\\\\n",
    "  \\sqrt{2} \\, a_1 a_2 \\\\\n",
    "  {a_2}^2\n",
    "  \\end{pmatrix}^T \\cdot \\begin{pmatrix}\n",
    "  {b_1}^2 \\\\\n",
    "  \\sqrt{2} \\, b_1 b_2 \\\\\n",
    "  {b_2}^2\n",
    "\\end{pmatrix} = {a_1}^2 {b_1}^2 + 2 a_1 b_1 a_2 b_2 + {a_2}^2 {b_2}^2 \\\\\n",
    " & \\quad = \\left( a_1 b_1 + a_2 b_2 \\right)^2 = \\left( \\begin{pmatrix}\n",
    "  a_1 \\\\\n",
    "  a_2\n",
    "\\end{pmatrix}^T \\cdot \\begin{pmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2\n",
    "  \\end{pmatrix} \\right)^2 = (\\mathbf{a}^T \\cdot \\mathbf{b})^2\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**W tekście omawiającym sztuczkę z jądrem:**\n",
    "[...], to możemy zastąpić iloczyn skalarny tych przekształconych wektorów wyrażeniem $ ({\\mathbf{x}^{(i)}}^T \\cdot \\mathbf{x}^{(j)})^2 $\n",
    "\n",
    "\n",
    "**Równanie 5.10. Najpopularniejsze jądra**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\text{Liniowe:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\mathbf{a}^T \\cdot \\mathbf{b} \\\\\n",
    "\\text{Wielomianowe:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\left(\\gamma \\mathbf{a}^T \\cdot \\mathbf{b} + r \\right)^d \\\\\n",
    "\\text{Gaussowskie RBF:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\exp({\\displaystyle -\\gamma \\left\\| \\mathbf{a} - \\mathbf{b} \\right\\|^2}) \\\\\n",
    "\\text{Sigmoidalne:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\tanh\\left(\\gamma \\mathbf{a}^T \\cdot \\mathbf{b} + r\\right)\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**Równanie 5.11. Wyliczanie prognoz za pomocą kernelizowanej maszyny SVM**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "h_{\\hat{\\mathbf{w}}, \\hat{b}}\\left(\\phi(\\mathbf{x}^{(n)})\\right) & = \\,\\hat{\\mathbf{w}}^T \\cdot \\phi(\\mathbf{x}^{(n)}) + \\hat{b} = \\left(\\sum_{i=1}^{m}{\\hat{\\alpha}}^{(i)}t^{(i)}\\phi(\\mathbf{x}^{(i)})\\right)^T \\cdot \\phi(\\mathbf{x}^{(n)}) + \\hat{b}\\\\\n",
    " & = \\, \\sum_{i=1}^{m}{\\hat{\\alpha}}^{(i)}t^{(i)}\\left(\\phi(\\mathbf{x}^{(i)})^T \\cdot \\phi(\\mathbf{x}^{(n)})\\right)  + \\hat{b}\\\\\n",
    " & = \\sum\\limits_{\\scriptstyle i=1 \\atop {\\scriptstyle {\\hat{\\alpha}}^{(i)} > 0}}^{m}{\\hat{\\alpha}}^{(i)}t^{(i)} K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(n)}) + \\hat{b}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 5.12. Obliczanie punktu obciążenia za pomocą sztuczki z jądrem**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\hat{b} & = \\dfrac{1}{n_s}\\sum\\limits_{\\scriptstyle i=1 \\atop {\\scriptstyle {\\hat{\\alpha}}^{(i)} > 0}}^{m}{\\left(1 - t^{(i)}{\\hat{\\mathbf{w}}}^T \\cdot \\phi(\\mathbf{x}^{(i)})\\right)} = \\dfrac{1}{n_s}\\sum\\limits_{\\scriptstyle i=1 \\atop {\\scriptstyle {\\hat{\\alpha}}^{(i)} > 0}}^{m}{\\left(1 - t^{(i)}{\n",
    " \\left(\\sum_{j=1}^{m}{\\hat{\\alpha}}^{(j)}t^{(j)}\\phi(\\mathbf{x}^{(j)})\\right)\n",
    " }^T \\cdot \\phi(\\mathbf{x}^{(i)})\\right)}\\\\\n",
    " & = \\dfrac{1}{n_s}\\sum\\limits_{\\scriptstyle i=1 \\atop {\\scriptstyle {\\hat{\\alpha}}^{(i)} > 0}}^{m}{\\left(1 - t^{(i)}\n",
    "\\sum\\limits_{\\scriptstyle j=1 \\atop {\\scriptstyle {\\hat{\\alpha}}^{(j)} > 0}}^{m}{\n",
    "  {\\hat{\\alpha}}^{(j)} t^{(j)} K(\\mathbf{x}^{(i)},\\mathbf{x}^{(j)})\n",
    "}\n",
    "\\right)}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 5.13. Funkcja kosztu liniowego klasyfikatora SVM**\n",
    "\n",
    "$\n",
    "J(\\mathbf{w}, b) = \\dfrac{1}{2} \\mathbf{w}^T \\cdot \\mathbf{w} \\quad + \\quad C {\\displaystyle \\sum\\limits_{i=1}^{m}max\\left(0, 1 - t^{(i)}(\\mathbf{w}^T \\cdot \\mathbf{x}^{(i)} + b) \\right)}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 6.\n",
    "**Równanie 6.1. Wskaźnik Giniego**\n",
    "\n",
    "$\n",
    "G_i = 1 - \\sum\\limits_{k=1}^{n}{{p_{i,k}}^2}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 6.2. Funkcja kosztu algorytmu CART używana w zadaniach klasyfikacji**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "&J(k, t_k) = \\dfrac{m_{\\text{lewy}}}{m}G_\\text{lewy} + \\dfrac{m_{\\text{prawy}}}{m}G_{\\text{prawy}}\\\\\n",
    "&\\text{gdzie }\\begin{cases}\n",
    "G_\\text{lewy/prawy} \\text{ — miara zanieczyszczenia lewego/prawego podzbioru,}\\\\\n",
    "m_\\text{lewy/prawy} \\text{ — liczba próbek w lewym/prawym podzbiorze.}\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**Przykład obliczania entropii:**\n",
    "\n",
    "$ -\\frac{49}{54}\\log(\\frac{49}{54}) - \\frac{5}{54}\\log(\\frac{5}{54}) $\n",
    "\n",
    "\n",
    "**Równanie 6.3. Entropia**\n",
    "\n",
    "$\n",
    "H_i = -\\sum\\limits_{k=1 \\atop p_{i,k} \\ne 0}^{n}{{p_{i,k}}\\log(p_{i,k})}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 6.4. Funkcja kosztu CART stosowana w regresji**\n",
    "\n",
    "$\n",
    "J(k, t_k) = \\dfrac{m_{\\text{lewy}}}{m}\\text{MSE}_\\text{lewy} + \\dfrac{m_{\\text{prawy}}}{m}\\text{MSE}_{\\text{prawy}} \\quad\n",
    "\\text{gdzie }\n",
    "\\begin{cases}\n",
    "\\text{MSE}_{\\text{węzeł}} = \\sum\\limits_{\\scriptstyle i \\in \\text{węzeł}}(\\hat{y}_{\\text{węzeł}} - y^{(i)})^2\\\\\n",
    "\\hat{y}_\\text{węzeł} = \\dfrac{1}{m_{\\text{węzeł}}}\\sum\\limits_{\\scriptstyle i \\in \\text{węzeł}}y^{(i)}\n",
    "\\end{cases}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 7.\n",
    "\n",
    "**Równanie 7.1. Ważony współczynnik błędu dla j-tego predyktora**\n",
    "\n",
    "$\n",
    "r_j = \\dfrac{\\displaystyle \\sum\\limits_{\\textstyle {i=1 \\atop \\hat{y}_j^{(i)} \\ne y^{(i)}}}^{m}{w^{(i)}}}{\\displaystyle \\sum\\limits_{i=1}^{m}{w^{(i)}}} \\quad\n",
    "\\text{gdzie }\\hat{y}_j^{(i)}\\text{ jest prognozą }j^{\\text{tego}}\\text{ predyktora dla }i^{\\text{tej}}\\text{ próbki.}\n",
    "$\n",
    "\n",
    "**Równanie 7.2. Waga predyktora**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\alpha_j = \\eta \\log{\\dfrac{1 - r_j}{r_j}}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 7.3. Reguła aktualizowania wag**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "& \\text{ for } i = 1, 2, \\dots, m \\\\\n",
    "& w^{(i)} \\leftarrow\n",
    "\\begin{cases}\n",
    "w^{(i)} & \\text{jeśli }\\hat{y_j}^{(i)} = y^{(i)}\\\\\n",
    "w^{(i)} \\exp(\\alpha_j) & \\text{jeśli }\\hat{y_j}^{(i)} \\ne y^{(i)}\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**W tekście:**\n",
    "\n",
    "Teraz wszystkie wagi próbek zostają znormalizowane (tj. podzielone przez $ \\sum_{i=1}^{m}{w^{(i)}} $).\n",
    "\n",
    "\n",
    "**Równanie 7.4. Prognozy algorytmu AdaBoost**\n",
    "\n",
    "$\n",
    "\\hat{y}(\\mathbf{x}) = \\underset{k}{\\operatorname{argmax}}{\\sum\\limits_{\\scriptstyle j=1 \\atop \\scriptstyle \\hat{y}_j(\\mathbf{x}) = k}^{N}{\\alpha_j}} \\quad \\text{gdzie }N\\text{ oznacza liczbę predyktorów.}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 8.\n",
    "\n",
    "**Równanie 8.1. Macierz głównych składowych**\n",
    "\n",
    "$\n",
    "\\mathbf{V}^T =\n",
    "\\begin{pmatrix}\n",
    "  \\mid & \\mid & & \\mid \\\\\n",
    "  \\mathbf{c_1} & \\mathbf{c_2} & \\cdots & \\mathbf{c_n} \\\\\n",
    "  \\mid & \\mid & & \\mid\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 8.2. Rzutowanie zestawu danych uczących na d wymiarów**\n",
    "\n",
    "$\n",
    "\\mathbf{X}_{d\\text{-rzut.}} = \\mathbf{X} \\cdot \\mathbf{W}_d\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 8.3. Odwrotna transformacja PCA przywracająca pierwotną liczbę wymiarów**\n",
    "\n",
    "$\n",
    "\\mathbf{X}_{\\text{zrekonstruowany}} = \\mathbf{X}_{d\\text{-wym}} \\cdot {\\mathbf{W}_d}^T\n",
    "$\n",
    "\n",
    "\n",
    "$ \\sum_{j=1}^{m}{w_{i,j}\\mathbf{x}^{(j)}} $\n",
    "\n",
    "\n",
    "**Równanie 8.4. Etap pierwszy algorytmu LLE: liniowe modelowanie lokalnych relacji**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "& \\hat{\\mathbf{W}} = \\underset{\\mathbf{W}}{\\operatorname{argmin}}{\\displaystyle \\sum\\limits_{i=1}^{m}} \\left\\|\\mathbf{x}^{(i)} - \\sum\\limits_{j=1}^{m}{w_{i,j}}\\mathbf{x}^{(j)}\\right\\|^2\\\\\n",
    "& \\text{pod warunkiem, że  }\n",
    "\\begin{cases}\n",
    "  w_{i,j}=0 & \\text{jeśli }\\mathbf{x}^{(j)} \\text{ nie jest jednym z }k\\text{ najbliższych sąsiadów }\\mathbf{x}^{(i)}\\\\\n",
    "  \\sum\\limits_{j=1}^{m}w_{i,j} = 1 & \\text{dla }i=1, 2, \\dots, m\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**W tekście**\n",
    "\n",
    "[...] to chcemy, aby kwadrat odległości pomiędzy $\\mathbf{z}^{(i)}$ a $ \\sum_{j=1}^{m}{\\hat{w}_{i,j}\\mathbf{z}^{(j)}} $ był jak najmniejszy.\n",
    "\n",
    "\n",
    "**Równanie 8.5. Etap drugi algorytmu LLE: redukowanie wymiarowości przy jednoczesnym zachowaniu relacji**\n",
    "\n",
    "$\n",
    "\\hat{\\mathbf{Z}} = \\underset{\\mathbf{Z}}{\\operatorname{argmin}}{\\displaystyle \\sum\\limits_{i=1}^{m}} \\left\\|\\mathbf{z}^{(i)} - \\sum\\limits_{j=1}^{m}{\\hat{w}_{i,j}}\\mathbf{z}^{(j)}\\right\\|^2\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 9.\n",
    "\n",
    "**Równanie 9.1. Prostowana jednostka liniowa**\n",
    "\n",
    "$\n",
    "h_{\\mathbf{w}, b}(\\mathbf{X}) = \\max(\\mathbf{X} \\cdot \\mathbf{w} + b, 0)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 10.\n",
    "\n",
    "**Równanie 10.1. Najpowszechniejsze funkcje skokowe wykorzystywane w perceptronach**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\operatorname{heaviside}(z) =\n",
    "\\begin{cases}\n",
    "0 & \\text{jeśli }z < 0\\\\\n",
    "1 & \\text{jeśli }z \\ge 0\n",
    "\\end{cases} & \\quad\\quad\n",
    "\\operatorname{sgn}(z) =\n",
    "\\begin{cases}\n",
    "-1 & \\text{jeśli }z < 0\\\\\n",
    "0 & \\text{jeśli }z = 0\\\\\n",
    "+1 & \\text{jeśli }z > 0\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 10.2. Reguła uczenia perceptronu (aktualizowanie wag)**\n",
    "\n",
    "$\n",
    "{w_{i,j}}^{(\\text{kolejny krok})} = w_{i,j} + \\eta (y_j - \\hat{y}_j) x_i\n",
    "$\n",
    "\n",
    "\n",
    "**W tekście**\n",
    "\n",
    "Będziemy ją inicjować losowo za pomocą rozkładu normalnego (gaussowskiego) uciętego  z odchyleniem standardowym $ 2 / \\sqrt{\\text{n}_\\text{wejścia}} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 11.\n",
    "**Równanie 11.1. Inicjacja Xaviera (podczas stosowania logistycznej funkcji aktywacji)**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "& \\text{Rozkład normalny ze średnią o wartości 0 i odchyleniem standardowym }\n",
    "\\sigma = \\sqrt{\\dfrac{2}{n_\\text{wejścia} + n_\\text{wyjścia}}}\\\\\n",
    "& \\text{Lub rozkład jednorodny pomiędzy –r i r, gdzie }\n",
    "r = \\sqrt{\\dfrac{6}{n_\\text{wejścia} + n_\\text{wyjścia}}}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**W tekście**\n",
    "\n",
    "Gdy liczba wejść jest w przybliżeniu taka sama, jak liczba wyjść, wzory te ulegają uproszczeniu (np. $ \\sigma = 1 / \\sqrt{n_\\text{wejścia}} $ lub $ r = \\sqrt{3} / \\sqrt{n_\\text{wejścia}} $).\n",
    "\n",
    "**Tabela 11.1. Parametry inicjujące dla różnych funkcji aktywacji**\n",
    "\n",
    "* Logistyczna jednorodna: $ r = \\sqrt{\\dfrac{6}{n_\\text{wejścia} + n_\\text{wyjścia}}} $\n",
    "* Logisticzna normalna: $ \\sigma = \\sqrt{\\dfrac{2}{n_\\text{wejścia} + n_\\text{wyjścia}}} $\n",
    "* Tangensa hiperbolicznego jednorodna: $ r = 4 \\sqrt{\\dfrac{6}{n_\\text{wejścia} + n_\\text{wyjścia}}} $\n",
    "* Tangensa hiperbolicznego normalna: $ \\sigma = 4 \\sqrt{\\dfrac{2}{n_\\text{wejścia} + n_\\text{wyjścia}}} $\n",
    "* ReLU (i jej odmiany) jednorodna: $ r = \\sqrt{2} \\sqrt{\\dfrac{6}{n_\\text{wejścia} + n_\\text{wyjścia}}} $\n",
    "* ReLU (i jej odmiany) normalna: $ \\sigma = \\sqrt{2} \\sqrt{\\dfrac{2}{n_\\text{wejścia} + n_\\text{wyjścia}}} $\n",
    "\n",
    "**Równanie 11.2. Funkcja aktywacji ELU**\n",
    "\n",
    "$\n",
    "\\operatorname{ELU}_\\alpha(z) =\n",
    "\\begin{cases}\n",
    "\\alpha(\\exp(z) - 1) & \\text{jeśli } z < 0\\\\\n",
    "z & \\text{jeśli } z \\ge 0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 11.3. Algorytm normalizacji wsadowej**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "1.\\quad & \\mathbf{\\mu}_B = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{\\mathbf{x}^{(i)}}\\\\\n",
    "2.\\quad & {\\mathbf{\\sigma}_B}^2 = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{(\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B)^2}\\\\\n",
    "3.\\quad & \\hat{\\mathbf{x}}^{(i)} = \\dfrac{\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B}{\\sqrt{{\\mathbf{\\sigma}_B}^2 + \\epsilon}}\\\\\n",
    "4.\\quad & \\mathbf{z}^{(i)} = \\gamma \\hat{\\mathbf{x}}^{(i)} + \\beta\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**W tekście**\n",
    "\n",
    "[...] przy danej nowej wartości $v$, średnia ruchoma $v$ jest aktualizowana za pomocą wzoru:\n",
    "\n",
    "$ \\hat{v} \\gets \\hat{v} \\times \\text{moment} + v \\times (1 - \\text{moment}) $\n",
    "\n",
    "**Równanie 11.4. Algorytm optymalizacji momentowej**\n",
    "\n",
    "1. $\\mathbf{m} \\gets \\beta \\mathbf{m} - \\eta \\nabla_\\mathbf{\\theta}J(\\mathbf{\\theta})$\n",
    "2. $\\mathbf{\\theta} \\gets \\mathbf{\\theta} + \\mathbf{m}$\n",
    "\n",
    "**W tekście**\n",
    "\n",
    "Możemy łatwo sprawdzić, że przy stałym gradiencie prędkość graniczna (tj. maksymalny rozmiar aktualizacji wag) będzie równa iloczynowi tego gradientu, współczynnika uczenia i wyrażenia $ \\frac{1}{1 - \\beta} $.\n",
    "\n",
    "\n",
    "**Równanie 11.5. Algorytm przyśpieszonego spadku wzdłuż gradientu**\n",
    "\n",
    "1. $\\mathbf{m} \\gets \\beta \\mathbf{m} - \\eta \\nabla_\\mathbf{\\theta}J(\\mathbf{\\theta} + \\beta \\mathbf{m})$\n",
    "2. $\\mathbf{\\theta} \\gets \\mathbf{\\theta} + \\mathbf{m}$\n",
    "\n",
    "**Równanie 11.6. Algorytm AdaGrad**\n",
    "\n",
    "1. $\\mathbf{s} \\gets \\mathbf{s} + \\nabla_\\mathbf{\\theta}J(\\mathbf{\\theta}) \\otimes \\nabla_\\mathbf{\\theta}J(\\mathbf{\\theta})$\n",
    "2. $\\mathbf{\\theta} \\gets \\mathbf{\\theta} - \\eta \\, \\nabla_\\mathbf{\\theta}J(\\mathbf{\\theta}) \\oslash {\\sqrt{\\mathbf{s} + \\epsilon}}$\n",
    "\n",
    "**W tekście:**\n",
    "\n",
    "Ta zwektoryzowana postać jest równoważna obliczeniu $s_i \\gets s_i + \\left( \\dfrac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_i} \\right)^2$ dla każdego elementu $s_i$ w wektorze $\\mathbf{s}$.\n",
    "\n",
    "**W tekście:**\n",
    "\n",
    "Ta zwektoryzowana forma jest równoznaczna operacji $ \\theta_i \\gets \\theta_i - \\eta \\, \\dfrac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_i} \\dfrac{1}{\\sqrt{s_i + \\epsilon}} $ dla wszystkich parametrów $\\theta_i$ (równocześnie).\n",
    "\n",
    "\n",
    "**Równanie 11.7. Algorytm RMSProp**\n",
    "\n",
    "1. $\\mathbf{s} \\gets \\beta \\mathbf{s} + (1 - \\beta ) \\nabla_\\mathbf{\\theta}J(\\mathbf{\\theta}) \\otimes \\nabla_\\mathbf{\\theta}J(\\mathbf{\\theta})$\n",
    "2. $\\mathbf{\\theta} \\gets \\mathbf{\\theta} - \\eta \\, \\nabla_\\mathbf{\\theta}J(\\mathbf{\\theta}) \\oslash {\\sqrt{\\mathbf{s} + \\epsilon}}$\n",
    "\n",
    "\n",
    "**Równanie 11.8. Algorytm Adam**\n",
    "\n",
    "1. $\\mathbf{m} \\gets \\beta_1 \\mathbf{m} - (1 - \\beta_1) \\nabla_\\mathbf{\\theta}J(\\mathbf{\\theta})$\n",
    "2. $\\mathbf{s} \\gets \\beta_2 \\mathbf{s} + (1 - \\beta_2) \\nabla_\\mathbf{\\theta}J(\\mathbf{\\theta}) \\otimes \\nabla_\\mathbf{\\theta}J(\\mathbf{\\theta})$\n",
    "3. $\\mathbf{m} \\gets \\left(\\dfrac{\\mathbf{m}}{1 - {\\beta_1}^T}\\right)$\n",
    "4. $\\mathbf{s} \\gets \\left(\\dfrac{\\mathbf{s}}{1 - {\\beta_2}^T}\\right)$\n",
    "5. $\\mathbf{\\theta} \\gets \\mathbf{\\theta} + \\eta \\, \\mathbf{m} \\oslash {\\sqrt{\\mathbf{s} + \\epsilon}}$\n",
    "\n",
    "**W tekście:**\n",
    "\n",
    "Zazwyczaj implementujemy to ograniczenie wyliczając $\\left\\| \\mathbf{w} \\right\\|_2$ po każdym przebiegu uczenia i w razie potrzeby przycinając wartość $\\mathbf{w}$ $ \\left( \\mathbf{w} \\gets \\mathbf{w} \\dfrac{r}{\\left\\| \\mathbf{w} \\right\\|_2} \\right) $.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 13.\n",
    "\n",
    "**Równanie 13.1. Obliczanie wartości wyjściowej neuronu w warstwie splotowej**\n",
    "\n",
    "$\n",
    "z_{i,j,k} = b_k + \\sum\\limits_{u = 0}^{f_h - 1} \\, \\, \\sum\\limits_{v = 0}^{f_w - 1} \\, \\, \\sum\\limits_{k' = 0}^{f_{n'} - 1} \\, \\, x_{i', j', k'} . w_{u, v, k', k}\n",
    "\\quad \\text{gdzie }\n",
    "\\begin{cases}\n",
    "i' = i \\times s_h + u \\\\\n",
    "j' = j \\times s_w + v\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "**Równanie 13.2. Normalizacja odpowiedzi lokalnej**\n",
    "\n",
    "$\n",
    "b_i = a_i  \\left(k + \\alpha \\sum\\limits_{j=j_\\text{niska}}^{j_\\text{wysoka}}{{a_j}^2} \\right)^{-\\beta} \\quad \\text{gdzie }\n",
    "\\begin{cases}\n",
    "  j_\\text{wysoka} = \\min\\left(i + \\dfrac{r}{2}, f_n-1\\right) \\\\\n",
    "  j_\\text{niska} = \\max\\left(0, i - \\dfrac{r}{2}\\right)\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 14.\n",
    "\n",
    "**Równanie 14.1. Wyniki warstwy neuronów rekurencyjnych dla pojedynczej próbki**\n",
    "\n",
    "$\n",
    "\\mathbf{y}_{(t)} = \\phi\\left({{\\mathbf{x}_{(t)}}^T \\cdot \\mathbf{w}_x} + {\\mathbf{y}_{(t-1)}}^T \\cdot {\\mathbf{w}_y} + b \\right)\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 14.2. Wyniki warstwy neuronów rekurencyjnych dla wszystkich próbek tworzących mini-grupę**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\mathbf{Y}_{(t)} & = \\phi\\left(\\mathbf{X}_{(t)} \\cdot \\mathbf{W}_{x} + \\mathbf{Y}_{(t-1)}\\cdot  \\mathbf{W}_{y} + \\mathbf{b} \\right) \\\\\n",
    "& = \\phi\\left(\n",
    "\\left[\\mathbf{X}_{(t)} \\quad \\mathbf{Y}_{(t-1)} \\right]\n",
    " \\cdot \\mathbf{W} + \\mathbf{b} \\right) \\text{ gdzie } \\mathbf{W}=\n",
    "\\left[ \\begin{matrix}\n",
    "  \\mathbf{W}_x\\\\\n",
    "  \\mathbf{W}_y\n",
    "\\end{matrix} \\right]\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**W tekście:**\n",
    "\n",
    "Podobnie jak w przypadku klasycznej propagacji wstecznej, najpierw wykonywany jest przebieg do przodu poprzez rozwiniętą sieć (przerywane strzałki); następnie sekwencja wyjściowa jest oceniana za pomocą funkcji kosztu $ C(\\mathbf{Y}_{(t_\\text{min})}, \\mathbf{Y}_{(t_\\text{min}+1)}, \\dots, \\mathbf{Y}_{(t_\\text{max})}) $ (gdzie $t_\\text{min}$ i $t_\\text{max}$ są, odpowiednio, pierwszym i ostatnim taktem, nie licząc ignorowanych wyjść)[...]\n",
    "\n",
    "\n",
    "**Równanie 14.3. Obliczenia komórki LSTM**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\mathbf{i}_{(t)}&=\\sigma({\\mathbf{W}_{xi}}^T \\cdot \\mathbf{x}_{(t)} + {\\mathbf{W}_{hi}}^T \\cdot \\mathbf{h}_{(t-1)} + \\mathbf{b}_i)\\\\\n",
    "\\mathbf{f}_{(t)}&=\\sigma({\\mathbf{W}_{xf}}^T \\cdot \\mathbf{x}_{(t)} + {\\mathbf{W}_{hf}}^T \\cdot \\mathbf{h}_{(t-1)} + \\mathbf{b}_f)\\\\\n",
    "\\mathbf{o}_{(t)}&=\\sigma({\\mathbf{W}_{xo}}^T \\cdot \\mathbf{x}_{(t)} + {\\mathbf{W}_{ho}}^T \\cdot \\mathbf{h}_{(t-1)} + \\mathbf{b}_o)\\\\\n",
    "\\mathbf{g}_{(t)}&=\\operatorname{tanh}({\\mathbf{W}_{xg}}^T \\cdot \\mathbf{x}_{(t)} + {\\mathbf{W}_{hg}}^T \\cdot \\mathbf{h}_{(t-1)} + \\mathbf{b}_g)\\\\\n",
    "\\mathbf{c}_{(t)}&=\\mathbf{f}_{(t)} \\otimes \\mathbf{c}_{(t-1)} \\, + \\, \\mathbf{i}_{(t)} \\otimes \\mathbf{g}_{(t)}\\\\\n",
    "\\mathbf{y}_{(t)}&=\\mathbf{h}_{(t)} = \\mathbf{o}_{(t)} \\otimes \\operatorname{tanh}(\\mathbf{c}_{(t)})\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 14.4. Obliczenia jednostki GRU**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\mathbf{z}_{(t)}&=\\sigma({\\mathbf{W}_{xz}}^T \\cdot \\mathbf{x}_{(t)} + {\\mathbf{W}_{hz}}^T \\cdot \\mathbf{h}_{(t-1)}) \\\\\n",
    "\\mathbf{r}_{(t)}&=\\sigma({\\mathbf{W}_{xr}}^T \\cdot \\mathbf{x}_{(t)} + {\\mathbf{W}_{hr}}^T \\cdot \\mathbf{h}_{(t-1)}) \\\\\n",
    "\\mathbf{g}_{(t)}&=\\operatorname{tanh}\\left({\\mathbf{W}_{xg}}^T \\cdot \\mathbf{x}_{(t)} + {\\mathbf{W}_{hg}}^T \\cdot (\\mathbf{r}_{(t)} \\otimes \\mathbf{h}_{(t-1)})\\right) \\\\\n",
    "\\mathbf{h}_{(t)}&=(1-\\mathbf{z}_{(t)}) \\otimes \\mathbf{h}_{(t-1)} + \\mathbf{z}_{(t)} \\otimes \\mathbf{g}_{(t)}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 15.\n",
    "\n",
    "**Równanie 15.1. Dywergencja Kullbacka-Leiblera**\n",
    "\n",
    "$\n",
    "D_{\\mathrm{KL}}(P\\|Q) = \\sum\\limits_{i} P(i) \\log \\dfrac{P(i)}{Q(i)}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 15.2. Dywergencja KL pomiędzy docelową rzadkością p a rzeczywistą rzadkością q**\n",
    "\n",
    "$\n",
    "D_{\\mathrm{KL}}(p\\|q) = p \\, \\log \\dfrac{p}{q} + (1-p) \\log \\dfrac{1-p}{1-q}\n",
    "$\n",
    "\n",
    "**W tekście:**\n",
    "\n",
    "Popularną odmianą tego rozwiązania jest trenowanie kodera do wyliczania wyniku $\\gamma = \\log\\left(\\sigma^2\\right)$ zamiast $\\sigma$.\n",
    "Za każdym razem, gdy będzie potrzebne odchylenie standardowe, wystarczy obliczyć $ \\sigma = \\exp\\left(\\dfrac{\\gamma}{2}\\right) $.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozdział 16.\n",
    "\n",
    "**Równanie 16.1. Równanie optymalności Bellmana**\n",
    "\n",
    "$\n",
    "V^*(s) = \\underset{a}{\\max}\\sum\\limits_{s'}{T(s, a, s') [R(s, a, s') + \\gamma . V^*(s')]} \\quad \\text{dla wszystkich }s\n",
    "$\n",
    "\n",
    "**Równanie 16.2. Algorytm iteracji wartości**\n",
    "\n",
    "$\n",
    "  V_{k+1}(s) \\gets \\underset{a}{\\max}\\sum\\limits_{s'}{T(s, a, s') [R(s, a, s') + \\gamma . V_k(s')]} \\quad \\text{dla wszystkich }s\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 16.3. Algorytm iteracji Q-wartości**\n",
    "\n",
    "$\n",
    "  Q_{k+1}(s, a) \\gets \\sum\\limits_{s'}{T(s, a, s') [R(s, a, s') + \\gamma . \\underset{a'}{\\max}\\,{Q_k(s',a')}]} \\quad \\text{dla wszystkich } (s,a)\n",
    "$\n",
    "\n",
    "**W tekście:**\n",
    "\n",
    "Po otrzymaniu optymalnych Q-wartości zdefiniowanie optymalnej polityki (określanej jako $\\pi^{*}(s)$) okazuje się banalne: gdy agent znajduje się w stanie s, powinien wybrać czynność mającą największą Q-wartość dla tego stanu: $ \\pi^{*}(s) = \\underset{a}{\\operatorname{argmax}} \\, Q^*(s, a) $.\n",
    "\n",
    "\n",
    "**Równanie 16.4. Algorytm uczenia TD**\n",
    "\n",
    "$\n",
    "V_{k+1}(s) \\gets (1-\\alpha)V_k(s) + \\alpha\\left(r + \\gamma . V_k(s')\\right)\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 16.5. Algorytm Q-Learning**\n",
    "\n",
    "$\n",
    "Q_{k+1}(s, a) \\gets (1-\\alpha)Q_k(s,a) + \\alpha\\left(r + \\gamma . \\underset{a'}{\\max} \\, Q_k(s', a')\\right)\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 16.6. Algorytm Q-Learning wykorzystujący funkcję poszukiwania**\n",
    "\n",
    "$\n",
    "  Q(s, a) \\gets (1-\\alpha)Q(s,a) + \\alpha\\left(r + \\gamma . \\underset{\\alpha'}{\\max}f(Q(s', a'), N(s', a'))\\right)\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie 16.7. Funkcja kosztu głębokiego Q-uczenia**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "& J(\\mathbf{\\theta}_\\text{krytyk}) = \\dfrac{1}{m}\\sum\\limits_{i=1}^m\\left(y^{(i)} - Q(s^{(i)},a^{(i)},\\mathbf{\\theta}_\\text{krytyk})\\right)^2 \\\\\n",
    "& \\text{gdzie } y^{(i)} = r^{(i)} + \\gamma . \\underset{a'}{\\max}Q(s'^{(i)},a',\\mathbf{\\theta}_\\text{aktor})\n",
    "\\end{split}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dodatek A\n",
    "\n",
    "Wzory pojawiające się w tekście:\n",
    "\n",
    "$\n",
    "\\mathbf{H} =\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{H'} & 0 & \\cdots\\\\\n",
    "0 & 0 & \\\\\n",
    "\\vdots & & \\ddots\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\mathbf{A} =\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{A'} & \\mathbf{I}_m \\\\\n",
    "\\mathbf{0} & -\\mathbf{I}_m\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$ 1 - \\frac{1}{5}^2 - \\frac{4}{5}^2 $\n",
    "\n",
    "\n",
    "$ 1 - \\frac{1}{2}^2 - \\frac{1}{2}^2  $\n",
    "\n",
    "\n",
    "$ \\frac{2}{5} \\times $\n",
    "\n",
    "\n",
    "$ \\frac{3}{5} \\times 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dodatek C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wzory pojawiające się w tekście:\n",
    "\n",
    "$ (\\hat{x}, \\hat{y}) $\n",
    "\n",
    "\n",
    "$ \\hat{\\alpha} $\n",
    "\n",
    "\n",
    "$ (\\hat{x}, \\hat{y}, \\hat{\\alpha}) $\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "\\frac{\\partial}{\\partial x}g(x, y, \\alpha) = 2x - 3\\alpha\\\\\n",
    "\\frac{\\partial}{\\partial y}g(x, y, \\alpha) = 2 - 2\\alpha\\\\\n",
    "\\frac{\\partial}{\\partial \\alpha}g(x, y, \\alpha) = -3x - 2y - 1\\\\\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "$ 2\\hat{x} - 3\\hat{\\alpha} = 2 - 2\\hat{\\alpha} = -3\\hat{x} - 2\\hat{y} - 1 = 0 $\n",
    "\n",
    "\n",
    "$ \\hat{x} = \\frac{3}{2} $\n",
    "\n",
    "\n",
    "$ \\hat{y} = -\\frac{11}{4} $\n",
    "\n",
    "\n",
    "$ \\hat{\\alpha} = 1 $\n",
    "\n",
    "\n",
    "**Równanie C.1. Uogólniony langrażjan dla problemu marginesu twardego**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\mathcal{L}(\\mathbf{w}, b, \\mathbf{\\alpha}) = \\frac{1}{2}\\mathbf{w}^T \\cdot \\mathbf{w} - \\sum\\limits_{i=1}^{m}{\\alpha^{(i)} \\left(t^{(i)}(\\mathbf{w}^T \\cdot \\mathbf{x}^{(i)} + b) - 1\\right)} \\\\\n",
    "\\text{gdzie}\\quad \\alpha^{(i)} \\ge 0 \\quad \\text{for }i = 1, 2, \\dots, m\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**Więcej wzorów w tekście:**\n",
    "\n",
    "$ (\\hat{\\mathbf{w}}, \\hat{b}, \\hat{\\mathbf{\\alpha}}) $\n",
    "\n",
    "\n",
    "$ t^{(i)}((\\hat{\\mathbf{w}})^T \\cdot \\mathbf{x}^{(i)} + \\hat{b}) \\ge 1 \\quad \\text{dla } i = 1, 2, \\dots, m $\n",
    "\n",
    "\n",
    "$ {\\hat{\\alpha}}^{(i)} \\ge 0 \\quad \\text{dla } i = 1, 2, \\dots, m $\n",
    "\n",
    "\n",
    "$ {\\hat{\\alpha}}^{(i)} = 0 $\n",
    "\n",
    "\n",
    "$ t^{(i)}((\\hat{\\mathbf{w}})^T \\cdot \\mathbf{x}^{(i)} + \\hat{b}) = 1 $\n",
    "\n",
    "\n",
    "$ {\\hat{\\alpha}}^{(i)} = 0 $\n",
    "\n",
    "\n",
    "**Równanie C.2. Pochodne cząstkowe uogólnionego lagranżjana**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w}, b, \\mathbf{\\alpha}) = \\mathbf{w} - \\sum\\limits_{i=1}^{m}\\alpha^{(i)}t^{(i)}\\mathbf{x}^{(i)}\\\\\n",
    "\\dfrac{\\partial}{\\partial b}\\mathcal{L}(\\mathbf{w}, b, \\mathbf{\\alpha}) = -\\sum\\limits_{i=1}^{m}\\alpha^{(i)}t^{(i)}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie C.3. Własności punktów stacjonarnych**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\hat{\\mathbf{w}} = \\sum_{i=1}^{m}{\\hat{\\alpha}}^{(i)}t^{(i)}\\mathbf{x}^{(i)}\\\\\n",
    "\\sum_{i=1}^{m}{\\hat{\\alpha}}^{(i)}t^{(i)} = 0\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie C.4. Forma dualna problemu SVM**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\mathcal{L}(\\hat{\\mathbf{w}}, \\hat{b}, \\mathbf{\\alpha}) = \\dfrac{1}{2}\\sum\\limits_{i=1}^{m}{\n",
    "  \\sum\\limits_{j=1}^{m}{\n",
    "  \\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} {\\mathbf{x}^{(i)}}^T \\cdot \\mathbf{x}^{(j)}\n",
    "  }\n",
    "} \\quad - \\quad \\sum\\limits_{i=1}^{m}{\\alpha^{(i)}}\\\\\n",
    "\\text{gdzie}\\quad \\alpha^{(i)} \\ge 0 \\quad \\text{dla }i = 1, 2, \\dots, m\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**Jeszcze więcej wzorów w tekście:**\n",
    "\n",
    "$ \\hat{\\mathbf{\\alpha}} $\n",
    "\n",
    "\n",
    "$ {\\hat{\\alpha}}^{(i)} \\ge 0 $\n",
    "\n",
    "\n",
    "$ \\hat{\\mathbf{\\alpha}} $\n",
    "\n",
    "\n",
    "$ \\hat{\\mathbf{w}} $\n",
    "\n",
    "\n",
    "$ \\hat{b} $\n",
    "\n",
    "\n",
    "$ \\hat{b} = 1 - t^{(k)}({\\hat{\\mathbf{w}}}^T \\cdot \\mathbf{x}^{(k)}) $\n",
    "\n",
    "\n",
    "**Równanie C.5. Oszacowanie członu obciążenia za pomocą formy dualnej**\n",
    "\n",
    "$\n",
    "\\hat{b} = \\dfrac{1}{n_s}\\sum\\limits_{\\scriptstyle i=1 \\atop {\\scriptstyle {\\hat{\\alpha}}^{(i)} > 0}}^{m}{\\left[1 - t^{(i)}({\\hat{\\mathbf{w}}}^T \\cdot \\mathbf{x}^{(i)})\\right]}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dodatek D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Równanie D.1. Pochodne cząstkowe funkcji $f(x,y)$**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\dfrac{\\partial f}{\\partial x} & = \\dfrac{\\partial(x^2y)}{\\partial x} + \\dfrac{\\partial y}{\\partial x} + \\dfrac{\\partial 2}{\\partial x} = y \\dfrac{\\partial(x^2)}{\\partial x} + 0 + 0 = 2xy \\\\\n",
    "\\dfrac{\\partial f}{\\partial y} & = \\dfrac{\\partial(x^2y)}{\\partial y} + \\dfrac{\\partial y}{\\partial y} + \\dfrac{\\partial 2}{\\partial y} = x^2 + 1 + 0 = x^2 + 1 \\\\\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**W tekście:**\n",
    "\n",
    "$ \\frac{\\partial g}{\\partial x} = 0 + (0 \\times x + y \\times 1) = y $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial x}{\\partial x} = 1 $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial y}{\\partial x} = 0 $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial (u \\times v)}{\\partial x} = \\frac{\\partial v}{\\partial x} \\times u + \\frac{\\partial u}{\\partial x} \\times u  $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial g}{\\partial x} = 0 + (0 \\times x + y \\times 1)  $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial g}{\\partial x} = y $\n",
    "\n",
    "\n",
    "**Równanie D.2. Pochodna funkcji _h_(_x_) w punkcie _x_~0~**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "h'(x) & = \\underset{\\textstyle x \\to x_0}{\\lim}\\dfrac{h(x) - h(x_0)}{x - x_0}\\\\\n",
    "      & = \\underset{\\textstyle \\epsilon \\to 0}{\\lim}\\dfrac{h(x_0 + \\epsilon) - h(x_0)}{\\epsilon}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "\n",
    "**Równanie D.3. Przykłady kilku działań na liczbach dualnych**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "&\\lambda(a + b\\epsilon) = \\lambda a + \\lambda b \\epsilon\\\\\n",
    "&(a + b\\epsilon) + (c + d\\epsilon) = (a + c) + (b + d)\\epsilon \\\\\n",
    "&(a + b\\epsilon) \\times (c + d\\epsilon) = ac + (ad + bc)\\epsilon + (bd)\\epsilon^2 = ac + (ad + bc)\\epsilon\\\\\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "**W tekście:**\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial x}(3, 4) $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial y}(3, 4) $\n",
    "\n",
    "\n",
    "**Równanie D.4. Reguła łańcuchowa**\n",
    "\n",
    "$\n",
    "\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial n_i} \\times \\dfrac{\\partial n_i}{\\partial x}\n",
    "$\n",
    "\n",
    "**W tekście:**\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial n_7} = 1 $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial n_5} = \\frac{\\partial f}{\\partial n_7} \\times \\frac{\\partial n_7}{\\partial n_5} $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial n_7} = 1 $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial n_7}{\\partial n_5} $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial n_7}{\\partial n_5} = 1 $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial n_5} = 1 \\times 1 = 1 $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial n_4} = \\frac{\\partial f}{\\partial n_5} \\times \\frac{\\partial n_5}{\\partial n_4} $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial n_5}{\\partial n_4} = n_2 $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial n_4} = 1 \\times n_2 = 4 $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial x} = 24 $\n",
    "\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial y} = 10 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dodatek E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Równanie E.1. Prawdopodobieństwo, że i-ty neuron umieści na wyjściu wartość 1**\n",
    "\n",
    "$\n",
    "p\\left(s_i^{(\\text{kolejny krok})} = 1\\right) \\, = \\, \\sigma\\left(\\frac{\\textstyle \\sum\\limits_{j = 1}^N{w_{i,j}s_j + b_i}}{\\textstyle T}\\right)\n",
    "$\n",
    "\n",
    "**W tekście:**\n",
    "\n",
    "$ \\dot{\\mathbf{x}} $\n",
    "\n",
    "\n",
    "$ \\dot{\\mathbf{h}} $\n",
    "\n",
    "\n",
    "**Równanie E.2. Aktualizacja wagi w algorytmie rozbieżności kontrastowej**\n",
    "\n",
    "$\n",
    "w_{i,j}^{(\\text{kolejny krok})} = w_{i,j} + \\eta(\\mathbf{x}\\mathbf{h}^T - \\dot{\\mathbf{x}} \\dot {\\mathbf{h}}^T)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glosariusz\n",
    "\n",
    "W tekście:\n",
    "\n",
    "$\\ell _1$\n",
    "\n",
    "\n",
    "$\\ell _2$\n",
    "\n",
    "\n",
    "$\\ell _k$\n",
    "\n",
    "\n",
    "$ \\chi^2 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na wypadek, gdyby już Cię rozbolały oczy od tych wszystkich wzorów, zakończmy ten notatnik najpiękniejszym równaniem świata. Nie, nie jest to $E = mc²$, lecz tożsamość Eulera:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$e^{i\\pi}+1=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
