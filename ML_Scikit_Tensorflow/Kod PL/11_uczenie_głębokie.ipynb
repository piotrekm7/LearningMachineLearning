{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rozdział 11. Uczenie głębokie**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Notatnik ten zawiera przykładowy kod i rozwiązania ćwiczeń opisane w rozdziale 11._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfiguracja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upewnijmy się najpierw, że notatnik ten będzie działał w środowiskach Python 2 i 3, importujmy kilka powszechnie stosowanych modułów, upewnijmy się, że moduł MatplotLib będzie prawidłowo tworzył wykresy, a także przygotujmy funkcję zapisującą rysunki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obsługa środowisk Python 2 i Python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Importowanie popularnych modułów\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# W celu zachowania powtarzalności wyników w kolejnych przebiegach\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Generowanie ładnych wykresów\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Lokacja, w której będą zapisywane rysunki\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"głębokie\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"rysunki\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Zapisywanie rysunku\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem zanikających/eksplodujących gradientów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapisywanie rysunku wykres_nasycenia_funkcji_sigmoidalnej\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29b11f1a278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Nasycenie', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Nasycenie', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Liniowa', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoidalna funkcja aktywacji\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"wykres_nasycenia_funkcji_sigmoidalnej\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicjacja Xaviera i He'ego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga: w książce wykorzystywana jest funkcja `tensorflow.contrib.layers.fully_connected()` zamiast funkcji `tf.layers.dense()` (która jeszcze nie istniała w czasie pisania rozdziału). Obecnie zalecane jest używanie funkcji `tf.layers.dense()`, ponieważ wszelkie elementy modułu `contrib` mogą być modyfikowane lub usuwane bez zapowiedzi. Funkcja `dense()` jest niemal identyczna jak funkcja `fully_connected()`, cechuje je jednak kilka pomniejszych różnic:\n",
    "* zmianie uległy nazwy kilku parametrów: `scope` został zmieniony na `name`, `activation_fn` staje się `activation` (w analogiczny sposób został usunięty przyrostek `_fn` z innych parametrów, takich jak `normalizer_fn`), `weights_initializer` jest przemianowany na `kernel_initializer` itd.,\n",
    "* domyślną wartością parametru `activation` jest teraz `None`, a nie `tf.nn.relu`,\n",
    "* nie jest obsługiwana funkcja `tensorflow.contrib.framework.arg_scope()` (opisana w dalszej części rozdziału 11.),\n",
    "* nie są obsługiwane parametry regularyztora (opisane w dalszej części rozdziału 11.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # zestaw MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"ukryta1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nienasycające funkcje aktywacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przeciekająca funkcja ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapisywanie rysunku wykres_przeciekającej_funkcji_relu\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29b170005f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Wyciek', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"'Przeciekająca' funkcja aktywacji ReLU\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"wykres_przeciekającej_funkcji_relu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementacja przeciekającej funkcji ReLU w module TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"ukryta1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyuczmy sieć neuronową wobec zbioru danych MNIST wykorzystując przeciekającą funkcję ReLU. Najpierw stwórzmy graf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # zestaw MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"ukryta1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"ukryta2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"wyjscia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"uczenie\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytajmy teraz dane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/dane/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/dane/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/dane/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/dane/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/dane/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Dokładność: dla mini-grupy 0.86 dla zestawu walidacyjnego 0.9044\n",
      "5 Dokładność: dla mini-grupy 0.94 dla zestawu walidacyjnego 0.951\n",
      "10 Dokładność: dla mini-grupy 0.96 dla zestawu walidacyjnego 0.9666\n",
      "15 Dokładność: dla mini-grupy 1.0 dla zestawu walidacyjnego 0.972\n",
      "20 Dokładność: dla mini-grupy 1.0 dla zestawu walidacyjnego 0.9748\n",
      "25 Dokładność: dla mini-grupy 1.0 dla zestawu walidacyjnego 0.9764\n",
      "30 Dokładność: dla mini-grupy 0.98 dla zestawu walidacyjnego 0.978\n",
      "35 Dokładność: dla mini-grupy 0.96 dla zestawu walidacyjnego 0.9792\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "            print(epoch, \"Dokładność: dla mini-grupy\", acc_train, \"dla zestawu walidacyjnego\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapisywanie rysunku wykres_funkcji_elu\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQkkRRQIiKSSAooERFJpP8PtISUN2S6RH8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29b16fb6c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Funkcja aktywacji ELU ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"wykres_funkcji_elu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementacja funkcji ELU w module TensorFlow jest banalna; wystarczy określić funkcję aktywacji podczas tworzenia każdej warstwy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"ukryta1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta funkcja aktywacji została opisana w tej [znakomitej publikacji](https://arxiv.org/pdf/1706.02515.pdf) autorstwa Güntera Klambauera, Thomasa Unterthinera i Andreasa Mayra, która ukazała się w czerwcu 2017 roku (zamierzam o niej wspomnieć w kolejnych wydaniach książki). Osiąga ona znacznie lepsze wyniki od pozostałych funkcji aktywacji w obszarze głębokich sieci neuronowych, dlatego naprawdę warto ją wypróbować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapisywanie rysunku wykres_funkcji_selu\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhDbYqSIJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29b186f4cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Funkcja aktywacji SELU\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"wykres_funkcji_selu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dzięki tej funkcji aktywacji nawet stuwarstwowa głęboka sieć neuronowa zachowuje w przybliżeniu średnią 0 i odchylenie standardowe równe 1 we wszystkich warstwach, co pozwala uniknąć problemu zanikających/eksplodujących gradientów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warstwa 0: -0.26 < średnia < 0.27, 0.74 < odchylenie standardowe < 1.27\n",
      "Warstwa 10: -0.24 < średnia < 0.27, 0.74 < odchylenie standardowe < 1.27\n",
      "Warstwa 20: -0.17 < średnia < 0.18, 0.74 < odchylenie standardowe < 1.24\n",
      "Warstwa 30: -0.27 < średnia < 0.24, 0.78 < odchylenie standardowe < 1.20\n",
      "Warstwa 40: -0.38 < średnia < 0.39, 0.74 < odchylenie standardowe < 1.25\n",
      "Warstwa 50: -0.27 < średnia < 0.31, 0.73 < odchylenie standardowe < 1.27\n",
      "Warstwa 60: -0.26 < średnia < 0.43, 0.74 < odchylenie standardowe < 1.35\n",
      "Warstwa 70: -0.19 < średnia < 0.21, 0.75 < odchylenie standardowe < 1.21\n",
      "Warstwa 80: -0.18 < średnia < 0.16, 0.72 < odchylenie standardowe < 1.19\n",
      "Warstwa 90: -0.19 < średnia < 0.16, 0.75 < odchylenie standardowe < 1.20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 10 == 0:\n",
    "        print(\"Warstwa {}: {:.2f} < średnia < {:.2f}, {:.2f} < odchylenie standardowe < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej przedstawiam implementację w module TensorFlow (niemal z pewnością w przyszłych wersjach tejże biblioteki zostanie wstawiona funkcja `tf.nn.selu()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcje SELU mogą być również łączone z metodą porzucania; sprawdź [tę implementację](https://github.com/bioinf-jku/SNNs/blob/master/selu.py) stworzoną w instytucie bioinformatyki na Uniwersytecie Johannesa Keplera w Linz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy sieć neuronową dla zestawu danych MNIST, wykorzystującą funkcję aktywacji SELU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # zestaw MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"ukryta1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"ukryta2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"wyjscia\")\n",
    "\n",
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"uczenie\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wytrenujmy ją teraz. Nie zapomnijmy przeskalować danych wejściowych do średniej równej 0 i odchylenia standardowego równego 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Dokładność: dla mini-grupy 0.96 dla zbioru walidacyjnego: 0.924\n",
      "5 Dokładność: dla mini-grupy 1.0 dla zbioru walidacyjnego: 0.9568\n",
      "10 Dokładność: dla mini-grupy 0.94 dla zbioru walidacyjnego: 0.967\n",
      "15 Dokładność: dla mini-grupy 0.98 dla zbioru walidacyjnego: 0.9686\n",
      "20 Dokładność: dla mini-grupy 1.0 dla zbioru walidacyjnego: 0.971\n",
      "25 Dokładność: dla mini-grupy 1.0 dla zbioru walidacyjnego: 0.9692\n",
      "30 Dokładność: dla mini-grupy 1.0 dla zbioru walidacyjnego: 0.9702\n",
      "35 Dokładność: dla mini-grupy 1.0 dla zbioru walidacyjnego: 0.971\n"
     ]
    }
   ],
   "source": [
    "means = mnist.train.images.mean(axis=0, keepdims=True)\n",
    "stds = mnist.train.images.std(axis=0, keepdims=True) + 1e-10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            X_val_scaled = (mnist.validation.images - means) / stds\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_val_scaled, y: mnist.validation.labels})\n",
    "            print(epoch, \"Dokładność: dla mini-grupy\", acc_train, \"dla zbioru walidacyjnego:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_model_ostateczny_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizacja wsadowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga: w książce wykorzystywałem funkcję `tensorflow.contrib.layers.batch_norm()` zamiast funkcji `tf.layers.batch_normalization()` (która jeszcze nie istniała w trakcie pisania tego rozdziału). Obecnie zalecane jest używanie funkcji  `tf.layers.batch_normalization()`, ponieważ wszelkie elementy modułu contrib mogą być modyfikowane lub usuwane bez zapowiedzi. Obecnie nie wykorzystujemy funkcji `batch_norm()` jako parametru regularyzacji funkcji `fully_connected()`, lecz wprowadzamy funkcję `batch_normalization()` i jawnie tworzymy oddzielną warstwę. Parametry różnią się nieznacznie, zwłaszcza:  \n",
    "* parametr `decay` jest przemianowany na `momentum`,\n",
    "* parametr `is_training` został przemianowany na `training`,\n",
    "* parametr `updates_collections` został usunięty: operacje aktualizowania wymagane przez normalizację wsadową znajdują się teraz w kolekcji `UPDATE_OPS`, a my musimy je jawnie uruchamiać w trakcie uczenia (patrz poniższa faza wykonawcza),\n",
    "* nie musimy wyznaczać wartości `scale=True`, gdyż jest ona domyślna.\n",
    "\n",
    "Zwróć również uwagę, że w celu przeprowadzenia normalizacji wsadowej tuż _przed_ funkcją aktywacji w każdej warstwie ukrytej, ręcznie wprowadzamy funkcję ELU zaraz za warstwą normalizacji wsadowej.\n",
    "\n",
    "Uwaga: funkcja `tf.layers.dense()` jest niekompatybilna z funkcją `tf.contrib.layers.arg_scope()` (która jest używana w książce), dlatego zastępuję ją teraz funkcją `functools.partial()`. W ten sposób możemy z łatwością stworzyć funkcję `my_dense_layer()`, której jedynym zadaniem będzie wywoływanie funkcji `tf.layers.dense()` z automatycznie wyznaczonymi pożądanymi wartościami parametrów (chyba że zostają one przesłonięte w trakcie wywoływania funkcji `my_dense_layer()`). Jak widać, sam kod nie ulega większym zmianom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='uczenie')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"ukryta1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"ukryta2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"wyjscia\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='uczenie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby uniknąć ciągłego powtarzania parametrów, możemy skorzystać z funkcji `partial()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"ukryta1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"ukryta2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"wyjscia\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy teraz sieć neuronową dla zbioru MNIST, wykorzystując funkcję aktywacji ELU i normalizację wsadową w każdej warstwie: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='uczenie')\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"ukryta1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"ukryta2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"wyjscia\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "with tf.name_scope(\"uczenie\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga: korzystamy z funkcji `tf.layers.batch_normalization()` zamiast z funkcji `tf.contrib.layers.batch_norm()` (stosowanej w książce), dlatego musimy jawnie uruchomić dodatkowe operacje aktualizacji wymagane przez normalizację wsadową (`sess.run([training_op, extra_update_ops],...`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Dokładność dla zbioru testowego: 0.8721\n",
      "1 Dokładność dla zbioru testowego: 0.8981\n",
      "2 Dokładność dla zbioru testowego: 0.9134\n",
      "3 Dokładność dla zbioru testowego: 0.9238\n",
      "4 Dokładność dla zbioru testowego: 0.9297\n",
      "5 Dokładność dla zbioru testowego: 0.9352\n",
      "6 Dokładność dla zbioru testowego: 0.9405\n",
      "7 Dokładność dla zbioru testowego: 0.9436\n",
      "8 Dokładność dla zbioru testowego: 0.947\n",
      "9 Dokładność dla zbioru testowego: 0.9502\n",
      "10 Dokładność dla zbioru testowego: 0.9528\n",
      "11 Dokładność dla zbioru testowego: 0.9549\n",
      "12 Dokładność dla zbioru testowego: 0.9566\n",
      "13 Dokładność dla zbioru testowego: 0.9589\n",
      "14 Dokładność dla zbioru testowego: 0.9602\n",
      "15 Dokładność dla zbioru testowego: 0.9612\n",
      "16 Dokładność dla zbioru testowego: 0.962\n",
      "17 Dokładność dla zbioru testowego: 0.9637\n",
      "18 Dokładność dla zbioru testowego: 0.9651\n",
      "19 Dokładność dla zbioru testowego: 0.9656\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co!? To wcale nie jest dobra dokładność dla zbioru MNIST. Oczywiście, jeśli pozwolisz modelowi uczyć się dłużej, uzyska znacznie lepszą dokładność, ale przy tak płytkiej sieci zarówno normalizacja wsadowa, jak i funkcja ELU będą miały niewielki wpływ na wydajność: rozwijają one skrzydła dopiero w znacznie głębszych sieciach neuronowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że moglibyśmy również uzależnić operację uczenia od operacji aktualizowania:\n",
    "\n",
    "```python\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "```\n",
    "\n",
    "W ten sposób wystarczyłoby jedynie uruchomić operację `training_op` w trakcie uczenia, a moduł TensorFlow automatycznie przeprowadzałby również operacje aktualizowania:\n",
    "\n",
    "```python\n",
    "sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeszcze jedna sprawa: zauważ, że lista modyfikowalnych zmiennych jest krótsza od listy wszystkich zmiennych globalnych. Wynika to z faktu, że średnie ruchome są zmiennymi niemodyfikowalnymi. Jeżeli chcesz korzystać z gotowej, już wyuczonej sieci neuronowej (patrz niżej), nie możesz zapominać o tych niemodyfikowalnych zmiennych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ukryta1/kernel:0',\n",
       " 'ukryta1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'ukryta2/kernel:0',\n",
       " 'ukryta2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'wyjscia/kernel:0',\n",
       " 'wyjscia/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ukryta1/kernel:0',\n",
       " 'ukryta1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'ukryta2/kernel:0',\n",
       " 'ukryta2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'wyjscia/kernel:0',\n",
       " 'wyjscia/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obcinanie gradientu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwózmy prostą sieć neuronową dla zbioru MNIST i dodajmy technikę obcinania gradientu. Pierwsza część sieci jest taka sama, jak wcześniej (jedyna różnica polega na dodaniu kilku warstw w celu ukazania mechanizmu wielokrotnego wykorzystywania gotowych modeli; patrz niżej):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # zbiór MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"ukryta1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"ukryta2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"ukryta3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"ukryta4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"ukryta5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"wyjscia\")\n",
    "\n",
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wprowadźmy teraz obcinanie gradientu. W tym celu musimy najpierw pobrać gradienty, obciąć je za pomocą funkcji `clip_by_value()`, a następnie wprowadzić je z powrotem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pozostała część procesu wygląda tak, jak zwykle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Dokładność dla zbioru testowego: 0.3138\n",
      "1 Dokładność dla zbioru testowego: 0.7999\n",
      "2 Dokładność dla zbioru testowego: 0.8805\n",
      "3 Dokładność dla zbioru testowego: 0.9037\n",
      "4 Dokładność dla zbioru testowego: 0.9122\n",
      "5 Dokładność dla zbioru testowego: 0.9197\n",
      "6 Dokładność dla zbioru testowego: 0.9245\n",
      "7 Dokładność dla zbioru testowego: 0.9298\n",
      "8 Dokładność dla zbioru testowego: 0.9331\n",
      "9 Dokładność dla zbioru testowego: 0.939\n",
      "10 Dokładność dla zbioru testowego: 0.943\n",
      "11 Dokładność dla zbioru testowego: 0.9448\n",
      "12 Dokładność dla zbioru testowego: 0.9455\n",
      "13 Dokładność dla zbioru testowego: 0.9485\n",
      "14 Dokładność dla zbioru testowego: 0.9525\n",
      "15 Dokładność dla zbioru testowego: 0.9513\n",
      "16 Dokładność dla zbioru testowego: 0.956\n",
      "17 Dokładność dla zbioru testowego: 0.9583\n",
      "18 Dokładność dla zbioru testowego: 0.9559\n",
      "19 Dokładność dla zbioru testowego: 0.9607\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wielokrotne stosowanie gotowych warstw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wielokrotne stosowanie modelu TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najpierw musimy wczytać strukturę grafu. Funkcja `import_meta_graph()` wykonuje tylko tę czynność i wczytuje operacje do domyślnego grafu, po czym zwraca węzeł `Saver`, za pomocą którego jesteśmy w stanie odtworzyć stan modelu. Zwróć uwagę, że domyślnie obiekt `Saver` zachowuje strukturę grafu w pliku `.meta`, zatem to ten plik należy wczytać:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./moj_model_ostateczny.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie musimy zająć się wszystkimi operacjami używanymi do uczenia modelu. Jeśli nie znasz struktury grafu, możesz wyświetlić listę wszystkich operacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "ukryta1/kernel/Initializer/random_uniform/shape\n",
      "ukryta1/kernel/Initializer/random_uniform/min\n",
      "ukryta1/kernel/Initializer/random_uniform/max\n",
      "ukryta1/kernel/Initializer/random_uniform/RandomUniform\n",
      "ukryta1/kernel/Initializer/random_uniform/sub\n",
      "ukryta1/kernel/Initializer/random_uniform/mul\n",
      "ukryta1/kernel/Initializer/random_uniform\n",
      "ukryta1/kernel\n",
      "ukryta1/kernel/Assign\n",
      "ukryta1/kernel/read\n",
      "ukryta1/bias/Initializer/zeros\n",
      "ukryta1/bias\n",
      "ukryta1/bias/Assign\n",
      "ukryta1/bias/read\n",
      "gsn/ukryta1/MatMul\n",
      "gsn/ukryta1/BiasAdd\n",
      "gsn/ukryta1/Relu\n",
      "ukryta2/kernel/Initializer/random_uniform/shape\n",
      "ukryta2/kernel/Initializer/random_uniform/min\n",
      "ukryta2/kernel/Initializer/random_uniform/max\n",
      "ukryta2/kernel/Initializer/random_uniform/RandomUniform\n",
      "ukryta2/kernel/Initializer/random_uniform/sub\n",
      "ukryta2/kernel/Initializer/random_uniform/mul\n",
      "ukryta2/kernel/Initializer/random_uniform\n",
      "ukryta2/kernel\n",
      "ukryta2/kernel/Assign\n",
      "ukryta2/kernel/read\n",
      "ukryta2/bias/Initializer/zeros\n",
      "ukryta2/bias\n",
      "ukryta2/bias/Assign\n",
      "ukryta2/bias/read\n",
      "gsn/ukryta2/MatMul\n",
      "gsn/ukryta2/BiasAdd\n",
      "gsn/ukryta2/Relu\n",
      "ukryta3/kernel/Initializer/random_uniform/shape\n",
      "ukryta3/kernel/Initializer/random_uniform/min\n",
      "ukryta3/kernel/Initializer/random_uniform/max\n",
      "ukryta3/kernel/Initializer/random_uniform/RandomUniform\n",
      "ukryta3/kernel/Initializer/random_uniform/sub\n",
      "ukryta3/kernel/Initializer/random_uniform/mul\n",
      "ukryta3/kernel/Initializer/random_uniform\n",
      "ukryta3/kernel\n",
      "ukryta3/kernel/Assign\n",
      "ukryta3/kernel/read\n",
      "ukryta3/bias/Initializer/zeros\n",
      "ukryta3/bias\n",
      "ukryta3/bias/Assign\n",
      "ukryta3/bias/read\n",
      "gsn/ukryta3/MatMul\n",
      "gsn/ukryta3/BiasAdd\n",
      "gsn/ukryta3/Relu\n",
      "ukryta4/kernel/Initializer/random_uniform/shape\n",
      "ukryta4/kernel/Initializer/random_uniform/min\n",
      "ukryta4/kernel/Initializer/random_uniform/max\n",
      "ukryta4/kernel/Initializer/random_uniform/RandomUniform\n",
      "ukryta4/kernel/Initializer/random_uniform/sub\n",
      "ukryta4/kernel/Initializer/random_uniform/mul\n",
      "ukryta4/kernel/Initializer/random_uniform\n",
      "ukryta4/kernel\n",
      "ukryta4/kernel/Assign\n",
      "ukryta4/kernel/read\n",
      "ukryta4/bias/Initializer/zeros\n",
      "ukryta4/bias\n",
      "ukryta4/bias/Assign\n",
      "ukryta4/bias/read\n",
      "gsn/ukryta4/MatMul\n",
      "gsn/ukryta4/BiasAdd\n",
      "gsn/ukryta4/Relu\n",
      "ukryta5/kernel/Initializer/random_uniform/shape\n",
      "ukryta5/kernel/Initializer/random_uniform/min\n",
      "ukryta5/kernel/Initializer/random_uniform/max\n",
      "ukryta5/kernel/Initializer/random_uniform/RandomUniform\n",
      "ukryta5/kernel/Initializer/random_uniform/sub\n",
      "ukryta5/kernel/Initializer/random_uniform/mul\n",
      "ukryta5/kernel/Initializer/random_uniform\n",
      "ukryta5/kernel\n",
      "ukryta5/kernel/Assign\n",
      "ukryta5/kernel/read\n",
      "ukryta5/bias/Initializer/zeros\n",
      "ukryta5/bias\n",
      "ukryta5/bias/Assign\n",
      "ukryta5/bias/read\n",
      "gsn/ukryta5/MatMul\n",
      "gsn/ukryta5/BiasAdd\n",
      "gsn/ukryta5/Relu\n",
      "wyjscia/kernel/Initializer/random_uniform/shape\n",
      "wyjscia/kernel/Initializer/random_uniform/min\n",
      "wyjscia/kernel/Initializer/random_uniform/max\n",
      "wyjscia/kernel/Initializer/random_uniform/RandomUniform\n",
      "wyjscia/kernel/Initializer/random_uniform/sub\n",
      "wyjscia/kernel/Initializer/random_uniform/mul\n",
      "wyjscia/kernel/Initializer/random_uniform\n",
      "wyjscia/kernel\n",
      "wyjscia/kernel/Assign\n",
      "wyjscia/kernel/read\n",
      "wyjscia/bias/Initializer/zeros\n",
      "wyjscia/bias\n",
      "wyjscia/bias/Assign\n",
      "wyjscia/bias/read\n",
      "gsn/wyjscia/MatMul\n",
      "gsn/wyjscia/BiasAdd\n",
      "strata/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "strata/Const\n",
      "strata/strata\n",
      "gradients/Shape\n",
      "gradients/Const\n",
      "gradients/Fill\n",
      "gradients/strata/strata_grad/Reshape/shape\n",
      "gradients/strata/strata_grad/Reshape\n",
      "gradients/strata/strata_grad/Shape\n",
      "gradients/strata/strata_grad/Tile\n",
      "gradients/strata/strata_grad/Shape_1\n",
      "gradients/strata/strata_grad/Shape_2\n",
      "gradients/strata/strata_grad/Const\n",
      "gradients/strata/strata_grad/Prod\n",
      "gradients/strata/strata_grad/Const_1\n",
      "gradients/strata/strata_grad/Prod_1\n",
      "gradients/strata/strata_grad/Maximum/y\n",
      "gradients/strata/strata_grad/Maximum\n",
      "gradients/strata/strata_grad/floordiv\n",
      "gradients/strata/strata_grad/Cast\n",
      "gradients/strata/strata_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/gsn/wyjscia/BiasAdd_grad/BiasAddGrad\n",
      "gradients/gsn/wyjscia/BiasAdd_grad/tuple/group_deps\n",
      "gradients/gsn/wyjscia/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/gsn/wyjscia/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/gsn/wyjscia/MatMul_grad/MatMul\n",
      "gradients/gsn/wyjscia/MatMul_grad/MatMul_1\n",
      "gradients/gsn/wyjscia/MatMul_grad/tuple/group_deps\n",
      "gradients/gsn/wyjscia/MatMul_grad/tuple/control_dependency\n",
      "gradients/gsn/wyjscia/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/gsn/ukryta5/Relu_grad/ReluGrad\n",
      "gradients/gsn/ukryta5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/gsn/ukryta5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/gsn/ukryta5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/gsn/ukryta5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/gsn/ukryta5/MatMul_grad/MatMul\n",
      "gradients/gsn/ukryta5/MatMul_grad/MatMul_1\n",
      "gradients/gsn/ukryta5/MatMul_grad/tuple/group_deps\n",
      "gradients/gsn/ukryta5/MatMul_grad/tuple/control_dependency\n",
      "gradients/gsn/ukryta5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/gsn/ukryta4/Relu_grad/ReluGrad\n",
      "gradients/gsn/ukryta4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/gsn/ukryta4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/gsn/ukryta4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/gsn/ukryta4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/gsn/ukryta4/MatMul_grad/MatMul\n",
      "gradients/gsn/ukryta4/MatMul_grad/MatMul_1\n",
      "gradients/gsn/ukryta4/MatMul_grad/tuple/group_deps\n",
      "gradients/gsn/ukryta4/MatMul_grad/tuple/control_dependency\n",
      "gradients/gsn/ukryta4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/gsn/ukryta3/Relu_grad/ReluGrad\n",
      "gradients/gsn/ukryta3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/gsn/ukryta3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/gsn/ukryta3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/gsn/ukryta3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/gsn/ukryta3/MatMul_grad/MatMul\n",
      "gradients/gsn/ukryta3/MatMul_grad/MatMul_1\n",
      "gradients/gsn/ukryta3/MatMul_grad/tuple/group_deps\n",
      "gradients/gsn/ukryta3/MatMul_grad/tuple/control_dependency\n",
      "gradients/gsn/ukryta3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/gsn/ukryta2/Relu_grad/ReluGrad\n",
      "gradients/gsn/ukryta2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/gsn/ukryta2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/gsn/ukryta2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/gsn/ukryta2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/gsn/ukryta2/MatMul_grad/MatMul\n",
      "gradients/gsn/ukryta2/MatMul_grad/MatMul_1\n",
      "gradients/gsn/ukryta2/MatMul_grad/tuple/group_deps\n",
      "gradients/gsn/ukryta2/MatMul_grad/tuple/control_dependency\n",
      "gradients/gsn/ukryta2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/gsn/ukryta1/Relu_grad/ReluGrad\n",
      "gradients/gsn/ukryta1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/gsn/ukryta1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/gsn/ukryta1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/gsn/ukryta1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/gsn/ukryta1/MatMul_grad/MatMul\n",
      "gradients/gsn/ukryta1/MatMul_grad/MatMul_1\n",
      "gradients/gsn/ukryta1/MatMul_grad/tuple/group_deps\n",
      "gradients/gsn/ukryta1/MatMul_grad/tuple/control_dependency\n",
      "gradients/gsn/ukryta1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_ukryta1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_ukryta1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_ukryta2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_ukryta2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_ukryta3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_ukryta3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_ukryta4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_ukryta4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_ukryta5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_ukryta5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_wyjscia/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_wyjscia/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "ocena/InTopK\n",
      "ocena/Cast\n",
      "ocena/Const\n",
      "ocena/dokladnosc\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/Assign_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/Assign_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/Assign_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/Assign_5\n",
      "save/RestoreV2_6/tensor_names\n",
      "save/RestoreV2_6/shape_and_slices\n",
      "save/RestoreV2_6\n",
      "save/Assign_6\n",
      "save/RestoreV2_7/tensor_names\n",
      "save/RestoreV2_7/shape_and_slices\n",
      "save/RestoreV2_7\n",
      "save/Assign_7\n",
      "save/RestoreV2_8/tensor_names\n",
      "save/RestoreV2_8/shape_and_slices\n",
      "save/RestoreV2_8\n",
      "save/Assign_8\n",
      "save/RestoreV2_9/tensor_names\n",
      "save/RestoreV2_9/shape_and_slices\n",
      "save/RestoreV2_9\n",
      "save/Assign_9\n",
      "save/RestoreV2_10/tensor_names\n",
      "save/RestoreV2_10/shape_and_slices\n",
      "save/RestoreV2_10\n",
      "save/Assign_10\n",
      "save/RestoreV2_11/tensor_names\n",
      "save/RestoreV2_11/shape_and_slices\n",
      "save/RestoreV2_11\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ups, całkiem dużo tych operacji! Znacznie łatwiej jest wykorzystać narzędzie TensorBoard i zwizualizować ten graf. Za pomocą poniższej sztudzki możemy dokonać tego wewnątrz notatnika Jupyter (jeżeli poniższy kod nie chce współpracować z Twoją przeglądarką, musisz użyć obiektu `FileWriter` i zapisać ten graf, a następnie zwizualizować go w aplikacji TensorBoard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Wyciąga duże wartości stałych z obiektu graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Wizualizuje graf TensorFlow.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 784\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;ukryta1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;ukryta1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;ukryta1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;ukryta1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;ukryta1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;ukryta1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;ukryta1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta1/kernel&quot;\\n  input: &quot;ukryta1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;ukryta1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta1/bias&quot;\\n  input: &quot;ukryta1/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;ukryta1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;ukryta1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;gsn/ukryta1/MatMul&quot;\\n  input: &quot;ukryta1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta1/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;gsn/ukryta1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;ukryta2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 22\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;ukryta2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;ukryta2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;ukryta2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;ukryta2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;ukryta2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;ukryta2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta2/kernel&quot;\\n  input: &quot;ukryta2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;ukryta2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta2/bias&quot;\\n  input: &quot;ukryta2/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;ukryta2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gsn/ukryta1/Relu&quot;\\n  input: &quot;ukryta2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;gsn/ukryta2/MatMul&quot;\\n  input: &quot;ukryta2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta2/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;gsn/ukryta2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;ukryta3/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 39\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;ukryta3/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;ukryta3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;ukryta3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;ukryta3/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;ukryta3/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;ukryta3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta3/kernel&quot;\\n  input: &quot;ukryta3/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;ukryta3/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta3/bias&quot;\\n  input: &quot;ukryta3/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta3/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;ukryta3/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta3/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gsn/ukryta2/Relu&quot;\\n  input: &quot;ukryta3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;gsn/ukryta3/MatMul&quot;\\n  input: &quot;ukryta3/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta3/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;gsn/ukryta3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;ukryta4/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 56\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;ukryta4/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;ukryta4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;ukryta4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;ukryta4/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;ukryta4/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;ukryta4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta4/kernel&quot;\\n  input: &quot;ukryta4/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;ukryta4/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta4/bias&quot;\\n  input: &quot;ukryta4/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta4/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;ukryta4/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta4/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gsn/ukryta3/Relu&quot;\\n  input: &quot;ukryta4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta4/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;gsn/ukryta4/MatMul&quot;\\n  input: &quot;ukryta4/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta4/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;gsn/ukryta4/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;ukryta5/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 73\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;ukryta5/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;ukryta5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;ukryta5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;ukryta5/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;ukryta5/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;ukryta5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta5/kernel&quot;\\n  input: &quot;ukryta5/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;ukryta5/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta5/bias&quot;\\n  input: &quot;ukryta5/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ukryta5/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;ukryta5/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta5/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gsn/ukryta4/Relu&quot;\\n  input: &quot;ukryta5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta5/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;gsn/ukryta5/MatMul&quot;\\n  input: &quot;ukryta5/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/ukryta5/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;gsn/ukryta5/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;wyjscia/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 90\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;wyjscia/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;wyjscia/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;wyjscia/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;wyjscia/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;wyjscia/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;wyjscia/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;wyjscia/kernel&quot;\\n  input: &quot;wyjscia/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;wyjscia/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;wyjscia/bias&quot;\\n  input: &quot;wyjscia/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;wyjscia/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;wyjscia/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/wyjscia/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gsn/ukryta5/Relu&quot;\\n  input: &quot;wyjscia/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gsn/wyjscia/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;gsn/wyjscia/MatMul&quot;\\n  input: &quot;wyjscia/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;strata/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;gsn/wyjscia/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;strata/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;strata/strata&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;strata/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/strata/strata_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/strata/strata_grad/Reshape&quot;\\n  input: &quot;gradients/strata/strata_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/strata/strata_grad/Shape_1&quot;\\n  input: &quot;gradients/strata/strata_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/strata/strata_grad/Shape_2&quot;\\n  input: &quot;gradients/strata/strata_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/strata/strata_grad/Prod_1&quot;\\n  input: &quot;gradients/strata/strata_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/strata/strata_grad/Prod&quot;\\n  input: &quot;gradients/strata/strata_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/strata/strata_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/strata_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/strata/strata_grad/Tile&quot;\\n  input: &quot;gradients/strata/strata_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients/strata/strata_grad/truediv&quot;\\n  input: &quot;gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/wyjscia/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/wyjscia/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/gsn/wyjscia/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/wyjscia/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/gsn/wyjscia/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/strata/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/wyjscia/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/wyjscia/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/gsn/wyjscia/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/wyjscia/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/wyjscia/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/gsn/wyjscia/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;wyjscia/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/wyjscia/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gsn/ukryta5/Relu&quot;\\n  input: &quot;gradients/gsn/wyjscia/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/wyjscia/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/gsn/wyjscia/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/wyjscia/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/wyjscia/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/wyjscia/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/wyjscia/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/wyjscia/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/wyjscia/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/wyjscia/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/gsn/wyjscia/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/wyjscia/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta5/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/gsn/wyjscia/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gsn/ukryta5/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/gsn/ukryta5/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/gsn/ukryta5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta5/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta5/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta5/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/gsn/ukryta5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;ukryta5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta5/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gsn/ukryta4/Relu&quot;\\n  input: &quot;gradients/gsn/ukryta5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta5/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/gsn/ukryta5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/ukryta5/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta5/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/ukryta5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta5/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta5/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta5/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/gsn/ukryta5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta5/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/gsn/ukryta5/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gsn/ukryta4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/gsn/ukryta4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/gsn/ukryta4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta4/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/gsn/ukryta4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;ukryta4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gsn/ukryta3/Relu&quot;\\n  input: &quot;gradients/gsn/ukryta4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/gsn/ukryta4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/ukryta4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/ukryta4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/gsn/ukryta4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/gsn/ukryta4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gsn/ukryta3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/gsn/ukryta3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/gsn/ukryta3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta3/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/gsn/ukryta3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;ukryta3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gsn/ukryta2/Relu&quot;\\n  input: &quot;gradients/gsn/ukryta3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/gsn/ukryta3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/ukryta3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/ukryta3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/gsn/ukryta3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/gsn/ukryta3/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gsn/ukryta2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/gsn/ukryta2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/gsn/ukryta2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta2/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/gsn/ukryta2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;ukryta2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gsn/ukryta1/Relu&quot;\\n  input: &quot;gradients/gsn/ukryta2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/gsn/ukryta2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/ukryta2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/ukryta2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/gsn/ukryta2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/gsn/ukryta2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gsn/ukryta1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/gsn/ukryta1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/gsn/ukryta1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta1/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/gsn/ukryta1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/gsn/ukryta1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;ukryta1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/gsn/ukryta1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/gsn/ukryta1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/ukryta1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/gsn/ukryta1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/gsn/ukryta1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/gsn/ukryta1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/gsn/ukryta1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/gsn/ukryta1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/ukryta1/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value/Minimum&quot;\\n  input: &quot;clip_by_value/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/ukryta1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_1/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_1/Minimum&quot;\\n  input: &quot;clip_by_value_1/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/ukryta2/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_2/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_2/Minimum&quot;\\n  input: &quot;clip_by_value_2/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/ukryta2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_3/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_3/Minimum&quot;\\n  input: &quot;clip_by_value_3/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/ukryta3/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_4/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_4/Minimum&quot;\\n  input: &quot;clip_by_value_4/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/ukryta3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_5/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_5/Minimum&quot;\\n  input: &quot;clip_by_value_5/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/ukryta4/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_6/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_6/Minimum&quot;\\n  input: &quot;clip_by_value_6/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/ukryta4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_7/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_7/Minimum&quot;\\n  input: &quot;clip_by_value_7/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/ukryta5/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_8/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_8/Minimum&quot;\\n  input: &quot;clip_by_value_8/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/ukryta5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_9/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_9/Minimum&quot;\\n  input: &quot;clip_by_value_9/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/wyjscia/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_10/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_10/Minimum&quot;\\n  input: &quot;clip_by_value_10/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/gsn/wyjscia/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_11/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_11/Minimum&quot;\\n  input: &quot;clip_by_value_11/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_ukryta1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;ukryta1/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_ukryta1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;ukryta1/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_ukryta2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;ukryta2/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_ukryta2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;ukryta2/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_ukryta3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;ukryta3/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_ukryta3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;ukryta3/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_ukryta4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;ukryta4/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_ukryta4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;ukryta4/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_ukryta5/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;ukryta5/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_ukryta5/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;ukryta5/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_wyjscia/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;wyjscia/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_wyjscia/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;wyjscia/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_ukryta1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_ukryta1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_ukryta2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_ukryta2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_ukryta3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_ukryta3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_ukryta4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_ukryta4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_ukryta5/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_ukryta5/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_wyjscia/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_wyjscia/bias/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;ocena/InTopK&quot;\\n  op: &quot;InTopK&quot;\\n  input: &quot;gsn/wyjscia/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;k&quot;\\n    value {\\n      i: 1\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ocena/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;ocena/InTopK&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ocena/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ocena/dokladnosc&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;ocena/Cast&quot;\\n  input: &quot;ocena/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^ukryta1/kernel/Assign&quot;\\n  input: &quot;^ukryta1/bias/Assign&quot;\\n  input: &quot;^ukryta2/kernel/Assign&quot;\\n  input: &quot;^ukryta2/bias/Assign&quot;\\n  input: &quot;^ukryta3/kernel/Assign&quot;\\n  input: &quot;^ukryta3/bias/Assign&quot;\\n  input: &quot;^ukryta4/kernel/Assign&quot;\\n  input: &quot;^ukryta4/bias/Assign&quot;\\n  input: &quot;^ukryta5/kernel/Assign&quot;\\n  input: &quot;^ukryta5/bias/Assign&quot;\\n  input: &quot;^wyjscia/kernel/Assign&quot;\\n  input: &quot;^wyjscia/bias/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;ukryta1/bias&quot;\\n        string_val: &quot;ukryta1/kernel&quot;\\n        string_val: &quot;ukryta2/bias&quot;\\n        string_val: &quot;ukryta2/kernel&quot;\\n        string_val: &quot;ukryta3/bias&quot;\\n        string_val: &quot;ukryta3/kernel&quot;\\n        string_val: &quot;ukryta4/bias&quot;\\n        string_val: &quot;ukryta4/kernel&quot;\\n        string_val: &quot;ukryta5/bias&quot;\\n        string_val: &quot;ukryta5/kernel&quot;\\n        string_val: &quot;wyjscia/bias&quot;\\n        string_val: &quot;wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;ukryta1/bias&quot;\\n  input: &quot;ukryta1/kernel&quot;\\n  input: &quot;ukryta2/bias&quot;\\n  input: &quot;ukryta2/kernel&quot;\\n  input: &quot;ukryta3/bias&quot;\\n  input: &quot;ukryta3/kernel&quot;\\n  input: &quot;ukryta4/bias&quot;\\n  input: &quot;ukryta4/kernel&quot;\\n  input: &quot;ukryta5/bias&quot;\\n  input: &quot;ukryta5/kernel&quot;\\n  input: &quot;wyjscia/bias&quot;\\n  input: &quot;wyjscia/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;ukryta1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta1/bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_1/tensor_names&quot;\\n  input: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta1/kernel&quot;\\n  input: &quot;save/RestoreV2_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;ukryta2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_2/tensor_names&quot;\\n  input: &quot;save/RestoreV2_2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta2/bias&quot;\\n  input: &quot;save/RestoreV2_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_3/tensor_names&quot;\\n  input: &quot;save/RestoreV2_3/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta2/kernel&quot;\\n  input: &quot;save/RestoreV2_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;ukryta3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_4/tensor_names&quot;\\n  input: &quot;save/RestoreV2_4/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta3/bias&quot;\\n  input: &quot;save/RestoreV2_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_5/tensor_names&quot;\\n  input: &quot;save/RestoreV2_5/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta3/kernel&quot;\\n  input: &quot;save/RestoreV2_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;ukryta4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_6/tensor_names&quot;\\n  input: &quot;save/RestoreV2_6/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta4/bias&quot;\\n  input: &quot;save/RestoreV2_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_7/tensor_names&quot;\\n  input: &quot;save/RestoreV2_7/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta4/kernel&quot;\\n  input: &quot;save/RestoreV2_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;ukryta5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_8/tensor_names&quot;\\n  input: &quot;save/RestoreV2_8/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta5/bias&quot;\\n  input: &quot;save/RestoreV2_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_9/tensor_names&quot;\\n  input: &quot;save/RestoreV2_9/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;ukryta5/kernel&quot;\\n  input: &quot;save/RestoreV2_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@ukryta5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;wyjscia/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_10/tensor_names&quot;\\n  input: &quot;save/RestoreV2_10/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_10&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;wyjscia/bias&quot;\\n  input: &quot;save/RestoreV2_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_11/tensor_names&quot;\\n  input: &quot;save/RestoreV2_11/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_11&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;wyjscia/kernel&quot;\\n  input: &quot;save/RestoreV2_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@wyjscia/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n  input: &quot;^save/Assign_6&quot;\\n  input: &quot;^save/Assign_7&quot;\\n  input: &quot;^save/Assign_8&quot;\\n  input: &quot;^save/Assign_9&quot;\\n  input: &quot;^save/Assign_10&quot;\\n  input: &quot;^save/Assign_11&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdy już wiemy, które operacje będą nam potrzebne, możemy zająć się nimi przy użyciu metod `get_operation_by_name()` lub `get_tensor_by_name()` grafu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"ocena/dokladnosc:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli jesteś autorką/autorem pierwotnego modelu, możesz ułatwić życie osobom wykorzystującym jego części poprzez nadawanie zrozumiałych nazw operacjom oraz ich dokumentowanie. Innym rozwiązaniem jest stworzenie kolekcji przechowującej wszystkie istotne operacje, którymi inni użytkownicy będą chcieli się zajmować:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"moje_wazne_operacje\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W ten sposób osoby wykorzystujące Twój model mogłyby po po prostu napisać:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"moje_wazne_operacje\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy teraz rozpocząć sesję, odtworzyć stan modelu i kontynuować uczenie za pomocą danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./moj_model_ostateczny.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./moj_model_ostateczny.ckpt\")\n",
    "    # kontynuujemy uczenie modelu..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W zasadzie możemy go już przetestować!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./moj_model_ostateczny.ckpt\n",
      "0 Dokładność dla zbioru testowego: 0.961\n",
      "1 Dokładność dla zbioru testowego: 0.9608\n",
      "2 Dokładność dla zbioru testowego: 0.9621\n",
      "3 Dokładność dla zbioru testowego: 0.9611\n",
      "4 Dokładność dla zbioru testowego: 0.9641\n",
      "5 Dokładność dla zbioru testowego: 0.9652\n",
      "6 Dokładność dla zbioru testowego: 0.9663\n",
      "7 Dokładność dla zbioru testowego: 0.9628\n",
      "8 Dokładność dla zbioru testowego: 0.9664\n",
      "9 Dokładność dla zbioru testowego: 0.9669\n",
      "10 Dokładność dla zbioru testowego: 0.9665\n",
      "11 Dokładność dla zbioru testowego: 0.9669\n",
      "12 Dokładność dla zbioru testowego: 0.9677\n",
      "13 Dokładność dla zbioru testowego: 0.9681\n",
      "14 Dokładność dla zbioru testowego: 0.9688\n",
      "15 Dokładność dla zbioru testowego: 0.9685\n",
      "16 Dokładność dla zbioru testowego: 0.9693\n",
      "17 Dokładność dla zbioru testowego: 0.9702\n",
      "18 Dokładność dla zbioru testowego: 0.9676\n",
      "19 Dokładność dla zbioru testowego: 0.9684\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./moj_model_ostateczny.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_nowy_model_ostateczny.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli masz dostęp do kodu oryginalnego grafu, możesz ewentualnie użyć poniższej konstrukcji zamiast funkcji `import_meta_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"ukryta1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"ukryta2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"ukryta3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"ukryta4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"ukryta5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"wyjscia\")\n",
    "\n",
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I kontynuujemy uczenie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./moj_model_ostateczny.ckpt\n",
      "0 Dokładność dla zestawu testowego: 0.9612\n",
      "1 Dokładność dla zestawu testowego: 0.9619\n",
      "2 Dokładność dla zestawu testowego: 0.9625\n",
      "3 Dokładność dla zestawu testowego: 0.9618\n",
      "4 Dokładność dla zestawu testowego: 0.9644\n",
      "5 Dokładność dla zestawu testowego: 0.9636\n",
      "6 Dokładność dla zestawu testowego: 0.9647\n",
      "7 Dokładność dla zestawu testowego: 0.965\n",
      "8 Dokładność dla zestawu testowego: 0.9669\n",
      "9 Dokładność dla zestawu testowego: 0.968\n",
      "10 Dokładność dla zestawu testowego: 0.9678\n",
      "11 Dokładność dla zestawu testowego: 0.9679\n",
      "12 Dokładność dla zestawu testowego: 0.9687\n",
      "13 Dokładność dla zestawu testowego: 0.9686\n",
      "14 Dokładność dla zestawu testowego: 0.9683\n",
      "15 Dokładność dla zestawu testowego: 0.9692\n",
      "16 Dokładność dla zestawu testowego: 0.9678\n",
      "17 Dokładność dla zestawu testowego: 0.9695\n",
      "18 Dokładność dla zestawu testowego: 0.9688\n",
      "19 Dokładność dla zestawu testowego: 0.9707\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./moj_model_ostateczny.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Dokładność dla zestawu testowego:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_nowy_model_ostateczny.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalnie chcemy wielokrotnie wykorzystywać tylko dolne warstwy. Jeżeli korzystasz z funkcji `import_meta_graph()`, zostaje wczytany cały graf, ale może po prostu ignorować nieinteresujące Cię części. W tym przykładzie dodaliśmy nową, czwartą ukrytą na wierzchu gotowej trzeciej warstwy (i ignorując starą czwartą warstwę). Tworzymy również nową warstwę wyjściową, a wraz z nią nową funkcję straty, a także optymalizator minimalizujący wartość tej funkcji. Potrzebujemy także kolejnego obiektu `Saver` zapisującego cały graf (pełny stary graf, a także nowe operacje), a także operację inicjującą wszystkie nowe zmienne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # nowa warstwa\n",
    "n_outputs = 10  # nowa warstwa\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./moj_model_ostateczny.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"gsn/ukryta4/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"nowa_ukryta4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"nowe_wyjscia\")\n",
    "\n",
    "with tf.name_scope(\"nowa_strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "with tf.name_scope(\"nowa_ocena\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")\n",
    "\n",
    "with tf.name_scope(\"nowe_uczenie\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy wyuczyć ten model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./moj_model_ostateczny.ckpt\n",
      "0 Dokładność dla zbioru testowego: 0.9142\n",
      "1 Dokładność dla zbioru testowego: 0.9347\n",
      "2 Dokładność dla zbioru testowego: 0.9435\n",
      "3 Dokładność dla zbioru testowego: 0.9487\n",
      "4 Dokładność dla zbioru testowego: 0.9517\n",
      "5 Dokładność dla zbioru testowego: 0.9545\n",
      "6 Dokładność dla zbioru testowego: 0.9544\n",
      "7 Dokładność dla zbioru testowego: 0.956\n",
      "8 Dokładność dla zbioru testowego: 0.9588\n",
      "9 Dokładność dla zbioru testowego: 0.9619\n",
      "10 Dokładność dla zbioru testowego: 0.9617\n",
      "11 Dokładność dla zbioru testowego: 0.9617\n",
      "12 Dokładność dla zbioru testowego: 0.9625\n",
      "13 Dokładność dla zbioru testowego: 0.9641\n",
      "14 Dokładność dla zbioru testowego: 0.9628\n",
      "15 Dokładność dla zbioru testowego: 0.9641\n",
      "16 Dokładność dla zbioru testowego: 0.9665\n",
      "17 Dokładność dla zbioru testowego: 0.9668\n",
      "18 Dokładność dla zbioru testowego: 0.9674\n",
      "19 Dokładność dla zbioru testowego: 0.9689\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./moj_model_ostateczny.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./moj_nowy_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli masz dostęp do kodu architektury pierwotnego grafu, możesz wykorzystać tylko potrzebne elementy, a pozostałe usunąć:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # zbiór MNIST\n",
    "n_hidden1 = 300 # ponownie użyta\n",
    "n_hidden2 = 50  # ponownie użyta\n",
    "n_hidden3 = 50  # ponownie użyta\n",
    "n_hidden4 = 20  # nowa!\n",
    "n_outputs = 10  # nowa!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"ukryta1\")       # ponownie użyta\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"ukryta2\") # ponownie użyta\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"ukryta3\") # ponownie użyta\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"ukryta4\") # nowa!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"wyjscia\")                         # nowa!\n",
    "\n",
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")\n",
    "\n",
    "with tf.name_scope(\"uczenie\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Musimy jednak stworzyć dwa obiekty `Saver`: jeden odtwarzający gotowy model (podając mu zmienne do odtworzenia, gdyż w przeciwnym wypadku będzie \"narzekał\", że grafy nie pasują do siebie), a drugi zapisujący nowy model po jego wytrenowaniu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./moj_model_ostateczny.ckpt\n",
      "0 Dokładność dla zbioru testowego: 0.9029\n",
      "1 Dokładność dla zbioru testowego: 0.9303\n",
      "2 Dokładność dla zbioru testowego: 0.9391\n",
      "3 Dokładność dla zbioru testowego: 0.9428\n",
      "4 Dokładność dla zbioru testowego: 0.9484\n",
      "5 Dokładność dla zbioru testowego: 0.9511\n",
      "6 Dokładność dla zbioru testowego: 0.9515\n",
      "7 Dokładność dla zbioru testowego: 0.9539\n",
      "8 Dokładność dla zbioru testowego: 0.9547\n",
      "9 Dokładność dla zbioru testowego: 0.9571\n",
      "10 Dokładność dla zbioru testowego: 0.9599\n",
      "11 Dokładność dla zbioru testowego: 0.9599\n",
      "12 Dokładność dla zbioru testowego: 0.9605\n",
      "13 Dokładność dla zbioru testowego: 0.9616\n",
      "14 Dokładność dla zbioru testowego: 0.9619\n",
      "15 Dokładność dla zbioru testowego: 0.9637\n",
      "16 Dokładność dla zbioru testowego: 0.9631\n",
      "17 Dokładność dla zbioru testowego: 0.9645\n",
      "18 Dokładność dla zbioru testowego: 0.9652\n",
      "19 Dokładność dla zbioru testowego: 0.9658\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"ukryta[123]\") # wyrażenie regularne\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # odtwarza warstwy 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./moj_model_ostateczny.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                      # nieukazane w książce\n",
    "        for iteration in range(mnist.train.num_examples // batch_size): # nieukazane\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)      # nieukazane\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})  # nieukazane\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,  # nieukazane\n",
    "                                                y: mnist.test.labels}) # nieukazane\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", accuracy_val)                   # nieukazane\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_nowy_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystywanie modeli utworzonych w innych środowiskach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym przykładzie znajdujemy operację przydzielania inicjatora dla każdej zmiennej, którą chcemy ponownie wykorzystać, a także otrzymujemy jej drugą daną wejściową, co odpowiada wartości inicjacji. Po uruchomieniu inicjatora zastępujemy wartości inicjacji wybranymi przez nas za pomocą obiektu `feed_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  61.   83.  105.]]\n"
     ]
    }
   ],
   "source": [
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Wczytuje wagi z innego środowiska\n",
    "original_b = [7., 8., 9.]                 # Wczytuje obciążenia z innego środowiska\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"ukryta1\")\n",
    "# [...] Pozostała część modelu\n",
    "\n",
    "# Wstawia węzły przydzielania dla zmiennych warstwy ukryta1\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"ukryta1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"ukryta1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] Uczenie modelu za pomocą Twoich danych\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # nieukazane w książce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga: zmienna wag stworzona przez funkcję `tf.layers.dense()` nosi nazwę `\"kernel\"` (a nie `\"weights\"`, tak jak to było w przypadku funkcji `tf.contrib.layers.fully_connected()` opisywanej w książce), natomiast zmienna obciążenia została przemianowana z `biases` na `bias`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Innym rozwiązaniem (początkowo było opisywane w książce) byłoby stworzenie wyspecjalizowanych węzłów przydzielania oraz węzłów zastępczych. Jest to bardziej rozwlekła i mniej efektywna metoda, ale możesz uznać ją za bardziej jednoznaczną:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  61.   83.  105.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Wczytuje wagi z innego środowiska\n",
    "original_b = [7., 8., 9.]                 # Wczytuje obciążenia z innego środowiska\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"ukryta1\")\n",
    "# [...] Pozostała część modelu\n",
    "\n",
    "# Wstawia węzły przydzielania dla zmiennych warstwy ukryta1\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # główny zakres\n",
    "    hidden1_weights = tf.get_variable(\"ukryta1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"ukryta1/bias\")\n",
    "\n",
    "# Tworzy wyspecjalizowane węzły zastępcze i węzły przydzielania\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] Uczenie modelu za pomocą Twoich danych\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że moglibyśmy zająć się zmiennymi za pomocą funkcji `get_collection()` i wyznaczając zakres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'ukryta1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'ukryta1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"ukryta1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ewentualnie możemy użyć metody `get_tensor_by_name()` grafu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ukryta1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"ukryta1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ukryta1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"ukryta1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zamrażanie niższych warstw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # zbiór MNIST\n",
    "n_hidden1 = 300 # ponownie wykorzystana\n",
    "n_hidden2 = 50  # ponownie wykorzystana\n",
    "n_hidden3 = 50  # ponownie wykorzystana\n",
    "n_hidden4 = 20  # nowa!\n",
    "n_outputs = 10  # nowa!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"ukryta1\")       # ponownie wykorzystana\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"ukryta2\") # ponownie wykorzystana\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"ukryta3\") # ponownie wykorzystana\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"ukryta4\") # nowa!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"wyjscia\")                         # nowa!\n",
    "\n",
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"uczenie\"):                                         # nieukazane w książce\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # nieukazane\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"ukryta[34]|wyjscia\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./moj_model_ostateczny.ckpt\n",
      "0 Dokładność dla zbioru testowego: 0.8988\n",
      "1 Dokładność dla zbioru testowego: 0.931\n",
      "2 Dokładność dla zbioru testowego: 0.937\n",
      "3 Dokładność dla zbioru testowego: 0.9415\n",
      "4 Dokładność dla zbioru testowego: 0.9435\n",
      "5 Dokładność dla zbioru testowego: 0.9479\n",
      "6 Dokładność dla zbioru testowego: 0.9494\n",
      "7 Dokładność dla zbioru testowego: 0.9523\n",
      "8 Dokładność dla zbioru testowego: 0.9518\n",
      "9 Dokładność dla zbioru testowego: 0.9521\n",
      "10 Dokładność dla zbioru testowego: 0.9538\n",
      "11 Dokładność dla zbioru testowego: 0.9538\n",
      "12 Dokładność dla zbioru testowego: 0.9537\n",
      "13 Dokładność dla zbioru testowego: 0.9547\n",
      "14 Dokładność dla zbioru testowego: 0.9537\n",
      "15 Dokładność dla zbioru testowego: 0.9551\n",
      "16 Dokładność dla zbioru testowego: 0.9552\n",
      "17 Dokładność dla zbioru testowego: 0.9551\n",
      "18 Dokładność dla zbioru testowego: 0.9551\n",
      "19 Dokładność dla zbioru testowego: 0.9556\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"ukryta[123]\") # wyrażenie regularne\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # odtwarza warstwy 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./moj_model_ostateczny.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_nowy_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # Zbiór MNIST\n",
    "n_hidden1 = 300 # ponownie użyta\n",
    "n_hidden2 = 50  # ponownie użyta\n",
    "n_hidden3 = 50  # ponownie użyta\n",
    "n_hidden4 = 20  # nowa!\n",
    "n_outputs = 10  # nowa!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"ukryta1\") # zamrożona ponownie użyta\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"ukryta2\") # zamrożona ponownie użyta\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"ukryta3\") # ponownie użyta, niezamrożona\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"ukryta4\") # nowa!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"wyjscia\") # nowa!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")\n",
    "\n",
    "with tf.name_scope(\"uczenie\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorytm uczenia modelu wygląda dokładnie tak samo, jak poprzednio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./moj_model_ostateczny.ckpt\n",
      "0 Dokładność dla zestawu testowego: 0.9037\n",
      "1 Dokładność dla zestawu testowego: 0.932\n",
      "2 Dokładność dla zestawu testowego: 0.9399\n",
      "3 Dokładność dla zestawu testowego: 0.9434\n",
      "4 Dokładność dla zestawu testowego: 0.9471\n",
      "5 Dokładność dla zestawu testowego: 0.949\n",
      "6 Dokładność dla zestawu testowego: 0.95\n",
      "7 Dokładność dla zestawu testowego: 0.9491\n",
      "8 Dokładność dla zestawu testowego: 0.9517\n",
      "9 Dokładność dla zestawu testowego: 0.952\n",
      "10 Dokładność dla zestawu testowego: 0.953\n",
      "11 Dokładność dla zestawu testowego: 0.9536\n",
      "12 Dokładność dla zestawu testowego: 0.953\n",
      "13 Dokładność dla zestawu testowego: 0.9534\n",
      "14 Dokładność dla zestawu testowego: 0.9525\n",
      "15 Dokładność dla zestawu testowego: 0.9535\n",
      "16 Dokładność dla zestawu testowego: 0.953\n",
      "17 Dokładność dla zestawu testowego: 0.9548\n",
      "18 Dokładność dla zestawu testowego: 0.9548\n",
      "19 Dokładność dla zestawu testowego: 0.9555\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"ukryta[123]\") # wyrażenie regularne\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # odtwarza warstwy 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./moj_model_ostateczny.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Dokładność dla zestawu testowego:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_nowy_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buforowanie zamrożonych warstw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # zbiór MNIST\n",
    "n_hidden1 = 300 # ponownie użyta\n",
    "n_hidden2 = 50  # ponownie użyta\n",
    "n_hidden3 = 50  # ponownie użyta\n",
    "n_hidden4 = 20  # nowa!\n",
    "n_outputs = 10  # nowa!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"ukryta1\") # zamrożona ponownie użyta\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"ukryta2\") # ponownie użyta, zamrożona i zbuforowana\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"ukryta3\") # ponownie użyta, niezamrożona\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"ukryta4\") # nowa!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"wyjscia\") # nowa!\n",
    "\n",
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")\n",
    "\n",
    "with tf.name_scope(\"uczenie\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"ukryta[123]\") # wyrażenie regularne\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # odtwarza warstwy 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./moj_model_ostateczny.ckpt\n",
      "0 Dokładność dla zbioru testowego: 0.9026\n",
      "1 Dokładność dla zbioru testowego: 0.932\n",
      "2 Dokładność dla zbioru testowego: 0.942\n",
      "3 Dokładność dla zbioru testowego: 0.945\n",
      "4 Dokładność dla zbioru testowego: 0.9472\n",
      "5 Dokładność dla zbioru testowego: 0.9476\n",
      "6 Dokładność dla zbioru testowego: 0.9509\n",
      "7 Dokładność dla zbioru testowego: 0.9507\n",
      "8 Dokładność dla zbioru testowego: 0.9516\n",
      "9 Dokładność dla zbioru testowego: 0.9525\n",
      "10 Dokładność dla zbioru testowego: 0.9513\n",
      "11 Dokładność dla zbioru testowego: 0.9524\n",
      "12 Dokładność dla zbioru testowego: 0.9523\n",
      "13 Dokładność dla zbioru testowego: 0.9539\n",
      "14 Dokładność dla zbioru testowego: 0.9537\n",
      "15 Dokładność dla zbioru testowego: 0.9534\n",
      "16 Dokładność dla zbioru testowego: 0.9549\n",
      "17 Dokładność dla zbioru testowego: 0.9538\n",
      "18 Dokładność dla zbioru testowego: 0.9541\n",
      "19 Dokładność dla zbioru testowego: 0.9548\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./moj_model_ostateczny.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: mnist.train.images})\n",
    "    h2_cache_test = sess.run(hidden2, feed_dict={X: mnist.test.images}) # nieukazane w książce\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(mnist.train.num_examples)\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(mnist.train.labels[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_test, # nieukazane\n",
    "                                                y: mnist.test.labels})  # nieukazane\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", accuracy_val)                    # nieukazane\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_nowy_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Szybsze optymalizatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optymalizacja momentowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorytm Nesterova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optymalizacja Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmonogramowanie współczynnika uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # Zbiór MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"ukryta1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"ukryta2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"wyjscia\")\n",
    "\n",
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"uczenie\"):       # nieukazane w książce\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"etap_globalny\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Dokładność dla zbioru testowego: 0.9579\n",
      "1 Dokładność dla zbioru testowego: 0.9686\n",
      "2 Dokładność dla zbioru testowego: 0.975\n",
      "3 Dokładność dla zbioru testowego: 0.9791\n",
      "4 Dokładność dla zbioru testowego: 0.9801\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularyzacja jako sposób na unikanie przetrenowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularyzacja $\\ell_1$ i $\\ell_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimlementujmy ręcznie regularyzację $\\ell_1$. Najpierw stworzymy, jak zwykle, model (tym razem dla uproszczenia wprowadzimy tylko jedną warstwę ukrytą):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # Zbiór MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"ukryta1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"wyjscia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz zajmiemy się wagami warstwy i obliczymy całkowitą funkcję straty, która jest równa sumie standardowej entropii krzyżowej i straty $\\ell_1$ (tj. wartości bezwzględne wag):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"ukryta1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"wyjscia/kernel:0\")\n",
    "\n",
    "scale = 0.001 # hiperparametr regularyzacji L1\n",
    "\n",
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"srednia_entropia_krzyz\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"strata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pozostała część kodu wygląda tak samo, jak zawsze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"uczenie\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Dokładność dla zbioru testowego: 0.8343\n",
      "1 Dokładność dla zbioru testowego: 0.8726\n",
      "2 Dokładność dla zbioru testowego: 0.8832\n",
      "3 Dokładność dla zbioru testowego: 0.8899\n",
      "4 Dokładność dla zbioru testowego: 0.8959\n",
      "5 Dokładność dla zbioru testowego: 0.8986\n",
      "6 Dokładność dla zbioru testowego: 0.9011\n",
      "7 Dokładność dla zbioru testowego: 0.9032\n",
      "8 Dokładność dla zbioru testowego: 0.9046\n",
      "9 Dokładność dla zbioru testowego: 0.9047\n",
      "10 Dokładność dla zbioru testowego: 0.9065\n",
      "11 Dokładność dla zbioru testowego: 0.9059\n",
      "12 Dokładność dla zbioru testowego: 0.9072\n",
      "13 Dokładność dla zbioru testowego: 0.9072\n",
      "14 Dokładność dla zbioru testowego: 0.9069\n",
      "15 Dokładność dla zbioru testowego: 0.9071\n",
      "16 Dokładność dla zbioru testowego: 0.9064\n",
      "17 Dokładność dla zbioru testowego: 0.9071\n",
      "18 Dokładność dla zbioru testowego: 0.9068\n",
      "19 Dokładność dla zbioru testowego: 0.9063\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ewentualnie możemy przekazać funkcję regularyzacji do funkcji `tf.layers.dense()`, co spowoduje utworzenie operacji obliczających funkcję straty regularyzacji, które zostaną dodane do kolekcji strat regularyzacji. Początkowy fragment kodu wygląda tak samo, jak powyżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # Zbiór MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz używamy funkcji `partial()` w celu uniknięcia ciągłego powtarzania tych samych argumentów. Zwróć uwagę, że wyznaczamy argument `kernel_regularizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"ukryta1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"ukryta2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"wyjscia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie musimy dodać stratę regularyzacji do bazowej funkcji straty: Next we must add the regularization losses to the base loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"strata\"):                                     # nieukazane w książce\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # nieukazane\n",
    "        labels=y, logits=logits)                                # nieukazane\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"srednia_entropia_krzyz\")   # nieukazane\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"strata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pozostała część kodu jest nam już dobrze znana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"uczenie\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Dokładność dla zbioru testowego: 0.8298\n",
      "1 Dokładność dla zbioru testowego: 0.8778\n",
      "2 Dokładność dla zbioru testowego: 0.8917\n",
      "3 Dokładność dla zbioru testowego: 0.9017\n",
      "4 Dokładność dla zbioru testowego: 0.9068\n",
      "5 Dokładność dla zbioru testowego: 0.9103\n",
      "6 Dokładność dla zbioru testowego: 0.9125\n",
      "7 Dokładność dla zbioru testowego: 0.9137\n",
      "8 Dokładność dla zbioru testowego: 0.9148\n",
      "9 Dokładność dla zbioru testowego: 0.9174\n",
      "10 Dokładność dla zbioru testowego: 0.9176\n",
      "11 Dokładność dla zbioru testowego: 0.9184\n",
      "12 Dokładność dla zbioru testowego: 0.9191\n",
      "13 Dokładność dla zbioru testowego: 0.9183\n",
      "14 Dokładność dla zbioru testowego: 0.9195\n",
      "15 Dokładność dla zbioru testowego: 0.9201\n",
      "16 Dokładność dla zbioru testowego: 0.918\n",
      "17 Dokładność dla zbioru testowego: 0.9184\n",
      "18 Dokładność dla zbioru testowego: 0.9181\n",
      "19 Dokładność dla zbioru testowego: 0.9174\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metoda porzucania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga: w książce jest używana funkcja `tf.contrib.layers.dropout()` zamiast funkcji `tf.layers.dropout()` (która nie istniała w czasie pisania tego rozdziału). Zalecane jest korzystanie z funkcji `tf.layers.dropout()`, ponieważ wszystkie składniki modułu `contrib` mogą być modyfikowane lub usuwane bez zapowiedzi. Funkcja `tf.layers.dropout()` niemal niczym nie różni się od funkcji `tf.contrib.layers.dropout()`, nie licząc kilku niewielkich zmian. Najważniejsze z nich to:\n",
    "* musisz określić współczynnik porzucania (`rate`), a nie prawdopodobieństwo pozostawiania neuronów (`keep_prob`), gdzie `rate` to po prostu `1 - keep_prob`,\n",
    "* parametr `is_training` został przemianowany na `training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='uczenie')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"ukryta1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"ukryta2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"wyjscia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "with tf.name_scope(\"uczenie\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Dokładność dla zbioru testowego: 0.9205\n",
      "1 Dokładność dla zbioru testowego: 0.9418\n",
      "2 Dokładność dla zbioru testowego: 0.948\n",
      "3 Dokładność dla zbioru testowego: 0.9513\n",
      "4 Dokładność dla zbioru testowego: 0.9544\n",
      "5 Dokładność dla zbioru testowego: 0.9567\n",
      "6 Dokładność dla zbioru testowego: 0.9602\n",
      "7 Dokładność dla zbioru testowego: 0.9588\n",
      "8 Dokładność dla zbioru testowego: 0.9597\n",
      "9 Dokładność dla zbioru testowego: 0.9659\n",
      "10 Dokładność dla zbioru testowego: 0.9644\n",
      "11 Dokładność dla zbioru testowego: 0.9629\n",
      "12 Dokładność dla zbioru testowego: 0.9664\n",
      "13 Dokładność dla zbioru testowego: 0.9659\n",
      "14 Dokładność dla zbioru testowego: 0.965\n",
      "15 Dokładność dla zbioru testowego: 0.9655\n",
      "16 Dokładność dla zbioru testowego: 0.9691\n",
      "17 Dokładność dla zbioru testowego: 0.9668\n",
      "18 Dokładność dla zbioru testowego: 0.9709\n",
      "19 Dokładność dla zbioru testowego: 0.9683\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_model_ostateczny.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularyzacja typu max-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wróćmy do naszej nieskomplikowanej sieci neuronowej zawierajacej jedynie dwie warstwy ukryte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"ukryta1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"ukryta2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"wyjscia\")\n",
    "\n",
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "with tf.name_scope(\"uczenie\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zajmijmy się teraz wagą pierwszej warstwy ukrytej i stwórzmy operację obliczającą obcięte wagi za pomocą funkcji `clip_by_norm()`. Następnie utworzymy operację przydzielającą obcięte wagi do zmiennej wag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"ukryta1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy to samo zrobić dla drugiej warstwy ukrytej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"ukryta2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodajmy inicjator i obiekt `Saver`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możemy wyuczyć model. Kod wygląda niemal tak samo, jak zawsze, jedynie tuż po rozpoczęciu operacji `training_op` uruchamiamy operacje `clip_weights` i `clip_weights2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Dokładność dla zbioru testowego: 0.9517\n",
      "1 Dokładność dla zbioru testowego: 0.967\n",
      "2 Dokładność dla zbioru testowego: 0.9709\n",
      "3 Dokładność dla zbioru testowego: 0.9745\n",
      "4 Dokładność dla zbioru testowego: 0.9745\n",
      "5 Dokładność dla zbioru testowego: 0.9774\n",
      "6 Dokładność dla zbioru testowego: 0.9776\n",
      "7 Dokładność dla zbioru testowego: 0.9794\n",
      "8 Dokładność dla zbioru testowego: 0.9785\n",
      "9 Dokładność dla zbioru testowego: 0.9803\n",
      "10 Dokładność dla zbioru testowego: 0.9811\n",
      "11 Dokładność dla zbioru testowego: 0.981\n",
      "12 Dokładność dla zbioru testowego: 0.9811\n",
      "13 Dokładność dla zbioru testowego: 0.9804\n",
      "14 Dokładność dla zbioru testowego: 0.9813\n",
      "15 Dokładność dla zbioru testowego: 0.9816\n",
      "16 Dokładność dla zbioru testowego: 0.9832\n",
      "17 Dokładność dla zbioru testowego: 0.9825\n",
      "18 Dokładność dla zbioru testowego: 0.9816\n",
      "19 Dokładność dla zbioru testowego: 0.9819\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:                                              # nieukazane w książce\n",
    "    init.run()                                                          # nieukazane\n",
    "    for epoch in range(n_epochs):                                       # nieukazane\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):  # nieukazane\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)       # nieukazane\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        # nieukazane\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,       # nieukazane\n",
    "                                            y: mnist.test.labels})      # nieukazane\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", acc_test)                        # nieukazane\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_model_ostateczny.ckpt\")               # nieukazane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyższa implementacja jest zrozumiała i spisuje się nieźle, ale jednocześnie nieco zagmatwana. Lepszym rozwiązaniem jest zdefiniowanie funkcji `max_norm_regularizer()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # nie ma członu straty regularyzacji\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie możemy wywołać tę funkcję, aby uzyskać regularyzator typu max-norm (z dowolnie wybranym przez nas progiem). Podczas tworzenia warstwy ukrytej możemy przekazać ten regularyzator argumentowi `kernel_regularizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"ukryta1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"ukryta2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"wyjscia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"strata\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "with tf.name_scope(\"uczenie\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"ocena\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proces uczenia przebiega po staremu, tyle że musimy po każdej operacji uczenia uruchomić operacje obcinania wag: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Dokładność dla zbioru testowego: 0.9534\n",
      "1 Dokładność dla zbioru testowego: 0.9655\n",
      "2 Dokładność dla zbioru testowego: 0.9701\n",
      "3 Dokładność dla zbioru testowego: 0.9743\n",
      "4 Dokładność dla zbioru testowego: 0.9755\n",
      "5 Dokładność dla zbioru testowego: 0.9765\n",
      "6 Dokładność dla zbioru testowego: 0.9758\n",
      "7 Dokładność dla zbioru testowego: 0.9789\n",
      "8 Dokładność dla zbioru testowego: 0.9758\n",
      "9 Dokładność dla zbioru testowego: 0.978\n",
      "10 Dokładność dla zbioru testowego: 0.9797\n",
      "11 Dokładność dla zbioru testowego: 0.9803\n",
      "12 Dokładność dla zbioru testowego: 0.9799\n",
      "13 Dokładność dla zbioru testowego: 0.9806\n",
      "14 Dokładność dla zbioru testowego: 0.9808\n",
      "15 Dokładność dla zbioru testowego: 0.9808\n",
      "16 Dokładność dla zbioru testowego: 0.9811\n",
      "17 Dokładność dla zbioru testowego: 0.9808\n",
      "18 Dokładność dla zbioru testowego: 0.981\n",
      "19 Dokładność dla zbioru testowego: 0.9813\n"
     ]
    }
   ],
   "source": [
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,     # nieukazane w książce\n",
    "                                            y: mnist.test.labels})    # nieukazane\n",
    "        print(epoch, \"Dokładność dla zbioru testowego:\", acc_test)                      # nieukazane\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_model_ostateczny.ckpt\")             # nieukazane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Rozwiązania ćwiczeń"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. do 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patrz dodatek A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Uczenie głębokie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ćwiczenie: Stwórz sieć GSN składającą się z pięciu warstw ukrytych po 100 neuronów w każdej, inicjacji He'ego i funkcji aktywacji ELU._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy potrzebować podobne sieci neuronowe w następnych ćwiczeniach, stwórzmy zatem funkcję odpowiedzialną za jej generowanie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n",
    "        activation=tf.nn.elu, initializer=he_init):\n",
    "    with tf.variable_scope(name, \"gsn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     name=\"ukryta%d\" % (layer + 1))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28 # zbiór MNIST\n",
    "n_outputs = 5\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logity\")\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_prawd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ćwiczenie: Korzystając z optymalizacji Adam i metody wczesnego zatrzymywania spróbuj wyuczyć tę sieć wobec zbioru danych MNIST, ale wyłącznie w zakresie cyfr 0 – 4, ponieważ będziemy korzystać za chwilę z uczenia transferowego na cyfry 5 – 9. Będziesz potrzebować warstwy wyjściowej składającej się z pięciu neuronów i zawierającej funkcję softmax; jak zwykle nie zapomnij zapisywać punktów kontrolnych w regularnych odstępach czasu i zachować modelu ostatecznego modelu, że móc z niego później skorzystać._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uzupełnijmy graf o funkcję kosztu, operację uczenia i pozostałe składniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss, name=\"operacja_uczenia\")\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytajmy zestaw danych MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/dane/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/dane/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/dane/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/dane/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/dane/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy teraz podzbiory uczący, walidacyjny i testowy (zestaw walidacyjny będzie nam potrzebny do implementacji wczesnego zatrzymywania):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tF. straty dla zbioru walidacyjnego: 0.190826\tNajlepsza wartość straty: 0.190826\tDokładność: 96.64%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.689649\tNajlepsza wartość straty: 0.190826\tDokładność: 18.73%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.660114\tNajlepsza wartość straty: 0.190826\tDokładność: 20.91%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.778077\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.667106\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.654532\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.680933\tNajlepsza wartość straty: 0.190826\tDokładność: 18.73%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.779077\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.699482\tNajlepsza wartość straty: 0.190826\tDokładność: 19.27%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.767771\tNajlepsza wartość straty: 0.190826\tDokładność: 20.91%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.629350\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.812643\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.675939\tNajlepsza wartość straty: 0.190826\tDokładność: 18.73%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.633259\tNajlepsza wartość straty: 0.190826\tDokładność: 20.91%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.652904\tNajlepsza wartość straty: 0.190826\tDokładność: 20.91%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.635943\tNajlepsza wartość straty: 0.190826\tDokładność: 20.91%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.718915\tNajlepsza wartość straty: 0.190826\tDokładność: 19.08%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.682456\tNajlepsza wartość straty: 0.190826\tDokładność: 19.27%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.675366\tNajlepsza wartość straty: 0.190826\tDokładność: 18.73%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.645805\tNajlepsza wartość straty: 0.190826\tDokładność: 19.08%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.722336\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "Wczesne zatrzymywanie!\n",
      "INFO:tensorflow:Restoring parameters from ./moj_model_mnist_0_do_4.ckpt\n",
      "Ostateczna dokładność dla zbioru testowego: 97.08%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, \"./moj_model_mnist_0_do_4.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Wczesne zatrzymywanie!\")\n",
    "                break\n",
    "        print(\"{}\\tF. straty dla zbioru walidacyjnego: {:.6f}\\tNajlepsza wartość straty: {:.6f}\\tDokładność: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./moj_model_mnist_0_do_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Ostateczna dokładność dla zbioru testowego: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uzyskujemy 97.08% dokładności wobec zbioru testowego. Nieźle, ale sprawdźmy, czy uzyskamy lepszy rezultat poprzez strojenie hiperparametrów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ćwiczenie: Dobierz optymalne wartości hiperparametrów za pomocą sprawdzianu krzyżowego i sprawdź, jaką precyzję może osiągnąć Twój model._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy klasę `DNNClassifier` kompatybilną z klasą `RandomizedSearchCV`, która będzie przeprowadzać strojenie hiperparametrów. Poniżej przedstawiam najważniejsze cechy tej implementacji:\n",
    "* metoda (konstruktor) `__init__()` tworzy jedynie zmienne przebiegu dla każdego hiperparametru,\n",
    "* metoda `fit()` tworzy graf, rozpoczyna sesję i przeprowadza uczenie modelu:\n",
    "  * wywołuje metodę `_build_graph()` konstruującą graf (w podobny sposób, jak w definiowanym wcześniej grafie). Po utworzeniu grafu zostają zapisane wszystkie ważne operacje w postaci zmiennych przebiegu po to, aby inne metody miały do nich łatwy dostęp. \n",
    "  * metoda `_dnn()` tworzy warstwy ukryte, podobnie jak powyższa funkcja `dnn()`, zawiera ona jednak dodatkowo obsługę normalizacji wsadowej i porzucania (przygotowane do kolejnych ćwiczeń),\n",
    "  * jeśli dostarczymy metodzie `fit()` zbiór walidacyjny (`X_valid` i `y_valid`), to zostanie zaimplementowane wczesne zatrzymywanie. Implementacja ta nie zachowuje najlepszego modelu na dysku, ale w pamięci operacyjnej: wykorzystywana jest tu metoda `_get_model_params()` do pobrania wszystkich zmiennych grafu i ich wartości, natomiast metoda `_restore_model_params()` pozwala odtwarzać wartości zmiennych (najlepszego znalezionego modelu). Ta sztuczka służy przyśpieszeniu procesu uczenia.\n",
    "  * po zakończeniu uczenia modelu przez metodę `fit()`, sesja pozostaje otwarta, dzięki czemu można szybko uzyskać prognozy bez konieczności zapisywania modelu i odtwarzania go dla każdek prognozy. Możemy zamknąć sesję wywołując metodę `close_session()`.\n",
    "* metoda `predict_proba()` wykorzystuje wytrenowany model do przewidywania prawdopodobieństwa przynależności do poszczególnych klas.\n",
    "* Metoda `predict()` wywołuje metodę `predict_proba()` i dla każdej próbki zwraca klasę o najwyższym prawdopodobieństwie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        \"\"\"Inicjuje klasę DNNClassifier poprzez przechowywanie wszystkich hiperparametrów.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Tworzy warstwy ukryte z obsługą normalizacji wsadowej i metody porzucania.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"ukryta%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"ukryta%d_wyj\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Tworzy taki sam model, jak wcześniej\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='uczenie')\n",
    "        else:\n",
    "            self._training = None\n",
    "\n",
    "        dnn_outputs = self._dnn(X)\n",
    "\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logity\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_prawd\")\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                                  logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Udostępnia  ważne operacje poprzez zmienne przebiegu\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Pobiera wartości wszystkich zmiennych (używane we wczesnym zatrzymywaniu, \n",
    "        metoda ta jest szybsza od zapisywania na dysk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Przydziela zmienne do określonych wartości (wykorzystywane we wczesnym zatrzymywaniu,\n",
    "        szybsze od wczytywania z dysku)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        \"\"\"Dopasowuje model do zbioru uczącego. Jeśli zostają dostarczone zmienne X_valid i y_valid\n",
    "        model korzysta ze wczesnego zatrzymywania.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # wyznacza zmienne n_inputs i n_outputs ze zbioru uczącego.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Przekształca wektor etykiet w wektor posortowanych indeksów klas zawierający\n",
    "        # wartości stałoprzecinkowe w zakresie od 0 do n_outputs - 1.\n",
    "        # Na przykład, jeśli wektor y jest równy [8, 8, 9, 5, 7, 6, 6, 6], to posortowane etykiety\n",
    "        # klas (self.classes_) będą równe [5, 6, 7, 8, 9], a wektor etykiet zostanie przekształcony\n",
    "        # do postaci [3, 3, 4, 0, 2, 1, 1, 1]\n",
    "        self.class_to_index_ = {label: index\n",
    "                                for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # dodatkowe operacje wykorzystywane w normalizacji wsadowej\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        # wymagane w przypadku wczesnego zatrzymywania\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        # Trenowanie modelu!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\tF. straty dla zbioru walidacyjnego: {:.6f}\\tNajlepsza wartość straty: {:.6f}\\tDokładność: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Wczesne zatrzymywanie!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch,\n",
    "                                                                self._y: y_batch})\n",
    "                    print(\"{}\\tF. straty ostatniej mini-grupy uczącej: {:.6f}\\tDokładność: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # Jeśli skorzystaliśmy z wczesnego zatrzymywania, to cofa się do najlepszego znalezionego modelu\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"Ta %s próbka nie została jeszcze dopasowana\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy, czy uzyskamy dokładnie taką samą dokładność, jak wcześniej, przy użyciu tej klasy (bez porzucania ani normalizacji wsadowej):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tF. straty dla zbioru walidacyjnego: 0.190826\tNajlepsza wartość straty: 0.190826\tDokładność: 96.64%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.689649\tNajlepsza wartość straty: 0.190826\tDokładność: 18.73%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.660114\tNajlepsza wartość straty: 0.190826\tDokładność: 20.91%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.778077\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.667106\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.654532\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.680933\tNajlepsza wartość straty: 0.190826\tDokładność: 18.73%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.779077\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.699482\tNajlepsza wartość straty: 0.190826\tDokładność: 19.27%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.767771\tNajlepsza wartość straty: 0.190826\tDokładność: 20.91%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.629350\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.812643\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.675939\tNajlepsza wartość straty: 0.190826\tDokładność: 18.73%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.633259\tNajlepsza wartość straty: 0.190826\tDokładność: 20.91%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.652904\tNajlepsza wartość straty: 0.190826\tDokładność: 20.91%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.635943\tNajlepsza wartość straty: 0.190826\tDokładność: 20.91%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.718915\tNajlepsza wartość straty: 0.190826\tDokładność: 19.08%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.682456\tNajlepsza wartość straty: 0.190826\tDokładność: 19.27%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.675366\tNajlepsza wartość straty: 0.190826\tDokładność: 18.73%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.645805\tNajlepsza wartość straty: 0.190826\tDokładność: 19.08%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.722336\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.656422\tNajlepsza wartość straty: 0.190826\tDokładność: 22.01%\n",
      "Wczesne zatrzymywanie!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function elu at 0x0000029B128289D8>,\n",
       "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x0000029B185681E0>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = DNNClassifier(random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model jest wyuczony. Sprawdźmy, czy uzyskamy taką samą dokładność jak wcześniej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97081144191476942"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak! Działa jak należy. Możemy teraz skorzystać z klasy `RandomizedSearchCV`, aby wyszukać lepsze hiperparametry (proces ten może zająć ponad godzinę w zależności od systemu):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.131135\tNajlepsza wartość straty: 0.131135\tDokładność: 96.36%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.130872\tNajlepsza wartość straty: 0.130872\tDokładność: 96.17%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.153402\tNajlepsza wartość straty: 0.130872\tDokładność: 96.36%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.145776\tNajlepsza wartość straty: 0.130872\tDokładność: 96.76%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.132235\tNajlepsza wartość straty: 0.130872\tDokładność: 96.87%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.140970\tNajlepsza wartość straty: 0.130872\tDokładność: 96.99%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.150057\tNajlepsza wartość straty: 0.130872\tDokładność: 96.25%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.138547\tNajlepsza wartość straty: 0.130872\tDokładność: 96.64%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.129539\tNajlepsza wartość straty: 0.129539\tDokładność: 96.05%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.131695\tNajlepsza wartość straty: 0.129539\tDokładność: 96.44%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.135688\tNajlepsza wartość straty: 0.129539\tDokładność: 96.68%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.787319\tNajlepsza wartość straty: 0.129539\tDokładność: 19.12%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.630504\tNajlepsza wartość straty: 0.129539\tDokładność: 20.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.644414\tNajlepsza wartość straty: 0.129539\tDokładność: 18.73%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.617375\tNajlepsza wartość straty: 0.129539\tDokładność: 22.01%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.611893\tNajlepsza wartość straty: 0.129539\tDokładność: 22.01%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.612366\tNajlepsza wartość straty: 0.129539\tDokładność: 20.91%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.616471\tNajlepsza wartość straty: 0.129539\tDokładność: 19.27%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.616871\tNajlepsza wartość straty: 0.129539\tDokładność: 19.27%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.624991\tNajlepsza wartość straty: 0.129539\tDokładność: 22.01%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.626455\tNajlepsza wartość straty: 0.129539\tDokładność: 19.08%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.632699\tNajlepsza wartość straty: 0.129539\tDokładność: 19.27%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.620706\tNajlepsza wartość straty: 0.129539\tDokładność: 18.73%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.613954\tNajlepsza wartość straty: 0.129539\tDokładność: 20.91%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.626613\tNajlepsza wartość straty: 0.129539\tDokładność: 22.01%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.620113\tNajlepsza wartość straty: 0.129539\tDokładność: 22.01%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.643669\tNajlepsza wartość straty: 0.129539\tDokładność: 18.73%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.622781\tNajlepsza wartość straty: 0.129539\tDokładność: 19.27%\n",
      "28\tF. straty dla zbioru walidacyjnego: 1.624650\tNajlepsza wartość straty: 0.129539\tDokładność: 19.27%\n",
      "29\tF. straty dla zbioru walidacyjnego: 1.615069\tNajlepsza wartość straty: 0.129539\tDokładność: 22.01%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=   7.1s\n",
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tF. straty dla zbioru walidacyjnego: 0.183487\tNajlepsza wartość straty: 0.183487\tDokładność: 95.93%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.106714\tNajlepsza wartość straty: 0.106714\tDokładność: 96.99%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.150361\tNajlepsza wartość straty: 0.106714\tDokładność: 95.78%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.127387\tNajlepsza wartość straty: 0.106714\tDokładność: 96.64%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.109397\tNajlepsza wartość straty: 0.106714\tDokładność: 96.99%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.132421\tNajlepsza wartość straty: 0.106714\tDokładność: 96.29%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.125783\tNajlepsza wartość straty: 0.106714\tDokładność: 96.44%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.242102\tNajlepsza wartość straty: 0.106714\tDokładność: 94.88%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.612336\tNajlepsza wartość straty: 0.106714\tDokładność: 19.27%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.618384\tNajlepsza wartość straty: 0.106714\tDokładność: 22.01%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.614465\tNajlepsza wartość straty: 0.106714\tDokładność: 19.27%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.625938\tNajlepsza wartość straty: 0.106714\tDokładność: 22.01%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.624183\tNajlepsza wartość straty: 0.106714\tDokładność: 20.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.636243\tNajlepsza wartość straty: 0.106714\tDokładność: 19.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.623178\tNajlepsza wartość straty: 0.106714\tDokładność: 22.01%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.617793\tNajlepsza wartość straty: 0.106714\tDokładność: 22.01%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.615209\tNajlepsza wartość straty: 0.106714\tDokładność: 23.38%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.204490\tNajlepsza wartość straty: 0.106714\tDokładność: 38.62%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.181334\tNajlepsza wartość straty: 0.106714\tDokładność: 40.38%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.447687\tNajlepsza wartość straty: 0.106714\tDokładność: 27.76%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.668693\tNajlepsza wartość straty: 0.106714\tDokładność: 20.64%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.301085\tNajlepsza wartość straty: 0.106714\tDokładność: 40.15%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.291410\tNajlepsza wartość straty: 0.106714\tDokładność: 40.30%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=   5.8s\n",
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.149692\tNajlepsza wartość straty: 0.149692\tDokładność: 95.39%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.225176\tNajlepsza wartość straty: 0.149692\tDokładność: 93.94%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.172184\tNajlepsza wartość straty: 0.149692\tDokładność: 95.15%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.156152\tNajlepsza wartość straty: 0.149692\tDokładność: 96.33%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.247590\tNajlepsza wartość straty: 0.149692\tDokładność: 93.63%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.263161\tNajlepsza wartość straty: 0.149692\tDokładność: 94.25%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.201231\tNajlepsza wartość straty: 0.149692\tDokładność: 94.45%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.202193\tNajlepsza wartość straty: 0.149692\tDokładność: 95.39%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.192854\tNajlepsza wartość straty: 0.149692\tDokładność: 95.62%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.174597\tNajlepsza wartość straty: 0.149692\tDokładność: 96.05%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.230082\tNajlepsza wartość straty: 0.149692\tDokładność: 94.33%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.436372\tNajlepsza wartość straty: 0.149692\tDokładność: 85.11%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.356172\tNajlepsza wartość straty: 0.149692\tDokładność: 90.23%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.330461\tNajlepsza wartość straty: 0.149692\tDokładność: 92.26%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.574453\tNajlepsza wartość straty: 0.149692\tDokładność: 80.45%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.835811\tNajlepsza wartość straty: 0.149692\tDokładność: 69.90%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.869377\tNajlepsza wartość straty: 0.149692\tDokładność: 72.63%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.707446\tNajlepsza wartość straty: 0.149692\tDokładność: 73.30%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.886275\tNajlepsza wartość straty: 0.149692\tDokładność: 58.29%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.793251\tNajlepsza wartość straty: 0.149692\tDokładność: 54.14%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.271176\tNajlepsza wartość straty: 0.149692\tDokładność: 41.32%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.297220\tNajlepsza wartość straty: 0.149692\tDokładność: 39.09%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=   5.8s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.171512\tNajlepsza wartość straty: 0.171512\tDokładność: 95.07%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.095914\tNajlepsza wartość straty: 0.095914\tDokładność: 97.03%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.098980\tNajlepsza wartość straty: 0.095914\tDokładność: 96.91%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.096964\tNajlepsza wartość straty: 0.095914\tDokładność: 96.91%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.076523\tNajlepsza wartość straty: 0.076523\tDokładność: 97.62%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.074163\tNajlepsza wartość straty: 0.074163\tDokładność: 97.89%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.072467\tNajlepsza wartość straty: 0.072467\tDokładność: 97.93%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.079879\tNajlepsza wartość straty: 0.072467\tDokładność: 97.69%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.075959\tNajlepsza wartość straty: 0.072467\tDokładność: 97.93%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.071581\tNajlepsza wartość straty: 0.071581\tDokładność: 98.16%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.062939\tNajlepsza wartość straty: 0.062939\tDokładność: 98.16%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.107305\tNajlepsza wartość straty: 0.062939\tDokładność: 97.22%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.081277\tNajlepsza wartość straty: 0.062939\tDokładność: 98.28%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.077059\tNajlepsza wartość straty: 0.062939\tDokładność: 98.32%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.080077\tNajlepsza wartość straty: 0.062939\tDokładność: 98.08%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.081046\tNajlepsza wartość straty: 0.062939\tDokładność: 98.12%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.076096\tNajlepsza wartość straty: 0.062939\tDokładność: 98.16%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.082443\tNajlepsza wartość straty: 0.062939\tDokładność: 98.51%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.083749\tNajlepsza wartość straty: 0.062939\tDokładność: 98.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.076372\tNajlepsza wartość straty: 0.062939\tDokładność: 97.97%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.074306\tNajlepsza wartość straty: 0.062939\tDokładność: 98.24%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.095687\tNajlepsza wartość straty: 0.062939\tDokładność: 98.12%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.077392\tNajlepsza wartość straty: 0.062939\tDokładność: 97.85%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.097903\tNajlepsza wartość straty: 0.062939\tDokładność: 98.08%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.088720\tNajlepsza wartość straty: 0.062939\tDokładność: 98.20%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.082949\tNajlepsza wartość straty: 0.062939\tDokładność: 98.32%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.090403\tNajlepsza wartość straty: 0.062939\tDokładność: 98.48%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.085375\tNajlepsza wartość straty: 0.062939\tDokładność: 98.24%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.075671\tNajlepsza wartość straty: 0.062939\tDokładność: 98.51%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.079122\tNajlepsza wartość straty: 0.062939\tDokładność: 98.32%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.094963\tNajlepsza wartość straty: 0.062939\tDokładność: 98.16%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.082639\tNajlepsza wartość straty: 0.062939\tDokładność: 98.24%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x0000029B12839B70>, total=   9.4s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.113188\tNajlepsza wartość straty: 0.113188\tDokładność: 96.60%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.081384\tNajlepsza wartość straty: 0.081384\tDokładność: 97.58%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.068770\tNajlepsza wartość straty: 0.068770\tDokładność: 98.12%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.077317\tNajlepsza wartość straty: 0.068770\tDokładność: 97.73%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.074333\tNajlepsza wartość straty: 0.068770\tDokładność: 97.97%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.085815\tNajlepsza wartość straty: 0.068770\tDokładność: 97.22%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.085233\tNajlepsza wartość straty: 0.068770\tDokładność: 97.69%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.077944\tNajlepsza wartość straty: 0.068770\tDokładność: 97.62%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.098302\tNajlepsza wartość straty: 0.068770\tDokładność: 97.54%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.101334\tNajlepsza wartość straty: 0.068770\tDokładność: 96.87%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.077176\tNajlepsza wartość straty: 0.068770\tDokładność: 98.12%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.065846\tNajlepsza wartość straty: 0.065846\tDokładność: 98.28%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.085368\tNajlepsza wartość straty: 0.065846\tDokładność: 98.08%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.061489\tNajlepsza wartość straty: 0.061489\tDokładność: 98.28%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.079496\tNajlepsza wartość straty: 0.061489\tDokładność: 98.05%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.065609\tNajlepsza wartość straty: 0.061489\tDokładność: 98.36%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.061526\tNajlepsza wartość straty: 0.061489\tDokładność: 98.48%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.062700\tNajlepsza wartość straty: 0.061489\tDokładność: 98.44%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.093224\tNajlepsza wartość straty: 0.061489\tDokładność: 98.08%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.083643\tNajlepsza wartość straty: 0.061489\tDokładność: 98.40%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.064508\tNajlepsza wartość straty: 0.061489\tDokładność: 98.59%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.064719\tNajlepsza wartość straty: 0.061489\tDokładność: 98.36%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.068598\tNajlepsza wartość straty: 0.061489\tDokładność: 98.16%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.093927\tNajlepsza wartość straty: 0.061489\tDokładność: 98.12%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.077118\tNajlepsza wartość straty: 0.061489\tDokładność: 98.67%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.091180\tNajlepsza wartość straty: 0.061489\tDokładność: 98.05%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.085288\tNajlepsza wartość straty: 0.061489\tDokładność: 98.40%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.060442\tNajlepsza wartość straty: 0.060442\tDokładność: 98.55%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.087844\tNajlepsza wartość straty: 0.060442\tDokładność: 98.28%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.072437\tNajlepsza wartość straty: 0.060442\tDokładność: 98.12%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.075639\tNajlepsza wartość straty: 0.060442\tDokładność: 98.16%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.067732\tNajlepsza wartość straty: 0.060442\tDokładność: 98.59%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.099239\tNajlepsza wartość straty: 0.060442\tDokładność: 98.55%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.079558\tNajlepsza wartość straty: 0.060442\tDokładność: 98.55%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.085547\tNajlepsza wartość straty: 0.060442\tDokładność: 98.51%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.096883\tNajlepsza wartość straty: 0.060442\tDokładność: 98.01%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.073843\tNajlepsza wartość straty: 0.060442\tDokładność: 98.36%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.096842\tNajlepsza wartość straty: 0.060442\tDokładność: 98.01%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.063650\tNajlepsza wartość straty: 0.060442\tDokładność: 98.51%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.075109\tNajlepsza wartość straty: 0.060442\tDokładność: 98.67%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.070305\tNajlepsza wartość straty: 0.060442\tDokładność: 98.48%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.074671\tNajlepsza wartość straty: 0.060442\tDokładność: 98.71%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.100321\tNajlepsza wartość straty: 0.060442\tDokładność: 98.48%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.093451\tNajlepsza wartość straty: 0.060442\tDokładność: 98.67%\n",
      "44\tF. straty dla zbioru walidacyjnego: 0.094709\tNajlepsza wartość straty: 0.060442\tDokładność: 98.16%\n",
      "45\tF. straty dla zbioru walidacyjnego: 0.098850\tNajlepsza wartość straty: 0.060442\tDokładność: 98.63%\n",
      "46\tF. straty dla zbioru walidacyjnego: 0.093627\tNajlepsza wartość straty: 0.060442\tDokładność: 98.67%\n",
      "47\tF. straty dla zbioru walidacyjnego: 0.099696\tNajlepsza wartość straty: 0.060442\tDokładność: 98.32%\n",
      "48\tF. straty dla zbioru walidacyjnego: 0.099620\tNajlepsza wartość straty: 0.060442\tDokładność: 98.48%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x0000029B12839B70>, total=  13.8s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.121159\tNajlepsza wartość straty: 0.121159\tDokładność: 96.68%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.093012\tNajlepsza wartość straty: 0.093012\tDokładność: 97.11%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.084517\tNajlepsza wartość straty: 0.084517\tDokładność: 97.50%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.070080\tNajlepsza wartość straty: 0.070080\tDokładność: 97.58%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.073499\tNajlepsza wartość straty: 0.070080\tDokładność: 97.69%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.077686\tNajlepsza wartość straty: 0.070080\tDokładność: 97.93%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.063263\tNajlepsza wartość straty: 0.063263\tDokładność: 98.20%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.066538\tNajlepsza wartość straty: 0.063263\tDokładność: 97.93%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.095481\tNajlepsza wartość straty: 0.063263\tDokładność: 97.22%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.072719\tNajlepsza wartość straty: 0.063263\tDokładność: 98.12%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.060233\tNajlepsza wartość straty: 0.060233\tDokładność: 98.28%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.076540\tNajlepsza wartość straty: 0.060233\tDokładność: 97.93%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.066694\tNajlepsza wartość straty: 0.060233\tDokładność: 98.51%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.067124\tNajlepsza wartość straty: 0.060233\tDokładność: 98.36%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.081196\tNajlepsza wartość straty: 0.060233\tDokładność: 97.69%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.056737\tNajlepsza wartość straty: 0.056737\tDokładność: 98.40%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.071117\tNajlepsza wartość straty: 0.056737\tDokładność: 98.51%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.071210\tNajlepsza wartość straty: 0.056737\tDokładność: 98.20%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.068937\tNajlepsza wartość straty: 0.056737\tDokładność: 98.63%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.081441\tNajlepsza wartość straty: 0.056737\tDokładność: 98.32%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.075225\tNajlepsza wartość straty: 0.056737\tDokładność: 98.44%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.092734\tNajlepsza wartość straty: 0.056737\tDokładność: 97.89%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.067307\tNajlepsza wartość straty: 0.056737\tDokładność: 98.44%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.089229\tNajlepsza wartość straty: 0.056737\tDokładność: 98.48%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.100539\tNajlepsza wartość straty: 0.056737\tDokładność: 97.69%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.086649\tNajlepsza wartość straty: 0.056737\tDokładność: 98.40%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.079722\tNajlepsza wartość straty: 0.056737\tDokładność: 98.28%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.092547\tNajlepsza wartość straty: 0.056737\tDokładność: 98.12%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.098174\tNajlepsza wartość straty: 0.056737\tDokładność: 98.20%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.110364\tNajlepsza wartość straty: 0.056737\tDokładność: 97.15%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.088850\tNajlepsza wartość straty: 0.056737\tDokładność: 98.48%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.086031\tNajlepsza wartość straty: 0.056737\tDokładność: 98.05%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.091254\tNajlepsza wartość straty: 0.056737\tDokładność: 98.40%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.092743\tNajlepsza wartość straty: 0.056737\tDokładność: 98.51%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.099257\tNajlepsza wartość straty: 0.056737\tDokładność: 98.05%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.096243\tNajlepsza wartość straty: 0.056737\tDokładność: 98.36%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.095529\tNajlepsza wartość straty: 0.056737\tDokładność: 97.93%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x0000029B12839B70>, total=  10.6s\n",
      "[CV] n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 800.520081\tNajlepsza wartość straty: 800.520081\tDokładność: 33.97%\n",
      "1\tF. straty dla zbioru walidacyjnego: 4.385228\tNajlepsza wartość straty: 4.385228\tDokładność: 61.73%\n",
      "2\tF. straty dla zbioru walidacyjnego: 2.910259\tNajlepsza wartość straty: 2.910259\tDokładność: 67.51%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.517659\tNajlepsza wartość straty: 1.517659\tDokładność: 70.88%\n",
      "4\tF. straty dla zbioru walidacyjnego: 4.526204\tNajlepsza wartość straty: 1.517659\tDokładność: 74.16%\n",
      "5\tF. straty dla zbioru walidacyjnego: 2.579665\tNajlepsza wartość straty: 1.517659\tDokładność: 64.97%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.039967\tNajlepsza wartość straty: 1.039967\tDokładność: 77.56%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.618346\tNajlepsza wartość straty: 0.618346\tDokładność: 82.21%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.109339\tNajlepsza wartość straty: 0.618346\tDokładność: 78.93%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.518543\tNajlepsza wartość straty: 0.518543\tDokładność: 87.06%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.410750\tNajlepsza wartość straty: 0.410750\tDokładność: 89.56%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.625297\tNajlepsza wartość straty: 0.410750\tDokładność: 86.86%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.688140\tNajlepsza wartość straty: 0.410750\tDokładność: 88.66%\n",
      "13\tF. straty dla zbioru walidacyjnego: 236.011383\tNajlepsza wartość straty: 0.410750\tDokładność: 44.06%\n",
      "14\tF. straty dla zbioru walidacyjnego: 59.823410\tNajlepsza wartość straty: 0.410750\tDokładność: 73.61%\n",
      "15\tF. straty dla zbioru walidacyjnego: 90.937668\tNajlepsza wartość straty: 0.410750\tDokładność: 55.08%\n",
      "16\tF. straty dla zbioru walidacyjnego: 30.833586\tNajlepsza wartość straty: 0.410750\tDokładność: 72.48%\n",
      "17\tF. straty dla zbioru walidacyjnego: 29.750740\tNajlepsza wartość straty: 0.410750\tDokładność: 79.98%\n",
      "18\tF. straty dla zbioru walidacyjnego: 13.498824\tNajlepsza wartość straty: 0.410750\tDokładność: 90.50%\n",
      "19\tF. straty dla zbioru walidacyjnego: 72.495865\tNajlepsza wartość straty: 0.410750\tDokładność: 75.53%\n",
      "20\tF. straty dla zbioru walidacyjnego: 22.407402\tNajlepsza wartość straty: 0.410750\tDokładność: 89.41%\n",
      "21\tF. straty dla zbioru walidacyjnego: 19.461544\tNajlepsza wartość straty: 0.410750\tDokładność: 89.60%\n",
      "22\tF. straty dla zbioru walidacyjnego: 11.638667\tNajlepsza wartość straty: 0.410750\tDokładność: 91.75%\n",
      "23\tF. straty dla zbioru walidacyjnego: 15.652671\tNajlepsza wartość straty: 0.410750\tDokładność: 91.13%\n",
      "24\tF. straty dla zbioru walidacyjnego: 2844.811523\tNajlepsza wartość straty: 0.410750\tDokładność: 18.73%\n",
      "25\tF. straty dla zbioru walidacyjnego: 63.451542\tNajlepsza wartość straty: 0.410750\tDokładność: 78.77%\n",
      "26\tF. straty dla zbioru walidacyjnego: 44.397560\tNajlepsza wartość straty: 0.410750\tDokładność: 71.93%\n",
      "27\tF. straty dla zbioru walidacyjnego: 29.596893\tNajlepsza wartość straty: 0.410750\tDokładność: 86.63%\n",
      "28\tF. straty dla zbioru walidacyjnego: 10.025848\tNajlepsza wartość straty: 0.410750\tDokładność: 92.38%\n",
      "29\tF. straty dla zbioru walidacyjnego: 13.018786\tNajlepsza wartość straty: 0.410750\tDokładność: 89.01%\n",
      "30\tF. straty dla zbioru walidacyjnego: 8.043139\tNajlepsza wartość straty: 0.410750\tDokładność: 92.46%\n",
      "31\tF. straty dla zbioru walidacyjnego: 28.530499\tNajlepsza wartość straty: 0.410750\tDokładność: 79.67%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  28.7s\n",
      "[CV] n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 60.233143\tNajlepsza wartość straty: 60.233143\tDokładność: 56.80%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.617118\tNajlepsza wartość straty: 0.617118\tDokładność: 84.28%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.768517\tNajlepsza wartość straty: 0.617118\tDokładność: 86.79%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.351044\tNajlepsza wartość straty: 0.351044\tDokładność: 91.05%\n",
      "4\tF. straty dla zbioru walidacyjnego: 38335.945312\tNajlepsza wartość straty: 0.351044\tDokładność: 32.64%\n",
      "5\tF. straty dla zbioru walidacyjnego: 247.095947\tNajlepsza wartość straty: 0.351044\tDokładność: 60.52%\n",
      "6\tF. straty dla zbioru walidacyjnego: 211.020432\tNajlepsza wartość straty: 0.351044\tDokładność: 63.72%\n",
      "7\tF. straty dla zbioru walidacyjnego: 82.934616\tNajlepsza wartość straty: 0.351044\tDokładność: 79.87%\n",
      "8\tF. straty dla zbioru walidacyjnego: 52.737331\tNajlepsza wartość straty: 0.351044\tDokładność: 82.88%\n",
      "9\tF. straty dla zbioru walidacyjnego: 40.705551\tNajlepsza wartość straty: 0.351044\tDokładność: 84.60%\n",
      "10\tF. straty dla zbioru walidacyjnego: 79.511559\tNajlepsza wartość straty: 0.351044\tDokładność: 74.08%\n",
      "11\tF. straty dla zbioru walidacyjnego: 40.357811\tNajlepsza wartość straty: 0.351044\tDokładność: 88.19%\n",
      "12\tF. straty dla zbioru walidacyjnego: 73.639351\tNajlepsza wartość straty: 0.351044\tDokładność: 70.33%\n",
      "13\tF. straty dla zbioru walidacyjnego: 50.636612\tNajlepsza wartość straty: 0.351044\tDokładność: 83.70%\n",
      "14\tF. straty dla zbioru walidacyjnego: 43.278233\tNajlepsza wartość straty: 0.351044\tDokładność: 89.33%\n",
      "15\tF. straty dla zbioru walidacyjnego: 502.911560\tNajlepsza wartość straty: 0.351044\tDokładność: 65.79%\n",
      "16\tF. straty dla zbioru walidacyjnego: 225.466263\tNajlepsza wartość straty: 0.351044\tDokładność: 55.79%\n",
      "17\tF. straty dla zbioru walidacyjnego: 55.858788\tNajlepsza wartość straty: 0.351044\tDokładność: 80.06%\n",
      "18\tF. straty dla zbioru walidacyjnego: 36.383057\tNajlepsza wartość straty: 0.351044\tDokładność: 88.55%\n",
      "19\tF. straty dla zbioru walidacyjnego: 40.049244\tNajlepsza wartość straty: 0.351044\tDokładność: 80.02%\n",
      "20\tF. straty dla zbioru walidacyjnego: 415.545105\tNajlepsza wartość straty: 0.351044\tDokładność: 56.33%\n",
      "21\tF. straty dla zbioru walidacyjnego: 30.738388\tNajlepsza wartość straty: 0.351044\tDokładność: 88.86%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1687.776001\tNajlepsza wartość straty: 0.351044\tDokładność: 59.93%\n",
      "23\tF. straty dla zbioru walidacyjnego: 722.048340\tNajlepsza wartość straty: 0.351044\tDokładność: 88.94%\n",
      "24\tF. straty dla zbioru walidacyjnego: 612.222839\tNajlepsza wartość straty: 0.351044\tDokładność: 83.07%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  22.2s\n",
      "[CV] n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 2.372510\tNajlepsza wartość straty: 2.372510\tDokładność: 62.04%\n",
      "1\tF. straty dla zbioru walidacyjnego: 9.133509\tNajlepsza wartość straty: 2.372510\tDokładność: 54.18%\n",
      "2\tF. straty dla zbioru walidacyjnego: 3.197797\tNajlepsza wartość straty: 2.372510\tDokładność: 71.66%\n",
      "3\tF. straty dla zbioru walidacyjnego: 3.479365\tNajlepsza wartość straty: 2.372510\tDokładność: 72.56%\n",
      "4\tF. straty dla zbioru walidacyjnego: 2.982099\tNajlepsza wartość straty: 2.372510\tDokładność: 81.04%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.101210\tNajlepsza wartość straty: 1.101210\tDokładność: 87.41%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.374337\tNajlepsza wartość straty: 1.101210\tDokładność: 83.89%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.475405\tNajlepsza wartość straty: 1.101210\tDokładność: 82.92%\n",
      "8\tF. straty dla zbioru walidacyjnego: 470.632080\tNajlepsza wartość straty: 1.101210\tDokładność: 31.74%\n",
      "9\tF. straty dla zbioru walidacyjnego: 170.168671\tNajlepsza wartość straty: 1.101210\tDokładność: 57.66%\n",
      "10\tF. straty dla zbioru walidacyjnego: 278.015991\tNajlepsza wartość straty: 1.101210\tDokładność: 61.18%\n",
      "11\tF. straty dla zbioru walidacyjnego: 120.567238\tNajlepsza wartość straty: 1.101210\tDokładność: 70.17%\n",
      "12\tF. straty dla zbioru walidacyjnego: 161.904739\tNajlepsza wartość straty: 1.101210\tDokładność: 72.01%\n",
      "13\tF. straty dla zbioru walidacyjnego: 69.559654\tNajlepsza wartość straty: 1.101210\tDokładność: 79.24%\n",
      "14\tF. straty dla zbioru walidacyjnego: 91.122765\tNajlepsza wartość straty: 1.101210\tDokładność: 77.83%\n",
      "15\tF. straty dla zbioru walidacyjnego: 70.470764\tNajlepsza wartość straty: 1.101210\tDokładność: 85.18%\n",
      "16\tF. straty dla zbioru walidacyjnego: 41.893570\tNajlepsza wartość straty: 1.101210\tDokładność: 88.15%\n",
      "17\tF. straty dla zbioru walidacyjnego: 44.393120\tNajlepsza wartość straty: 1.101210\tDokładność: 86.12%\n",
      "18\tF. straty dla zbioru walidacyjnego: 225.675827\tNajlepsza wartość straty: 1.101210\tDokładność: 85.26%\n",
      "19\tF. straty dla zbioru walidacyjnego: 86.939911\tNajlepsza wartość straty: 1.101210\tDokładność: 90.85%\n",
      "20\tF. straty dla zbioru walidacyjnego: 279.302002\tNajlepsza wartość straty: 1.101210\tDokładność: 90.58%\n",
      "21\tF. straty dla zbioru walidacyjnego: 151.955505\tNajlepsza wartość straty: 1.101210\tDokładność: 92.92%\n",
      "22\tF. straty dla zbioru walidacyjnego: 125.437645\tNajlepsza wartość straty: 1.101210\tDokładność: 90.54%\n",
      "23\tF. straty dla zbioru walidacyjnego: 57.845509\tNajlepsza wartość straty: 1.101210\tDokładność: 94.76%\n",
      "24\tF. straty dla zbioru walidacyjnego: 3670.417480\tNajlepsza wartość straty: 1.101210\tDokładność: 91.63%\n",
      "25\tF. straty dla zbioru walidacyjnego: 653.988770\tNajlepsza wartość straty: 1.101210\tDokładność: 84.36%\n",
      "26\tF. straty dla zbioru walidacyjnego: 431.141663\tNajlepsza wartość straty: 1.101210\tDokładność: 90.27%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  24.6s\n",
      "[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 13.012430\tNajlepsza wartość straty: 13.012430\tDokładność: 91.09%\n",
      "1\tF. straty dla zbioru walidacyjnego: 22.203869\tNajlepsza wartość straty: 13.012430\tDokładność: 87.26%\n",
      "2\tF. straty dla zbioru walidacyjnego: 158303.609375\tNajlepsza wartość straty: 13.012430\tDokładność: 55.12%\n",
      "3\tF. straty dla zbioru walidacyjnego: 79310.906250\tNajlepsza wartość straty: 13.012430\tDokładność: 68.45%\n",
      "4\tF. straty dla zbioru walidacyjnego: 15510.922852\tNajlepsza wartość straty: 13.012430\tDokładność: 89.33%\n",
      "5\tF. straty dla zbioru walidacyjnego: 11592.127930\tNajlepsza wartość straty: 13.012430\tDokładność: 88.55%\n",
      "6\tF. straty dla zbioru walidacyjnego: 5495.321289\tNajlepsza wartość straty: 13.012430\tDokładność: 92.85%\n",
      "7\tF. straty dla zbioru walidacyjnego: 5444.861328\tNajlepsza wartość straty: 13.012430\tDokładność: 92.03%\n",
      "8\tF. straty dla zbioru walidacyjnego: 3133.974609\tNajlepsza wartość straty: 13.012430\tDokładność: 94.84%\n",
      "9\tF. straty dla zbioru walidacyjnego: 3524.067139\tNajlepsza wartość straty: 13.012430\tDokładność: 93.90%\n",
      "10\tF. straty dla zbioru walidacyjnego: 7265.434570\tNajlepsza wartość straty: 13.012430\tDokładność: 93.98%\n",
      "11\tF. straty dla zbioru walidacyjnego: 6660.238281\tNajlepsza wartość straty: 13.012430\tDokładność: 92.53%\n",
      "12\tF. straty dla zbioru walidacyjnego: 3583.297852\tNajlepsza wartość straty: 13.012430\tDokładność: 95.39%\n",
      "13\tF. straty dla zbioru walidacyjnego: 3952.222900\tNajlepsza wartość straty: 13.012430\tDokładność: 93.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 5326.234375\tNajlepsza wartość straty: 13.012430\tDokładność: 93.75%\n",
      "15\tF. straty dla zbioru walidacyjnego: 2538.058350\tNajlepsza wartość straty: 13.012430\tDokładność: 95.74%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1845.792358\tNajlepsza wartość straty: 13.012430\tDokładność: 95.07%\n",
      "17\tF. straty dla zbioru walidacyjnego: 3700.631348\tNajlepsza wartość straty: 13.012430\tDokładność: 95.70%\n",
      "18\tF. straty dla zbioru walidacyjnego: 335365.843750\tNajlepsza wartość straty: 13.012430\tDokładność: 84.32%\n",
      "19\tF. straty dla zbioru walidacyjnego: 276410.625000\tNajlepsza wartość straty: 13.012430\tDokładność: 88.47%\n",
      "20\tF. straty dla zbioru walidacyjnego: 197002.203125\tNajlepsza wartość straty: 13.012430\tDokładność: 88.23%\n",
      "21\tF. straty dla zbioru walidacyjnego: 76710.539062\tNajlepsza wartość straty: 13.012430\tDokładność: 94.37%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  17.3s\n",
      "[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1620.873413\tNajlepsza wartość straty: 1620.873413\tDokładność: 77.33%\n",
      "1\tF. straty dla zbioru walidacyjnego: 412.625061\tNajlepsza wartość straty: 412.625061\tDokładność: 88.51%\n",
      "2\tF. straty dla zbioru walidacyjnego: 125.027069\tNajlepsza wartość straty: 125.027069\tDokładność: 95.11%\n",
      "3\tF. straty dla zbioru walidacyjnego: 116.909637\tNajlepsza wartość straty: 116.909637\tDokładność: 93.55%\n",
      "4\tF. straty dla zbioru walidacyjnego: 246.550140\tNajlepsza wartość straty: 116.909637\tDokładność: 91.36%\n",
      "5\tF. straty dla zbioru walidacyjnego: 138.984695\tNajlepsza wartość straty: 116.909637\tDokładność: 96.05%\n",
      "6\tF. straty dla zbioru walidacyjnego: 90.437157\tNajlepsza wartość straty: 90.437157\tDokładność: 95.93%\n",
      "7\tF. straty dla zbioru walidacyjnego: 62.147480\tNajlepsza wartość straty: 62.147480\tDokładność: 94.29%\n",
      "8\tF. straty dla zbioru walidacyjnego: 44.952271\tNajlepsza wartość straty: 44.952271\tDokładność: 95.39%\n",
      "9\tF. straty dla zbioru walidacyjnego: 24939836.000000\tNajlepsza wartość straty: 44.952271\tDokładność: 18.73%\n",
      "10\tF. straty dla zbioru walidacyjnego: 34513.132812\tNajlepsza wartość straty: 44.952271\tDokładność: 90.15%\n",
      "11\tF. straty dla zbioru walidacyjnego: 37321.476562\tNajlepsza wartość straty: 44.952271\tDokładność: 89.68%\n",
      "12\tF. straty dla zbioru walidacyjnego: 12868.982422\tNajlepsza wartość straty: 44.952271\tDokładność: 94.14%\n",
      "13\tF. straty dla zbioru walidacyjnego: 14249.498047\tNajlepsza wartość straty: 44.952271\tDokładność: 94.33%\n",
      "14\tF. straty dla zbioru walidacyjnego: 10827.159180\tNajlepsza wartość straty: 44.952271\tDokładność: 95.39%\n",
      "15\tF. straty dla zbioru walidacyjnego: 13647.011719\tNajlepsza wartość straty: 44.952271\tDokładność: 94.33%\n",
      "16\tF. straty dla zbioru walidacyjnego: 9707.417969\tNajlepsza wartość straty: 44.952271\tDokładność: 93.86%\n",
      "17\tF. straty dla zbioru walidacyjnego: 17137.650391\tNajlepsza wartość straty: 44.952271\tDokładność: 94.76%\n",
      "18\tF. straty dla zbioru walidacyjnego: 13920.938477\tNajlepsza wartość straty: 44.952271\tDokładność: 95.54%\n",
      "19\tF. straty dla zbioru walidacyjnego: 6034.750000\tNajlepsza wartość straty: 44.952271\tDokładność: 96.44%\n",
      "20\tF. straty dla zbioru walidacyjnego: 9131.042969\tNajlepsza wartość straty: 44.952271\tDokładność: 94.92%\n",
      "21\tF. straty dla zbioru walidacyjnego: 4828.199707\tNajlepsza wartość straty: 44.952271\tDokładność: 95.93%\n",
      "22\tF. straty dla zbioru walidacyjnego: 5932.536133\tNajlepsza wartość straty: 44.952271\tDokładność: 96.05%\n",
      "23\tF. straty dla zbioru walidacyjnego: 4870.350098\tNajlepsza wartość straty: 44.952271\tDokładność: 96.44%\n",
      "24\tF. straty dla zbioru walidacyjnego: 28645.798828\tNajlepsza wartość straty: 44.952271\tDokładność: 94.18%\n",
      "25\tF. straty dla zbioru walidacyjnego: 6593.981445\tNajlepsza wartość straty: 44.952271\tDokładność: 96.52%\n",
      "26\tF. straty dla zbioru walidacyjnego: 6455.593262\tNajlepsza wartość straty: 44.952271\tDokładność: 95.66%\n",
      "27\tF. straty dla zbioru walidacyjnego: 7260.628418\tNajlepsza wartość straty: 44.952271\tDokładność: 96.52%\n",
      "28\tF. straty dla zbioru walidacyjnego: 5548.128418\tNajlepsza wartość straty: 44.952271\tDokładność: 95.07%\n",
      "29\tF. straty dla zbioru walidacyjnego: 4110.149902\tNajlepsza wartość straty: 44.952271\tDokładność: 96.17%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  23.3s\n",
      "[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 115.211800\tNajlepsza wartość straty: 115.211800\tDokładność: 85.57%\n",
      "1\tF. straty dla zbioru walidacyjnego: 18.166527\tNajlepsza wartość straty: 18.166527\tDokładność: 93.78%\n",
      "2\tF. straty dla zbioru walidacyjnego: 14.747892\tNajlepsza wartość straty: 14.747892\tDokładność: 93.04%\n",
      "3\tF. straty dla zbioru walidacyjnego: 20.277140\tNajlepsza wartość straty: 14.747892\tDokładność: 91.05%\n",
      "4\tF. straty dla zbioru walidacyjnego: 8.318230\tNajlepsza wartość straty: 8.318230\tDokładność: 96.33%\n",
      "5\tF. straty dla zbioru walidacyjnego: 133706.031250\tNajlepsza wartość straty: 8.318230\tDokładność: 70.64%\n",
      "6\tF. straty dla zbioru walidacyjnego: 67987.632812\tNajlepsza wartość straty: 8.318230\tDokładność: 79.12%\n",
      "7\tF. straty dla zbioru walidacyjnego: 13783.468750\tNajlepsza wartość straty: 8.318230\tDokładność: 92.10%\n",
      "8\tF. straty dla zbioru walidacyjnego: 21139.125000\tNajlepsza wartość straty: 8.318230\tDokładność: 89.48%\n",
      "9\tF. straty dla zbioru walidacyjnego: 5693.002930\tNajlepsza wartość straty: 8.318230\tDokładność: 95.35%\n",
      "10\tF. straty dla zbioru walidacyjnego: 7442.672363\tNajlepsza wartość straty: 8.318230\tDokładność: 94.21%\n",
      "11\tF. straty dla zbioru walidacyjnego: 9047.648438\tNajlepsza wartość straty: 8.318230\tDokładność: 94.96%\n",
      "12\tF. straty dla zbioru walidacyjnego: 14721.864258\tNajlepsza wartość straty: 8.318230\tDokładność: 94.57%\n",
      "13\tF. straty dla zbioru walidacyjnego: 3852.659912\tNajlepsza wartość straty: 8.318230\tDokładność: 94.72%\n",
      "14\tF. straty dla zbioru walidacyjnego: 6593.734375\tNajlepsza wartość straty: 8.318230\tDokładność: 94.41%\n",
      "15\tF. straty dla zbioru walidacyjnego: 4512.522461\tNajlepsza wartość straty: 8.318230\tDokładność: 95.50%\n",
      "16\tF. straty dla zbioru walidacyjnego: 10497.817383\tNajlepsza wartość straty: 8.318230\tDokładność: 94.92%\n",
      "17\tF. straty dla zbioru walidacyjnego: 4821.115723\tNajlepsza wartość straty: 8.318230\tDokładność: 95.93%\n",
      "18\tF. straty dla zbioru walidacyjnego: 4150.395020\tNajlepsza wartość straty: 8.318230\tDokładność: 96.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 3102.059326\tNajlepsza wartość straty: 8.318230\tDokładność: 95.07%\n",
      "20\tF. straty dla zbioru walidacyjnego: 5382.121094\tNajlepsza wartość straty: 8.318230\tDokładność: 93.32%\n",
      "21\tF. straty dla zbioru walidacyjnego: 3933.725586\tNajlepsza wartość straty: 8.318230\tDokładność: 92.34%\n",
      "22\tF. straty dla zbioru walidacyjnego: 2281.433105\tNajlepsza wartość straty: 8.318230\tDokładność: 95.43%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1736.405762\tNajlepsza wartość straty: 8.318230\tDokładność: 94.61%\n",
      "24\tF. straty dla zbioru walidacyjnego: 6060945.000000\tNajlepsza wartość straty: 8.318230\tDokładność: 76.78%\n",
      "25\tF. straty dla zbioru walidacyjnego: 313629.343750\tNajlepsza wartość straty: 8.318230\tDokładność: 93.39%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  20.4s\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.107352\tNajlepsza wartość straty: 0.107352\tDokładność: 96.72%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.067582\tNajlepsza wartość straty: 0.067582\tDokładność: 98.08%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.070102\tNajlepsza wartość straty: 0.067582\tDokładność: 97.89%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.048823\tNajlepsza wartość straty: 0.048823\tDokładność: 98.36%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.057367\tNajlepsza wartość straty: 0.048823\tDokładność: 98.20%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.053407\tNajlepsza wartość straty: 0.048823\tDokładność: 98.36%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.046155\tNajlepsza wartość straty: 0.046155\tDokładność: 98.75%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.054474\tNajlepsza wartość straty: 0.046155\tDokładność: 98.67%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.053990\tNajlepsza wartość straty: 0.046155\tDokładność: 98.44%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.049635\tNajlepsza wartość straty: 0.046155\tDokładność: 98.51%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.072630\tNajlepsza wartość straty: 0.046155\tDokładność: 98.28%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.051085\tNajlepsza wartość straty: 0.046155\tDokładność: 98.71%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.050371\tNajlepsza wartość straty: 0.046155\tDokładność: 98.83%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.044364\tNajlepsza wartość straty: 0.044364\tDokładność: 98.87%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.065085\tNajlepsza wartość straty: 0.044364\tDokładność: 98.71%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.066779\tNajlepsza wartość straty: 0.044364\tDokładność: 98.87%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.053789\tNajlepsza wartość straty: 0.044364\tDokładność: 98.71%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.053181\tNajlepsza wartość straty: 0.044364\tDokładność: 98.87%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.047658\tNajlepsza wartość straty: 0.044364\tDokładność: 99.18%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.063554\tNajlepsza wartość straty: 0.044364\tDokładność: 98.94%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.081311\tNajlepsza wartość straty: 0.044364\tDokładność: 98.40%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.055556\tNajlepsza wartość straty: 0.044364\tDokładność: 98.79%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.068848\tNajlepsza wartość straty: 0.044364\tDokładność: 98.91%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.057993\tNajlepsza wartość straty: 0.044364\tDokładność: 98.94%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.083880\tNajlepsza wartość straty: 0.044364\tDokładność: 98.16%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.053344\tNajlepsza wartość straty: 0.044364\tDokładność: 99.18%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.075122\tNajlepsza wartość straty: 0.044364\tDokładność: 98.79%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.091402\tNajlepsza wartość straty: 0.044364\tDokładność: 98.91%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.075665\tNajlepsza wartość straty: 0.044364\tDokładność: 98.83%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.075533\tNajlepsza wartość straty: 0.044364\tDokładność: 99.18%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.067005\tNajlepsza wartość straty: 0.044364\tDokładność: 99.18%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.061186\tNajlepsza wartość straty: 0.044364\tDokładność: 98.91%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.059908\tNajlepsza wartość straty: 0.044364\tDokładność: 99.06%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.068134\tNajlepsza wartość straty: 0.044364\tDokładność: 98.91%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.051242\tNajlepsza wartość straty: 0.044364\tDokładność: 99.02%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  21.4s\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.123812\tNajlepsza wartość straty: 0.123812\tDokładność: 95.93%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.080257\tNajlepsza wartość straty: 0.080257\tDokładność: 97.73%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.066383\tNajlepsza wartość straty: 0.066383\tDokładność: 97.89%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.059022\tNajlepsza wartość straty: 0.059022\tDokładność: 98.08%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.070941\tNajlepsza wartość straty: 0.059022\tDokładność: 98.12%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.060014\tNajlepsza wartość straty: 0.059022\tDokładność: 98.20%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.068905\tNajlepsza wartość straty: 0.059022\tDokładność: 98.32%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.053722\tNajlepsza wartość straty: 0.053722\tDokładność: 98.36%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.067600\tNajlepsza wartość straty: 0.053722\tDokładność: 98.40%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.067012\tNajlepsza wartość straty: 0.053722\tDokładność: 98.51%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.059104\tNajlepsza wartość straty: 0.053722\tDokładność: 98.36%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.049261\tNajlepsza wartość straty: 0.049261\tDokładność: 98.75%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.053858\tNajlepsza wartość straty: 0.049261\tDokładność: 98.48%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.057398\tNajlepsza wartość straty: 0.049261\tDokładność: 98.51%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.060815\tNajlepsza wartość straty: 0.049261\tDokładność: 98.79%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.057494\tNajlepsza wartość straty: 0.049261\tDokładność: 98.63%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.059911\tNajlepsza wartość straty: 0.049261\tDokładność: 98.51%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.099617\tNajlepsza wartość straty: 0.049261\tDokładność: 98.20%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.076938\tNajlepsza wartość straty: 0.049261\tDokładność: 98.55%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.056873\tNajlepsza wartość straty: 0.049261\tDokładność: 98.51%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.065963\tNajlepsza wartość straty: 0.049261\tDokładność: 98.79%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.074764\tNajlepsza wartość straty: 0.049261\tDokładność: 98.87%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.082677\tNajlepsza wartość straty: 0.049261\tDokładność: 98.79%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.070752\tNajlepsza wartość straty: 0.049261\tDokładność: 98.71%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.080539\tNajlepsza wartość straty: 0.049261\tDokładność: 98.63%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.090234\tNajlepsza wartość straty: 0.049261\tDokładność: 98.87%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.094130\tNajlepsza wartość straty: 0.049261\tDokładność: 98.67%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.072154\tNajlepsza wartość straty: 0.049261\tDokładność: 98.55%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.076957\tNajlepsza wartość straty: 0.049261\tDokładność: 98.98%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.062817\tNajlepsza wartość straty: 0.049261\tDokładność: 98.71%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.065135\tNajlepsza wartość straty: 0.049261\tDokładność: 98.75%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.092065\tNajlepsza wartość straty: 0.049261\tDokładność: 98.63%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.072346\tNajlepsza wartość straty: 0.049261\tDokładność: 98.79%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  19.5s\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.106846\tNajlepsza wartość straty: 0.106846\tDokładność: 96.95%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.072165\tNajlepsza wartość straty: 0.072165\tDokładność: 97.81%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.057580\tNajlepsza wartość straty: 0.057580\tDokładność: 98.32%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.050661\tNajlepsza wartość straty: 0.050661\tDokładność: 98.32%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.053911\tNajlepsza wartość straty: 0.050661\tDokładność: 98.20%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.049198\tNajlepsza wartość straty: 0.049198\tDokładność: 98.59%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.060296\tNajlepsza wartość straty: 0.049198\tDokładność: 98.16%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.049832\tNajlepsza wartość straty: 0.049198\tDokładność: 98.67%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.077146\tNajlepsza wartość straty: 0.049198\tDokładność: 98.16%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.050382\tNajlepsza wartość straty: 0.049198\tDokładność: 98.44%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.053982\tNajlepsza wartość straty: 0.049198\tDokładność: 98.55%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.040587\tNajlepsza wartość straty: 0.040587\tDokładność: 98.91%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.040331\tNajlepsza wartość straty: 0.040331\tDokładność: 98.55%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.061834\tNajlepsza wartość straty: 0.040331\tDokładność: 98.87%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.056240\tNajlepsza wartość straty: 0.040331\tDokładność: 99.06%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.056692\tNajlepsza wartość straty: 0.040331\tDokładność: 98.59%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.058729\tNajlepsza wartość straty: 0.040331\tDokładność: 98.94%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.043278\tNajlepsza wartość straty: 0.040331\tDokładność: 98.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.049307\tNajlepsza wartość straty: 0.040331\tDokładność: 99.14%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.063369\tNajlepsza wartość straty: 0.040331\tDokładność: 98.16%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.061708\tNajlepsza wartość straty: 0.040331\tDokładność: 98.71%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.060583\tNajlepsza wartość straty: 0.040331\tDokładność: 98.67%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.062980\tNajlepsza wartość straty: 0.040331\tDokładność: 98.83%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.065990\tNajlepsza wartość straty: 0.040331\tDokładność: 99.06%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.060853\tNajlepsza wartość straty: 0.040331\tDokładność: 98.75%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.048856\tNajlepsza wartość straty: 0.040331\tDokładność: 99.02%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.052987\tNajlepsza wartość straty: 0.040331\tDokładność: 99.06%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.074090\tNajlepsza wartość straty: 0.040331\tDokładność: 98.83%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.064469\tNajlepsza wartość straty: 0.040331\tDokładność: 98.79%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.069991\tNajlepsza wartość straty: 0.040331\tDokładność: 98.79%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.073850\tNajlepsza wartość straty: 0.040331\tDokładność: 98.75%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.088937\tNajlepsza wartość straty: 0.040331\tDokładność: 98.75%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.066533\tNajlepsza wartość straty: 0.040331\tDokładność: 98.71%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.073222\tNajlepsza wartość straty: 0.040331\tDokładność: 98.83%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  20.2s\n",
      "[CV] n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.104860\tNajlepsza wartość straty: 0.104860\tDokładność: 96.83%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.063953\tNajlepsza wartość straty: 0.063953\tDokładność: 97.97%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.058816\tNajlepsza wartość straty: 0.058816\tDokładność: 98.01%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.062947\tNajlepsza wartość straty: 0.058816\tDokładność: 98.24%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.057538\tNajlepsza wartość straty: 0.057538\tDokładność: 98.24%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.062077\tNajlepsza wartość straty: 0.057538\tDokładność: 98.44%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.048897\tNajlepsza wartość straty: 0.048897\tDokładność: 98.71%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.066684\tNajlepsza wartość straty: 0.048897\tDokładność: 98.44%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.053573\tNajlepsza wartość straty: 0.048897\tDokładność: 98.40%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.061988\tNajlepsza wartość straty: 0.048897\tDokładność: 98.20%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.063151\tNajlepsza wartość straty: 0.048897\tDokładność: 98.59%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.071210\tNajlepsza wartość straty: 0.048897\tDokładność: 98.16%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.054979\tNajlepsza wartość straty: 0.048897\tDokładność: 98.71%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.054100\tNajlepsza wartość straty: 0.048897\tDokładność: 98.75%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.051875\tNajlepsza wartość straty: 0.048897\tDokładność: 98.75%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.056787\tNajlepsza wartość straty: 0.048897\tDokładność: 98.63%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.044779\tNajlepsza wartość straty: 0.044779\tDokładność: 98.63%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.049773\tNajlepsza wartość straty: 0.044779\tDokładność: 98.98%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.061082\tNajlepsza wartość straty: 0.044779\tDokładność: 98.67%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.051344\tNajlepsza wartość straty: 0.044779\tDokładność: 98.67%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.059503\tNajlepsza wartość straty: 0.044779\tDokładność: 98.67%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.044567\tNajlepsza wartość straty: 0.044567\tDokładność: 98.87%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.051911\tNajlepsza wartość straty: 0.044567\tDokładność: 98.91%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.083118\tNajlepsza wartość straty: 0.044567\tDokładność: 98.67%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.043989\tNajlepsza wartość straty: 0.043989\tDokładność: 99.02%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.052140\tNajlepsza wartość straty: 0.043989\tDokładność: 99.02%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.045547\tNajlepsza wartość straty: 0.043989\tDokładność: 99.10%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.080638\tNajlepsza wartość straty: 0.043989\tDokładność: 98.67%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.043266\tNajlepsza wartość straty: 0.043266\tDokładność: 98.94%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.054762\tNajlepsza wartość straty: 0.043266\tDokładność: 98.87%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.057143\tNajlepsza wartość straty: 0.043266\tDokładność: 98.87%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.053307\tNajlepsza wartość straty: 0.043266\tDokładność: 98.98%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.051716\tNajlepsza wartość straty: 0.043266\tDokładność: 98.94%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.052054\tNajlepsza wartość straty: 0.043266\tDokładność: 99.02%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.065210\tNajlepsza wartość straty: 0.043266\tDokładność: 98.94%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.076824\tNajlepsza wartość straty: 0.043266\tDokładność: 98.79%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.086455\tNajlepsza wartość straty: 0.043266\tDokładność: 98.71%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.080070\tNajlepsza wartość straty: 0.043266\tDokładność: 98.55%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.063698\tNajlepsza wartość straty: 0.043266\tDokładność: 98.55%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.058989\tNajlepsza wartość straty: 0.043266\tDokładność: 99.02%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.067848\tNajlepsza wartość straty: 0.043266\tDokładność: 98.71%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.103126\tNajlepsza wartość straty: 0.043266\tDokładność: 98.05%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.089583\tNajlepsza wartość straty: 0.043266\tDokładność: 98.75%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.058852\tNajlepsza wartość straty: 0.043266\tDokładność: 98.87%\n",
      "44\tF. straty dla zbioru walidacyjnego: 0.082203\tNajlepsza wartość straty: 0.043266\tDokładność: 98.75%\n",
      "45\tF. straty dla zbioru walidacyjnego: 0.066002\tNajlepsza wartość straty: 0.043266\tDokładność: 98.83%\n",
      "46\tF. straty dla zbioru walidacyjnego: 0.070608\tNajlepsza wartość straty: 0.043266\tDokładność: 98.79%\n",
      "47\tF. straty dla zbioru walidacyjnego: 0.098811\tNajlepsza wartość straty: 0.043266\tDokładność: 98.59%\n",
      "48\tF. straty dla zbioru walidacyjnego: 0.069906\tNajlepsza wartość straty: 0.043266\tDokładność: 98.79%\n",
      "49\tF. straty dla zbioru walidacyjnego: 0.051392\tNajlepsza wartość straty: 0.043266\tDokładność: 99.06%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  22.7s\n",
      "[CV] n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.099187\tNajlepsza wartość straty: 0.099187\tDokładność: 96.60%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.064002\tNajlepsza wartość straty: 0.064002\tDokładność: 98.16%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.062613\tNajlepsza wartość straty: 0.062613\tDokładność: 98.05%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.059415\tNajlepsza wartość straty: 0.059415\tDokładność: 98.05%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.055419\tNajlepsza wartość straty: 0.055419\tDokładność: 98.44%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.055219\tNajlepsza wartość straty: 0.055219\tDokładność: 98.28%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.054918\tNajlepsza wartość straty: 0.054918\tDokładność: 98.55%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.079196\tNajlepsza wartość straty: 0.054918\tDokładność: 97.89%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.070957\tNajlepsza wartość straty: 0.054918\tDokładność: 98.36%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.100464\tNajlepsza wartość straty: 0.054918\tDokładność: 98.05%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.063571\tNajlepsza wartość straty: 0.054918\tDokładność: 98.28%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.053808\tNajlepsza wartość straty: 0.053808\tDokładność: 98.51%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.072826\tNajlepsza wartość straty: 0.053808\tDokładność: 98.51%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.068908\tNajlepsza wartość straty: 0.053808\tDokładność: 98.75%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.099383\tNajlepsza wartość straty: 0.053808\tDokładność: 98.32%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.071935\tNajlepsza wartość straty: 0.053808\tDokładność: 98.48%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.079983\tNajlepsza wartość straty: 0.053808\tDokładność: 98.36%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.054571\tNajlepsza wartość straty: 0.053808\tDokładność: 98.55%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.078281\tNajlepsza wartość straty: 0.053808\tDokładność: 98.67%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.072314\tNajlepsza wartość straty: 0.053808\tDokładność: 98.48%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.083200\tNajlepsza wartość straty: 0.053808\tDokładność: 98.67%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.059466\tNajlepsza wartość straty: 0.053808\tDokładność: 98.59%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.081138\tNajlepsza wartość straty: 0.053808\tDokładność: 98.44%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.089183\tNajlepsza wartość straty: 0.053808\tDokładność: 98.48%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.076765\tNajlepsza wartość straty: 0.053808\tDokładność: 98.55%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.092608\tNajlepsza wartość straty: 0.053808\tDokładność: 98.36%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.064257\tNajlepsza wartość straty: 0.053808\tDokładność: 98.51%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.068952\tNajlepsza wartość straty: 0.053808\tDokładność: 98.48%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.077345\tNajlepsza wartość straty: 0.053808\tDokładność: 98.71%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.077715\tNajlepsza wartość straty: 0.053808\tDokładność: 98.71%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.098251\tNajlepsza wartość straty: 0.053808\tDokładność: 98.75%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.120894\tNajlepsza wartość straty: 0.053808\tDokładność: 98.36%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.085876\tNajlepsza wartość straty: 0.053808\tDokładność: 98.40%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  15.6s\n",
      "[CV] n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.081408\tNajlepsza wartość straty: 0.081408\tDokładność: 97.54%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.070426\tNajlepsza wartość straty: 0.070426\tDokładność: 98.05%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.069743\tNajlepsza wartość straty: 0.069743\tDokładność: 97.73%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.065497\tNajlepsza wartość straty: 0.065497\tDokładność: 98.24%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.061979\tNajlepsza wartość straty: 0.061979\tDokładność: 98.01%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.052188\tNajlepsza wartość straty: 0.052188\tDokładność: 98.24%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.071243\tNajlepsza wartość straty: 0.052188\tDokładność: 98.08%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.053410\tNajlepsza wartość straty: 0.052188\tDokładność: 98.51%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.051987\tNajlepsza wartość straty: 0.051987\tDokładność: 98.36%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.065696\tNajlepsza wartość straty: 0.051987\tDokładność: 98.44%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.076316\tNajlepsza wartość straty: 0.051987\tDokładność: 98.40%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.062761\tNajlepsza wartość straty: 0.051987\tDokładność: 98.75%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.065911\tNajlepsza wartość straty: 0.051987\tDokładność: 98.24%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.057673\tNajlepsza wartość straty: 0.051987\tDokładność: 98.63%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.055960\tNajlepsza wartość straty: 0.051987\tDokładność: 98.40%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.059860\tNajlepsza wartość straty: 0.051987\tDokładność: 98.48%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.066822\tNajlepsza wartość straty: 0.051987\tDokładność: 98.48%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.050725\tNajlepsza wartość straty: 0.050725\tDokładność: 98.87%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.077530\tNajlepsza wartość straty: 0.050725\tDokładność: 98.48%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.069716\tNajlepsza wartość straty: 0.050725\tDokładność: 98.51%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.082722\tNajlepsza wartość straty: 0.050725\tDokładność: 98.44%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.038141\tNajlepsza wartość straty: 0.038141\tDokładność: 98.83%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.055186\tNajlepsza wartość straty: 0.038141\tDokładność: 98.75%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.073129\tNajlepsza wartość straty: 0.038141\tDokładność: 98.75%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.071660\tNajlepsza wartość straty: 0.038141\tDokładność: 98.87%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.084803\tNajlepsza wartość straty: 0.038141\tDokładność: 98.67%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.090579\tNajlepsza wartość straty: 0.038141\tDokładność: 98.55%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.064773\tNajlepsza wartość straty: 0.038141\tDokładność: 98.55%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.088109\tNajlepsza wartość straty: 0.038141\tDokładność: 98.48%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.098467\tNajlepsza wartość straty: 0.038141\tDokładność: 98.63%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.073026\tNajlepsza wartość straty: 0.038141\tDokładność: 98.59%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.057037\tNajlepsza wartość straty: 0.038141\tDokładność: 98.91%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.068301\tNajlepsza wartość straty: 0.038141\tDokładność: 98.91%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.067058\tNajlepsza wartość straty: 0.038141\tDokładność: 98.98%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.081712\tNajlepsza wartość straty: 0.038141\tDokładność: 98.63%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.073240\tNajlepsza wartość straty: 0.038141\tDokładność: 98.55%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.066932\tNajlepsza wartość straty: 0.038141\tDokładność: 98.91%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.072483\tNajlepsza wartość straty: 0.038141\tDokładność: 98.79%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.070695\tNajlepsza wartość straty: 0.038141\tDokładność: 98.91%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.071653\tNajlepsza wartość straty: 0.038141\tDokładność: 98.91%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.072309\tNajlepsza wartość straty: 0.038141\tDokładność: 98.91%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.072957\tNajlepsza wartość straty: 0.038141\tDokładność: 98.91%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.073438\tNajlepsza wartość straty: 0.038141\tDokładność: 98.94%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  19.7s\n",
      "[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.140913\tNajlepsza wartość straty: 0.140913\tDokładność: 95.39%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.082842\tNajlepsza wartość straty: 0.082842\tDokładność: 97.62%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.086206\tNajlepsza wartość straty: 0.082842\tDokładność: 97.38%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.073511\tNajlepsza wartość straty: 0.073511\tDokładność: 97.54%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.062357\tNajlepsza wartość straty: 0.062357\tDokładność: 98.05%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.055234\tNajlepsza wartość straty: 0.055234\tDokładność: 98.36%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.058129\tNajlepsza wartość straty: 0.055234\tDokładność: 98.28%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.058522\tNajlepsza wartość straty: 0.055234\tDokładność: 98.36%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.072047\tNajlepsza wartość straty: 0.055234\tDokładność: 98.05%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.060190\tNajlepsza wartość straty: 0.055234\tDokładność: 98.32%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.060072\tNajlepsza wartość straty: 0.055234\tDokładność: 98.44%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.059313\tNajlepsza wartość straty: 0.055234\tDokładność: 98.59%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.067191\tNajlepsza wartość straty: 0.055234\tDokładność: 98.48%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.088140\tNajlepsza wartość straty: 0.055234\tDokładność: 97.85%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.056577\tNajlepsza wartość straty: 0.055234\tDokładność: 98.59%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.077203\tNajlepsza wartość straty: 0.055234\tDokładność: 98.51%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.057300\tNajlepsza wartość straty: 0.055234\tDokładność: 98.71%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.061469\tNajlepsza wartość straty: 0.055234\tDokładność: 98.55%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.068276\tNajlepsza wartość straty: 0.055234\tDokładność: 98.67%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.064643\tNajlepsza wartość straty: 0.055234\tDokładność: 98.48%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.062747\tNajlepsza wartość straty: 0.055234\tDokładność: 98.75%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.065393\tNajlepsza wartość straty: 0.055234\tDokładność: 98.75%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.071341\tNajlepsza wartość straty: 0.055234\tDokładność: 98.71%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.067728\tNajlepsza wartość straty: 0.055234\tDokładność: 98.87%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.069259\tNajlepsza wartość straty: 0.055234\tDokładność: 98.87%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.065849\tNajlepsza wartość straty: 0.055234\tDokładność: 98.91%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.065198\tNajlepsza wartość straty: 0.055234\tDokładność: 98.94%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  18.0s\n",
      "[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.127968\tNajlepsza wartość straty: 0.127968\tDokładność: 95.93%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.076743\tNajlepsza wartość straty: 0.076743\tDokładność: 97.73%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.059686\tNajlepsza wartość straty: 0.059686\tDokładność: 98.16%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.055498\tNajlepsza wartość straty: 0.055498\tDokładność: 98.40%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.049845\tNajlepsza wartość straty: 0.049845\tDokładność: 98.36%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.050816\tNajlepsza wartość straty: 0.049845\tDokładność: 98.44%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.036362\tNajlepsza wartość straty: 0.036362\tDokładność: 98.91%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.040791\tNajlepsza wartość straty: 0.036362\tDokładność: 98.67%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.055389\tNajlepsza wartość straty: 0.036362\tDokładność: 98.44%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.053494\tNajlepsza wartość straty: 0.036362\tDokładność: 98.48%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.054980\tNajlepsza wartość straty: 0.036362\tDokładność: 98.51%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.060280\tNajlepsza wartość straty: 0.036362\tDokładność: 98.44%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.046992\tNajlepsza wartość straty: 0.036362\tDokładność: 98.87%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.057022\tNajlepsza wartość straty: 0.036362\tDokładność: 98.59%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.055302\tNajlepsza wartość straty: 0.036362\tDokładność: 98.67%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.059650\tNajlepsza wartość straty: 0.036362\tDokładność: 98.75%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.049650\tNajlepsza wartość straty: 0.036362\tDokładność: 98.83%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.072130\tNajlepsza wartość straty: 0.036362\tDokładność: 98.44%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.055394\tNajlepsza wartość straty: 0.036362\tDokładność: 98.91%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.054146\tNajlepsza wartość straty: 0.036362\tDokładność: 98.87%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.072705\tNajlepsza wartość straty: 0.036362\tDokładność: 98.32%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.058888\tNajlepsza wartość straty: 0.036362\tDokładność: 98.67%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.053754\tNajlepsza wartość straty: 0.036362\tDokładność: 98.75%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.044195\tNajlepsza wartość straty: 0.036362\tDokładność: 98.71%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.045840\tNajlepsza wartość straty: 0.036362\tDokładność: 98.75%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.050205\tNajlepsza wartość straty: 0.036362\tDokładność: 98.91%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.035484\tNajlepsza wartość straty: 0.035484\tDokładność: 99.22%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.040640\tNajlepsza wartość straty: 0.035484\tDokładność: 99.02%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.038699\tNajlepsza wartość straty: 0.035484\tDokładność: 99.18%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.040517\tNajlepsza wartość straty: 0.035484\tDokładność: 99.18%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.040903\tNajlepsza wartość straty: 0.035484\tDokładność: 99.18%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.040918\tNajlepsza wartość straty: 0.035484\tDokładność: 99.18%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.040922\tNajlepsza wartość straty: 0.035484\tDokładność: 99.14%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.041372\tNajlepsza wartość straty: 0.035484\tDokładność: 99.14%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.041473\tNajlepsza wartość straty: 0.035484\tDokładność: 99.14%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.041733\tNajlepsza wartość straty: 0.035484\tDokładność: 99.14%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.041887\tNajlepsza wartość straty: 0.035484\tDokładność: 99.14%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.042228\tNajlepsza wartość straty: 0.035484\tDokładność: 99.10%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.042364\tNajlepsza wartość straty: 0.035484\tDokładność: 99.10%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.042701\tNajlepsza wartość straty: 0.035484\tDokładność: 99.10%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.042847\tNajlepsza wartość straty: 0.035484\tDokładność: 99.10%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.043116\tNajlepsza wartość straty: 0.035484\tDokładność: 99.10%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.043327\tNajlepsza wartość straty: 0.035484\tDokładność: 99.10%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.043551\tNajlepsza wartość straty: 0.035484\tDokładność: 99.10%\n",
      "44\tF. straty dla zbioru walidacyjnego: 0.043824\tNajlepsza wartość straty: 0.035484\tDokładność: 99.10%\n",
      "45\tF. straty dla zbioru walidacyjnego: 0.044000\tNajlepsza wartość straty: 0.035484\tDokładność: 99.10%\n",
      "46\tF. straty dla zbioru walidacyjnego: 0.044162\tNajlepsza wartość straty: 0.035484\tDokładność: 99.10%\n",
      "47\tF. straty dla zbioru walidacyjnego: 0.044352\tNajlepsza wartość straty: 0.035484\tDokładność: 99.10%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  29.8s\n",
      "[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.122152\tNajlepsza wartość straty: 0.122152\tDokładność: 95.93%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.085570\tNajlepsza wartość straty: 0.085570\tDokładność: 97.03%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.070516\tNajlepsza wartość straty: 0.070516\tDokładność: 98.01%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.049195\tNajlepsza wartość straty: 0.049195\tDokładność: 98.48%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.051055\tNajlepsza wartość straty: 0.049195\tDokładność: 98.71%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.045650\tNajlepsza wartość straty: 0.045650\tDokładność: 98.55%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.050154\tNajlepsza wartość straty: 0.045650\tDokładność: 98.40%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.043204\tNajlepsza wartość straty: 0.043204\tDokładność: 98.59%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.060054\tNajlepsza wartość straty: 0.043204\tDokładność: 98.28%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.042509\tNajlepsza wartość straty: 0.042509\tDokładność: 98.94%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.054238\tNajlepsza wartość straty: 0.042509\tDokładność: 98.83%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.049449\tNajlepsza wartość straty: 0.042509\tDokładność: 98.67%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.047335\tNajlepsza wartość straty: 0.042509\tDokładność: 98.98%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.045638\tNajlepsza wartość straty: 0.042509\tDokładność: 98.87%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.084760\tNajlepsza wartość straty: 0.042509\tDokładność: 98.08%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.055042\tNajlepsza wartość straty: 0.042509\tDokładność: 98.71%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.051753\tNajlepsza wartość straty: 0.042509\tDokładność: 98.83%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.058361\tNajlepsza wartość straty: 0.042509\tDokładność: 98.94%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.044774\tNajlepsza wartość straty: 0.042509\tDokładność: 98.98%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.070490\tNajlepsza wartość straty: 0.042509\tDokładność: 98.44%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.046227\tNajlepsza wartość straty: 0.042509\tDokładność: 98.94%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.049559\tNajlepsza wartość straty: 0.042509\tDokładność: 98.67%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.061478\tNajlepsza wartość straty: 0.042509\tDokładność: 98.79%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.057828\tNajlepsza wartość straty: 0.042509\tDokładność: 98.83%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.066557\tNajlepsza wartość straty: 0.042509\tDokładność: 98.79%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.063281\tNajlepsza wartość straty: 0.042509\tDokładność: 98.71%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.050980\tNajlepsza wartość straty: 0.042509\tDokładność: 98.94%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.070590\tNajlepsza wartość straty: 0.042509\tDokładność: 98.79%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.058855\tNajlepsza wartość straty: 0.042509\tDokładność: 98.83%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.063898\tNajlepsza wartość straty: 0.042509\tDokładność: 98.71%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.077786\tNajlepsza wartość straty: 0.042509\tDokładność: 98.51%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  19.6s\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.637583\tNajlepsza wartość straty: 1.637583\tDokładność: 19.27%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.621896\tNajlepsza wartość straty: 1.621896\tDokładność: 19.27%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.619831\tNajlepsza wartość straty: 1.619831\tDokładność: 19.27%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.616520\tNajlepsza wartość straty: 1.616520\tDokładność: 19.27%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.654958\tNajlepsza wartość straty: 1.616520\tDokładność: 22.01%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.621479\tNajlepsza wartość straty: 1.616520\tDokładność: 22.01%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.632693\tNajlepsza wartość straty: 1.616520\tDokładność: 19.27%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.614858\tNajlepsza wartość straty: 1.614858\tDokładność: 19.27%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.642606\tNajlepsza wartość straty: 1.614858\tDokładność: 19.27%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.639819\tNajlepsza wartość straty: 1.614858\tDokładność: 19.27%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.615375\tNajlepsza wartość straty: 1.614858\tDokładność: 22.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.630571\tNajlepsza wartość straty: 1.614858\tDokładność: 22.01%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.636737\tNajlepsza wartość straty: 1.614858\tDokładność: 19.08%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.613103\tNajlepsza wartość straty: 1.613103\tDokładność: 19.27%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.629263\tNajlepsza wartość straty: 1.613103\tDokładność: 19.08%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.617149\tNajlepsza wartość straty: 1.613103\tDokładność: 22.01%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.626923\tNajlepsza wartość straty: 1.613103\tDokładność: 18.73%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.608595\tNajlepsza wartość straty: 1.608595\tDokładność: 22.01%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.624737\tNajlepsza wartość straty: 1.608595\tDokładność: 19.27%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.624035\tNajlepsza wartość straty: 1.608595\tDokładność: 20.91%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.615449\tNajlepsza wartość straty: 1.608595\tDokładność: 19.27%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.629650\tNajlepsza wartość straty: 1.608595\tDokładność: 19.27%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.619098\tNajlepsza wartość straty: 1.608595\tDokładność: 19.27%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.621080\tNajlepsza wartość straty: 1.608595\tDokładność: 22.01%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.633989\tNajlepsza wartość straty: 1.608595\tDokładność: 22.01%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.647182\tNajlepsza wartość straty: 1.608595\tDokładność: 22.01%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.614127\tNajlepsza wartość straty: 1.608595\tDokładność: 19.08%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.638548\tNajlepsza wartość straty: 1.608595\tDokładność: 19.27%\n",
      "28\tF. straty dla zbioru walidacyjnego: 1.656264\tNajlepsza wartość straty: 1.608595\tDokładność: 22.01%\n",
      "29\tF. straty dla zbioru walidacyjnego: 1.617747\tNajlepsza wartość straty: 1.608595\tDokładność: 18.73%\n",
      "30\tF. straty dla zbioru walidacyjnego: 1.641874\tNajlepsza wartość straty: 1.608595\tDokładność: 19.27%\n",
      "31\tF. straty dla zbioru walidacyjnego: 1.631482\tNajlepsza wartość straty: 1.608595\tDokładność: 18.73%\n",
      "32\tF. straty dla zbioru walidacyjnego: 1.641183\tNajlepsza wartość straty: 1.608595\tDokładność: 18.73%\n",
      "33\tF. straty dla zbioru walidacyjnego: 1.647446\tNajlepsza wartość straty: 1.608595\tDokładność: 19.08%\n",
      "34\tF. straty dla zbioru walidacyjnego: 1.610284\tNajlepsza wartość straty: 1.608595\tDokładność: 22.01%\n",
      "35\tF. straty dla zbioru walidacyjnego: 1.624692\tNajlepsza wartość straty: 1.608595\tDokładność: 19.08%\n",
      "36\tF. straty dla zbioru walidacyjnego: 1.630852\tNajlepsza wartość straty: 1.608595\tDokładność: 18.73%\n",
      "37\tF. straty dla zbioru walidacyjnego: 1.632281\tNajlepsza wartość straty: 1.608595\tDokładność: 18.73%\n",
      "38\tF. straty dla zbioru walidacyjnego: 1.631021\tNajlepsza wartość straty: 1.608595\tDokładność: 22.01%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x0000029B12839B70>, total= 1.0min\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.631921\tNajlepsza wartość straty: 1.631921\tDokładność: 22.01%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.644836\tNajlepsza wartość straty: 1.631921\tDokładność: 19.08%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.611654\tNajlepsza wartość straty: 1.611654\tDokładność: 22.01%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.614178\tNajlepsza wartość straty: 1.611654\tDokładność: 22.01%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.617810\tNajlepsza wartość straty: 1.611654\tDokładność: 22.01%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.624092\tNajlepsza wartość straty: 1.611654\tDokładność: 22.01%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.635104\tNajlepsza wartość straty: 1.611654\tDokładność: 19.27%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.630919\tNajlepsza wartość straty: 1.611654\tDokładność: 18.73%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.639868\tNajlepsza wartość straty: 1.611654\tDokładność: 22.01%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.611889\tNajlepsza wartość straty: 1.611654\tDokładność: 20.91%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.613289\tNajlepsza wartość straty: 1.611654\tDokładność: 22.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.614457\tNajlepsza wartość straty: 1.611654\tDokładność: 22.01%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.615519\tNajlepsza wartość straty: 1.611654\tDokładność: 20.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.609585\tNajlepsza wartość straty: 1.609585\tDokładność: 22.01%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.628127\tNajlepsza wartość straty: 1.609585\tDokładność: 22.01%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.613724\tNajlepsza wartość straty: 1.609585\tDokładność: 19.08%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.621641\tNajlepsza wartość straty: 1.609585\tDokładność: 22.01%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.612427\tNajlepsza wartość straty: 1.609585\tDokładność: 19.08%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.622173\tNajlepsza wartość straty: 1.609585\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.611825\tNajlepsza wartość straty: 1.609585\tDokładność: 22.01%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.612151\tNajlepsza wartość straty: 1.609585\tDokładność: 20.91%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.657982\tNajlepsza wartość straty: 1.609585\tDokładność: 22.01%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.643388\tNajlepsza wartość straty: 1.609585\tDokładność: 19.27%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.647551\tNajlepsza wartość straty: 1.609585\tDokładność: 22.01%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.652206\tNajlepsza wartość straty: 1.609585\tDokładność: 18.73%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.610976\tNajlepsza wartość straty: 1.609585\tDokładność: 20.91%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.614099\tNajlepsza wartość straty: 1.609585\tDokładność: 20.91%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.614363\tNajlepsza wartość straty: 1.609585\tDokładność: 20.91%\n",
      "28\tF. straty dla zbioru walidacyjnego: 1.642987\tNajlepsza wartość straty: 1.609585\tDokładność: 19.08%\n",
      "29\tF. straty dla zbioru walidacyjnego: 1.614047\tNajlepsza wartość straty: 1.609585\tDokładność: 19.08%\n",
      "30\tF. straty dla zbioru walidacyjnego: 1.609289\tNajlepsza wartość straty: 1.609289\tDokładność: 22.01%\n",
      "31\tF. straty dla zbioru walidacyjnego: 1.634889\tNajlepsza wartość straty: 1.609289\tDokładność: 18.73%\n",
      "32\tF. straty dla zbioru walidacyjnego: 1.614009\tNajlepsza wartość straty: 1.609289\tDokładność: 19.08%\n",
      "33\tF. straty dla zbioru walidacyjnego: 1.619313\tNajlepsza wartość straty: 1.609289\tDokładność: 19.08%\n",
      "34\tF. straty dla zbioru walidacyjnego: 1.611677\tNajlepsza wartość straty: 1.609289\tDokładność: 22.01%\n",
      "35\tF. straty dla zbioru walidacyjnego: 1.638078\tNajlepsza wartość straty: 1.609289\tDokładność: 20.91%\n",
      "36\tF. straty dla zbioru walidacyjnego: 1.621398\tNajlepsza wartość straty: 1.609289\tDokładność: 22.01%\n",
      "37\tF. straty dla zbioru walidacyjnego: 1.617562\tNajlepsza wartość straty: 1.609289\tDokładność: 22.01%\n",
      "38\tF. straty dla zbioru walidacyjnego: 1.620767\tNajlepsza wartość straty: 1.609289\tDokładność: 20.91%\n",
      "39\tF. straty dla zbioru walidacyjnego: 1.627334\tNajlepsza wartość straty: 1.609289\tDokładność: 19.27%\n",
      "40\tF. straty dla zbioru walidacyjnego: 1.607883\tNajlepsza wartość straty: 1.607883\tDokładność: 22.01%\n",
      "41\tF. straty dla zbioru walidacyjnego: 1.634031\tNajlepsza wartość straty: 1.607883\tDokładność: 22.01%\n",
      "42\tF. straty dla zbioru walidacyjnego: 1.618327\tNajlepsza wartość straty: 1.607883\tDokładność: 20.91%\n",
      "43\tF. straty dla zbioru walidacyjnego: 1.618746\tNajlepsza wartość straty: 1.607883\tDokładność: 22.01%\n",
      "44\tF. straty dla zbioru walidacyjnego: 1.621665\tNajlepsza wartość straty: 1.607883\tDokładność: 19.08%\n",
      "45\tF. straty dla zbioru walidacyjnego: 1.629383\tNajlepsza wartość straty: 1.607883\tDokładność: 22.01%\n",
      "46\tF. straty dla zbioru walidacyjnego: 1.637658\tNajlepsza wartość straty: 1.607883\tDokładność: 18.73%\n",
      "47\tF. straty dla zbioru walidacyjnego: 1.609600\tNajlepsza wartość straty: 1.607883\tDokładność: 22.01%\n",
      "48\tF. straty dla zbioru walidacyjnego: 1.612162\tNajlepsza wartość straty: 1.607883\tDokładność: 22.01%\n",
      "49\tF. straty dla zbioru walidacyjnego: 1.643133\tNajlepsza wartość straty: 1.607883\tDokładność: 19.27%\n",
      "50\tF. straty dla zbioru walidacyjnego: 1.611987\tNajlepsza wartość straty: 1.607883\tDokładność: 22.01%\n",
      "51\tF. straty dla zbioru walidacyjnego: 1.619979\tNajlepsza wartość straty: 1.607883\tDokładność: 19.08%\n",
      "52\tF. straty dla zbioru walidacyjnego: 1.627026\tNajlepsza wartość straty: 1.607883\tDokładność: 19.08%\n",
      "53\tF. straty dla zbioru walidacyjnego: 1.618519\tNajlepsza wartość straty: 1.607883\tDokładność: 22.01%\n",
      "54\tF. straty dla zbioru walidacyjnego: 1.654812\tNajlepsza wartość straty: 1.607883\tDokładność: 18.73%\n",
      "55\tF. straty dla zbioru walidacyjnego: 1.645117\tNajlepsza wartość straty: 1.607883\tDokładność: 18.73%\n",
      "56\tF. straty dla zbioru walidacyjnego: 1.669471\tNajlepsza wartość straty: 1.607883\tDokładność: 19.08%\n",
      "57\tF. straty dla zbioru walidacyjnego: 1.619669\tNajlepsza wartość straty: 1.607883\tDokładność: 18.73%\n",
      "58\tF. straty dla zbioru walidacyjnego: 1.612549\tNajlepsza wartość straty: 1.607883\tDokładność: 22.01%\n",
      "59\tF. straty dla zbioru walidacyjnego: 1.619514\tNajlepsza wartość straty: 1.607883\tDokładność: 22.01%\n",
      "60\tF. straty dla zbioru walidacyjnego: 1.635963\tNajlepsza wartość straty: 1.607883\tDokładność: 18.73%\n",
      "61\tF. straty dla zbioru walidacyjnego: 1.627401\tNajlepsza wartość straty: 1.607883\tDokładność: 22.01%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x0000029B12839B70>, total= 1.7min\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.644317\tNajlepsza wartość straty: 1.644317\tDokładność: 19.27%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.624543\tNajlepsza wartość straty: 1.624543\tDokładność: 19.27%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.614159\tNajlepsza wartość straty: 1.614159\tDokładność: 19.27%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.616995\tNajlepsza wartość straty: 1.614159\tDokładność: 19.27%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.614500\tNajlepsza wartość straty: 1.614159\tDokładność: 22.01%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.627706\tNajlepsza wartość straty: 1.614159\tDokładność: 22.01%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.648677\tNajlepsza wartość straty: 1.614159\tDokładność: 19.27%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.627603\tNajlepsza wartość straty: 1.614159\tDokładność: 18.73%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.625684\tNajlepsza wartość straty: 1.614159\tDokładność: 22.01%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.615558\tNajlepsza wartość straty: 1.614159\tDokładność: 20.91%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.619129\tNajlepsza wartość straty: 1.614159\tDokładność: 22.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.639600\tNajlepsza wartość straty: 1.614159\tDokładność: 22.01%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.627613\tNajlepsza wartość straty: 1.614159\tDokładność: 19.08%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.615011\tNajlepsza wartość straty: 1.614159\tDokładność: 19.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.624331\tNajlepsza wartość straty: 1.614159\tDokładność: 19.08%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.612441\tNajlepsza wartość straty: 1.612441\tDokładność: 22.01%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.627839\tNajlepsza wartość straty: 1.612441\tDokładność: 22.01%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.613220\tNajlepsza wartość straty: 1.612441\tDokładność: 18.73%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.615752\tNajlepsza wartość straty: 1.612441\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.619096\tNajlepsza wartość straty: 1.612441\tDokładność: 22.01%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.609111\tNajlepsza wartość straty: 1.609111\tDokładność: 22.01%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.689011\tNajlepsza wartość straty: 1.609111\tDokładność: 22.01%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.618893\tNajlepsza wartość straty: 1.609111\tDokładność: 18.73%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.638928\tNajlepsza wartość straty: 1.609111\tDokładność: 22.01%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.660613\tNajlepsza wartość straty: 1.609111\tDokładność: 20.91%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.619334\tNajlepsza wartość straty: 1.609111\tDokładność: 19.27%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.611323\tNajlepsza wartość straty: 1.609111\tDokładność: 22.01%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.614304\tNajlepsza wartość straty: 1.609111\tDokładność: 20.91%\n",
      "28\tF. straty dla zbioru walidacyjnego: 1.628118\tNajlepsza wartość straty: 1.609111\tDokładność: 19.08%\n",
      "29\tF. straty dla zbioru walidacyjnego: 1.610351\tNajlepsza wartość straty: 1.609111\tDokładność: 20.91%\n",
      "30\tF. straty dla zbioru walidacyjnego: 1.626932\tNajlepsza wartość straty: 1.609111\tDokładność: 20.91%\n",
      "31\tF. straty dla zbioru walidacyjnego: 1.633272\tNajlepsza wartość straty: 1.609111\tDokładność: 18.73%\n",
      "32\tF. straty dla zbioru walidacyjnego: 1.609066\tNajlepsza wartość straty: 1.609066\tDokładność: 22.01%\n",
      "33\tF. straty dla zbioru walidacyjnego: 1.610652\tNajlepsza wartość straty: 1.609066\tDokładność: 19.08%\n",
      "34\tF. straty dla zbioru walidacyjnego: 1.616979\tNajlepsza wartość straty: 1.609066\tDokładność: 22.01%\n",
      "35\tF. straty dla zbioru walidacyjnego: 1.627558\tNajlepsza wartość straty: 1.609066\tDokładność: 19.27%\n",
      "36\tF. straty dla zbioru walidacyjnego: 1.609165\tNajlepsza wartość straty: 1.609066\tDokładność: 20.91%\n",
      "37\tF. straty dla zbioru walidacyjnego: 1.626449\tNajlepsza wartość straty: 1.609066\tDokładność: 19.27%\n",
      "38\tF. straty dla zbioru walidacyjnego: 1.614496\tNajlepsza wartość straty: 1.609066\tDokładność: 22.01%\n",
      "39\tF. straty dla zbioru walidacyjnego: 1.617754\tNajlepsza wartość straty: 1.609066\tDokładność: 19.27%\n",
      "40\tF. straty dla zbioru walidacyjnego: 1.623131\tNajlepsza wartość straty: 1.609066\tDokładność: 22.01%\n",
      "41\tF. straty dla zbioru walidacyjnego: 1.615777\tNajlepsza wartość straty: 1.609066\tDokładność: 19.27%\n",
      "42\tF. straty dla zbioru walidacyjnego: 1.617475\tNajlepsza wartość straty: 1.609066\tDokładność: 19.27%\n",
      "43\tF. straty dla zbioru walidacyjnego: 1.617776\tNajlepsza wartość straty: 1.609066\tDokładność: 19.08%\n",
      "44\tF. straty dla zbioru walidacyjnego: 1.626853\tNajlepsza wartość straty: 1.609066\tDokładność: 22.01%\n",
      "45\tF. straty dla zbioru walidacyjnego: 1.613777\tNajlepsza wartość straty: 1.609066\tDokładność: 22.01%\n",
      "46\tF. straty dla zbioru walidacyjnego: 1.618688\tNajlepsza wartość straty: 1.609066\tDokładność: 22.01%\n",
      "47\tF. straty dla zbioru walidacyjnego: 1.610630\tNajlepsza wartość straty: 1.609066\tDokładność: 22.01%\n",
      "48\tF. straty dla zbioru walidacyjnego: 1.614395\tNajlepsza wartość straty: 1.609066\tDokładność: 18.73%\n",
      "49\tF. straty dla zbioru walidacyjnego: 1.612180\tNajlepsza wartość straty: 1.609066\tDokładność: 19.27%\n",
      "50\tF. straty dla zbioru walidacyjnego: 1.623879\tNajlepsza wartość straty: 1.609066\tDokładność: 19.27%\n",
      "51\tF. straty dla zbioru walidacyjnego: 1.616768\tNajlepsza wartość straty: 1.609066\tDokładność: 22.01%\n",
      "52\tF. straty dla zbioru walidacyjnego: 1.620598\tNajlepsza wartość straty: 1.609066\tDokładność: 18.73%\n",
      "53\tF. straty dla zbioru walidacyjnego: 1.617682\tNajlepsza wartość straty: 1.609066\tDokładność: 22.01%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x0000029B12839B70>, total= 1.5min\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.115713\tNajlepsza wartość straty: 0.115713\tDokładność: 96.87%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.116847\tNajlepsza wartość straty: 0.115713\tDokładność: 96.64%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.100621\tNajlepsza wartość straty: 0.100621\tDokładność: 97.50%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.108586\tNajlepsza wartość straty: 0.100621\tDokładność: 97.15%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.110422\tNajlepsza wartość straty: 0.100621\tDokładność: 97.38%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.093041\tNajlepsza wartość straty: 0.093041\tDokładność: 97.65%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.074106\tNajlepsza wartość straty: 0.074106\tDokładność: 97.73%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.096543\tNajlepsza wartość straty: 0.074106\tDokładność: 97.77%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.079365\tNajlepsza wartość straty: 0.074106\tDokładność: 97.89%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.056395\tNajlepsza wartość straty: 0.056395\tDokładność: 98.20%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.101944\tNajlepsza wartość straty: 0.056395\tDokładność: 97.03%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.085649\tNajlepsza wartość straty: 0.056395\tDokładność: 98.05%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.107478\tNajlepsza wartość straty: 0.056395\tDokładność: 97.38%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.108597\tNajlepsza wartość straty: 0.056395\tDokładność: 97.34%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.096619\tNajlepsza wartość straty: 0.056395\tDokładność: 97.65%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.125277\tNajlepsza wartość straty: 0.056395\tDokładność: 97.38%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.084307\tNajlepsza wartość straty: 0.056395\tDokładność: 98.05%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.086416\tNajlepsza wartość straty: 0.056395\tDokładność: 98.28%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.101300\tNajlepsza wartość straty: 0.056395\tDokładność: 97.58%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.146451\tNajlepsza wartość straty: 0.056395\tDokładność: 96.95%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.133454\tNajlepsza wartość straty: 0.056395\tDokładność: 97.81%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.090577\tNajlepsza wartość straty: 0.056395\tDokładność: 98.20%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.075019\tNajlepsza wartość straty: 0.056395\tDokładność: 98.01%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.123621\tNajlepsza wartość straty: 0.056395\tDokładność: 98.05%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.092643\tNajlepsza wartość straty: 0.056395\tDokładność: 97.62%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.108278\tNajlepsza wartość straty: 0.056395\tDokładność: 97.42%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.097895\tNajlepsza wartość straty: 0.056395\tDokładność: 98.28%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.091020\tNajlepsza wartość straty: 0.056395\tDokładność: 97.93%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.118287\tNajlepsza wartość straty: 0.056395\tDokładność: 97.38%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.107869\tNajlepsza wartość straty: 0.056395\tDokładność: 97.58%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.079274\tNajlepsza wartość straty: 0.056395\tDokładność: 98.32%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=   9.6s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.129524\tNajlepsza wartość straty: 0.129524\tDokładność: 96.29%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.123175\tNajlepsza wartość straty: 0.123175\tDokładność: 96.52%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.090578\tNajlepsza wartość straty: 0.090578\tDokładność: 97.50%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.100976\tNajlepsza wartość straty: 0.090578\tDokładność: 97.26%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.102520\tNajlepsza wartość straty: 0.090578\tDokładność: 96.83%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.134996\tNajlepsza wartość straty: 0.090578\tDokładność: 96.64%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.094062\tNajlepsza wartość straty: 0.090578\tDokładność: 97.46%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.082952\tNajlepsza wartość straty: 0.082952\tDokładność: 97.62%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.085699\tNajlepsza wartość straty: 0.082952\tDokładność: 97.81%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.087963\tNajlepsza wartość straty: 0.082952\tDokładność: 97.65%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.104608\tNajlepsza wartość straty: 0.082952\tDokładność: 96.99%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.092549\tNajlepsza wartość straty: 0.082952\tDokładność: 97.65%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.079640\tNajlepsza wartość straty: 0.079640\tDokładność: 97.97%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.091561\tNajlepsza wartość straty: 0.079640\tDokładność: 97.65%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.098127\tNajlepsza wartość straty: 0.079640\tDokładność: 97.11%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.098924\tNajlepsza wartość straty: 0.079640\tDokładność: 97.50%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.096644\tNajlepsza wartość straty: 0.079640\tDokładność: 97.54%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.114256\tNajlepsza wartość straty: 0.079640\tDokładność: 96.95%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.075090\tNajlepsza wartość straty: 0.075090\tDokładność: 97.77%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.098740\tNajlepsza wartość straty: 0.075090\tDokładność: 97.73%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.115650\tNajlepsza wartość straty: 0.075090\tDokładność: 97.22%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.119497\tNajlepsza wartość straty: 0.075090\tDokładność: 97.58%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.088568\tNajlepsza wartość straty: 0.075090\tDokładność: 97.73%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.077990\tNajlepsza wartość straty: 0.075090\tDokładność: 97.89%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.164485\tNajlepsza wartość straty: 0.075090\tDokładność: 96.91%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.104580\tNajlepsza wartość straty: 0.075090\tDokładność: 97.65%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.091316\tNajlepsza wartość straty: 0.075090\tDokładność: 97.65%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.102297\tNajlepsza wartość straty: 0.075090\tDokładność: 97.26%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.104542\tNajlepsza wartość straty: 0.075090\tDokładność: 97.89%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.088211\tNajlepsza wartość straty: 0.075090\tDokładność: 97.69%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.077659\tNajlepsza wartość straty: 0.075090\tDokładność: 98.01%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.117184\tNajlepsza wartość straty: 0.075090\tDokładność: 98.16%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.100027\tNajlepsza wartość straty: 0.075090\tDokładność: 98.20%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.125208\tNajlepsza wartość straty: 0.075090\tDokładność: 98.08%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.116093\tNajlepsza wartość straty: 0.075090\tDokładność: 97.69%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.115433\tNajlepsza wartość straty: 0.075090\tDokładność: 97.54%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.093954\tNajlepsza wartość straty: 0.075090\tDokładność: 97.77%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.122996\tNajlepsza wartość straty: 0.075090\tDokładność: 97.34%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.103071\tNajlepsza wartość straty: 0.075090\tDokładność: 97.97%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.129940\tNajlepsza wartość straty: 0.075090\tDokładność: 97.22%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=  12.3s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.120269\tNajlepsza wartość straty: 0.120269\tDokładność: 96.40%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.118537\tNajlepsza wartość straty: 0.118537\tDokładność: 97.46%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.088630\tNajlepsza wartość straty: 0.088630\tDokładność: 97.69%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.091086\tNajlepsza wartość straty: 0.088630\tDokładność: 97.73%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.074480\tNajlepsza wartość straty: 0.074480\tDokładność: 98.12%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.093718\tNajlepsza wartość straty: 0.074480\tDokładność: 97.54%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.080615\tNajlepsza wartość straty: 0.074480\tDokładność: 98.01%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.073866\tNajlepsza wartość straty: 0.073866\tDokładność: 98.16%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.101037\tNajlepsza wartość straty: 0.073866\tDokładność: 98.24%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.080386\tNajlepsza wartość straty: 0.073866\tDokładność: 98.24%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.088669\tNajlepsza wartość straty: 0.073866\tDokładność: 97.81%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.082566\tNajlepsza wartość straty: 0.073866\tDokładność: 97.62%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.109005\tNajlepsza wartość straty: 0.073866\tDokładność: 97.34%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.084519\tNajlepsza wartość straty: 0.073866\tDokładność: 98.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.104962\tNajlepsza wartość straty: 0.073866\tDokładność: 97.58%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.084852\tNajlepsza wartość straty: 0.073866\tDokładność: 97.97%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.086790\tNajlepsza wartość straty: 0.073866\tDokładność: 97.97%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.084387\tNajlepsza wartość straty: 0.073866\tDokładność: 97.89%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.086400\tNajlepsza wartość straty: 0.073866\tDokładność: 98.12%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.092807\tNajlepsza wartość straty: 0.073866\tDokładność: 98.16%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.087880\tNajlepsza wartość straty: 0.073866\tDokładność: 98.01%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.097622\tNajlepsza wartość straty: 0.073866\tDokładność: 97.89%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.085171\tNajlepsza wartość straty: 0.073866\tDokładność: 98.08%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.102831\tNajlepsza wartość straty: 0.073866\tDokładność: 97.46%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.095424\tNajlepsza wartość straty: 0.073866\tDokładność: 98.24%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.080903\tNajlepsza wartość straty: 0.073866\tDokładność: 98.28%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.111600\tNajlepsza wartość straty: 0.073866\tDokładność: 98.01%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.076537\tNajlepsza wartość straty: 0.073866\tDokładność: 98.16%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.100445\tNajlepsza wartość straty: 0.073866\tDokładność: 98.08%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=   9.0s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.155068\tNajlepsza wartość straty: 0.155068\tDokładność: 96.72%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.072834\tNajlepsza wartość straty: 0.155068\tDokładność: 62.16%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.372475\tNajlepsza wartość straty: 0.155068\tDokładność: 88.55%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.279823\tNajlepsza wartość straty: 0.155068\tDokładność: 91.91%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.246689\tNajlepsza wartość straty: 0.155068\tDokładność: 93.12%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.218757\tNajlepsza wartość straty: 0.155068\tDokładność: 93.94%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.192163\tNajlepsza wartość straty: 0.155068\tDokładność: 94.37%\n",
      "7\tF. straty dla zbioru walidacyjnego: 146.075409\tNajlepsza wartość straty: 0.155068\tDokładność: 22.48%\n",
      "8\tF. straty dla zbioru walidacyjnego: 3624.194336\tNajlepsza wartość straty: 0.155068\tDokładność: 14.82%\n",
      "9\tF. straty dla zbioru walidacyjnego: 53.839340\tNajlepsza wartość straty: 0.155068\tDokładność: 29.20%\n",
      "10\tF. straty dla zbioru walidacyjnego: 22.007019\tNajlepsza wartość straty: 0.155068\tDokładność: 57.74%\n",
      "11\tF. straty dla zbioru walidacyjnego: 12.444075\tNajlepsza wartość straty: 0.155068\tDokładność: 64.39%\n",
      "12\tF. straty dla zbioru walidacyjnego: 17.008595\tNajlepsza wartość straty: 0.155068\tDokładność: 63.10%\n",
      "13\tF. straty dla zbioru walidacyjnego: 22.390625\tNajlepsza wartość straty: 0.155068\tDokładność: 56.80%\n",
      "14\tF. straty dla zbioru walidacyjnego: 8.328016\tNajlepsza wartość straty: 0.155068\tDokładność: 74.08%\n",
      "15\tF. straty dla zbioru walidacyjnego: 10.540731\tNajlepsza wartość straty: 0.155068\tDokładność: 68.45%\n",
      "16\tF. straty dla zbioru walidacyjnego: 14.824463\tNajlepsza wartość straty: 0.155068\tDokładność: 63.17%\n",
      "17\tF. straty dla zbioru walidacyjnego: 22.299997\tNajlepsza wartość straty: 0.155068\tDokładność: 64.07%\n",
      "18\tF. straty dla zbioru walidacyjnego: 9.108520\tNajlepsza wartość straty: 0.155068\tDokładność: 75.29%\n",
      "19\tF. straty dla zbioru walidacyjnego: 12.487736\tNajlepsza wartość straty: 0.155068\tDokładność: 63.64%\n",
      "20\tF. straty dla zbioru walidacyjnego: 9.104075\tNajlepsza wartość straty: 0.155068\tDokładność: 67.04%\n",
      "21\tF. straty dla zbioru walidacyjnego: 7.866041\tNajlepsza wartość straty: 0.155068\tDokładność: 76.23%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  10.6s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.137470\tNajlepsza wartość straty: 0.137470\tDokładność: 96.21%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.108198\tNajlepsza wartość straty: 0.108198\tDokładność: 97.07%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.175870\tNajlepsza wartość straty: 0.108198\tDokładność: 96.52%\n",
      "3\tF. straty dla zbioru walidacyjnego: 7.435691\tNajlepsza wartość straty: 0.108198\tDokładność: 44.80%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.955465\tNajlepsza wartość straty: 0.108198\tDokładność: 67.24%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.816113\tNajlepsza wartość straty: 0.108198\tDokładność: 71.50%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.690312\tNajlepsza wartość straty: 0.108198\tDokładność: 74.86%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.620058\tNajlepsza wartość straty: 0.108198\tDokładność: 76.90%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.584032\tNajlepsza wartość straty: 0.108198\tDokładność: 78.97%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.561050\tNajlepsza wartość straty: 0.108198\tDokładność: 82.49%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.698263\tNajlepsza wartość straty: 0.108198\tDokładność: 86.43%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.388488\tNajlepsza wartość straty: 0.108198\tDokładność: 88.23%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.369175\tNajlepsza wartość straty: 0.108198\tDokładność: 88.55%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.346608\tNajlepsza wartość straty: 0.108198\tDokładność: 90.11%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.570205\tNajlepsza wartość straty: 0.108198\tDokładność: 83.03%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.340666\tNajlepsza wartość straty: 0.108198\tDokładność: 89.80%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.299421\tNajlepsza wartość straty: 0.108198\tDokładność: 91.48%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.287572\tNajlepsza wartość straty: 0.108198\tDokładność: 91.28%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.367328\tNajlepsza wartość straty: 0.108198\tDokładność: 89.87%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.285016\tNajlepsza wartość straty: 0.108198\tDokładność: 92.42%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.273940\tNajlepsza wartość straty: 0.108198\tDokładność: 91.79%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.284522\tNajlepsza wartość straty: 0.108198\tDokładność: 91.40%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.500875\tNajlepsza wartość straty: 0.108198\tDokładność: 92.53%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  10.8s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.127744\tNajlepsza wartość straty: 0.127744\tDokładność: 96.64%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.179841\tNajlepsza wartość straty: 0.127744\tDokładność: 95.47%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.158286\tNajlepsza wartość straty: 0.127744\tDokładność: 97.30%\n",
      "3\tF. straty dla zbioru walidacyjnego: 11.455379\tNajlepsza wartość straty: 0.127744\tDokładność: 48.59%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.630481\tNajlepsza wartość straty: 0.127744\tDokładność: 82.92%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.484839\tNajlepsza wartość straty: 0.127744\tDokładność: 86.94%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.338266\tNajlepsza wartość straty: 0.127744\tDokładność: 92.26%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.280265\tNajlepsza wartość straty: 0.127744\tDokładność: 93.75%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.268104\tNajlepsza wartość straty: 0.127744\tDokładność: 93.82%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.237387\tNajlepsza wartość straty: 0.127744\tDokładność: 95.27%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.283735\tNajlepsza wartość straty: 0.127744\tDokładność: 93.67%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.227887\tNajlepsza wartość straty: 0.127744\tDokładność: 95.07%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.208581\tNajlepsza wartość straty: 0.127744\tDokładność: 95.62%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.396052\tNajlepsza wartość straty: 0.127744\tDokładność: 93.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.310101\tNajlepsza wartość straty: 0.127744\tDokładność: 94.49%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.256053\tNajlepsza wartość straty: 0.127744\tDokładność: 95.54%\n",
      "16\tF. straty dla zbioru walidacyjnego: 3.831961\tNajlepsza wartość straty: 0.127744\tDokładność: 55.28%\n",
      "17\tF. straty dla zbioru walidacyjnego: 5.145449\tNajlepsza wartość straty: 0.127744\tDokładność: 49.18%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.995660\tNajlepsza wartość straty: 0.127744\tDokładność: 59.07%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.892444\tNajlepsza wartość straty: 0.127744\tDokładność: 59.07%\n",
      "20\tF. straty dla zbioru walidacyjnego: 2.183429\tNajlepsza wartość straty: 0.127744\tDokładność: 60.40%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.372847\tNajlepsza wartość straty: 0.127744\tDokładność: 59.81%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  10.6s\n",
      "[CV] n_neurons=50, learning_rate=0.01, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.093032\tNajlepsza wartość straty: 0.093032\tDokładność: 97.46%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.088227\tNajlepsza wartość straty: 0.088227\tDokładność: 97.54%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.100583\tNajlepsza wartość straty: 0.088227\tDokładność: 97.58%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.140224\tNajlepsza wartość straty: 0.088227\tDokładność: 96.33%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.083658\tNajlepsza wartość straty: 0.083658\tDokładność: 97.54%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.064808\tNajlepsza wartość straty: 0.064808\tDokładność: 98.55%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.065360\tNajlepsza wartość straty: 0.064808\tDokładność: 98.20%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.177213\tNajlepsza wartość straty: 0.064808\tDokładność: 97.07%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.104619\tNajlepsza wartość straty: 0.064808\tDokładność: 97.89%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.110241\tNajlepsza wartość straty: 0.064808\tDokładność: 97.89%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.107910\tNajlepsza wartość straty: 0.064808\tDokładność: 98.08%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.140232\tNajlepsza wartość straty: 0.064808\tDokładność: 98.05%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.124025\tNajlepsza wartość straty: 0.064808\tDokładność: 98.08%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.121643\tNajlepsza wartość straty: 0.064808\tDokładność: 98.40%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.200970\tNajlepsza wartość straty: 0.064808\tDokładność: 98.28%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.079988\tNajlepsza wartość straty: 0.064808\tDokładność: 98.28%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.127818\tNajlepsza wartość straty: 0.064808\tDokładność: 98.44%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.208261\tNajlepsza wartość straty: 0.064808\tDokładność: 97.07%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.135112\tNajlepsza wartość straty: 0.064808\tDokładność: 97.34%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.116741\tNajlepsza wartość straty: 0.064808\tDokładność: 97.73%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.244508\tNajlepsza wartość straty: 0.064808\tDokładność: 98.24%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.157297\tNajlepsza wartość straty: 0.064808\tDokładność: 97.81%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.121554\tNajlepsza wartość straty: 0.064808\tDokładność: 98.16%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.154864\tNajlepsza wartość straty: 0.064808\tDokładność: 98.71%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.195426\tNajlepsza wartość straty: 0.064808\tDokładność: 97.46%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.214001\tNajlepsza wartość straty: 0.064808\tDokładność: 98.87%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.260616\tNajlepsza wartość straty: 0.064808\tDokładność: 98.20%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.01, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  17.3s\n",
      "[CV] n_neurons=50, learning_rate=0.01, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.082086\tNajlepsza wartość straty: 0.082086\tDokładność: 97.89%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.088669\tNajlepsza wartość straty: 0.082086\tDokładność: 97.62%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.077566\tNajlepsza wartość straty: 0.077566\tDokładność: 98.01%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.072786\tNajlepsza wartość straty: 0.072786\tDokładność: 98.16%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.099621\tNajlepsza wartość straty: 0.072786\tDokładność: 97.81%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.093004\tNajlepsza wartość straty: 0.072786\tDokładność: 98.08%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.098110\tNajlepsza wartość straty: 0.072786\tDokładność: 98.01%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.097784\tNajlepsza wartość straty: 0.072786\tDokładność: 97.54%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.139409\tNajlepsza wartość straty: 0.072786\tDokładność: 97.34%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.130245\tNajlepsza wartość straty: 0.072786\tDokładność: 97.65%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.106104\tNajlepsza wartość straty: 0.072786\tDokładność: 98.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.095913\tNajlepsza wartość straty: 0.072786\tDokładność: 98.32%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.113184\tNajlepsza wartość straty: 0.072786\tDokładność: 98.12%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.077468\tNajlepsza wartość straty: 0.072786\tDokładność: 98.63%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.067221\tNajlepsza wartość straty: 0.067221\tDokładność: 98.36%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.098578\tNajlepsza wartość straty: 0.067221\tDokładność: 98.48%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.166823\tNajlepsza wartość straty: 0.067221\tDokładność: 97.81%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.276005\tNajlepsza wartość straty: 0.067221\tDokładność: 96.36%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.095533\tNajlepsza wartość straty: 0.067221\tDokładność: 97.58%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.093897\tNajlepsza wartość straty: 0.067221\tDokładność: 97.62%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.089848\tNajlepsza wartość straty: 0.067221\tDokładność: 98.24%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.087136\tNajlepsza wartość straty: 0.067221\tDokładność: 98.08%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.098258\tNajlepsza wartość straty: 0.067221\tDokładność: 98.08%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.113116\tNajlepsza wartość straty: 0.067221\tDokładność: 98.51%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.100134\tNajlepsza wartość straty: 0.067221\tDokładność: 97.97%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.162323\tNajlepsza wartość straty: 0.067221\tDokładność: 98.48%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.182098\tNajlepsza wartość straty: 0.067221\tDokładność: 97.81%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.122535\tNajlepsza wartość straty: 0.067221\tDokładność: 97.97%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.090793\tNajlepsza wartość straty: 0.067221\tDokładność: 98.36%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.206864\tNajlepsza wartość straty: 0.067221\tDokładność: 97.58%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.323726\tNajlepsza wartość straty: 0.067221\tDokładność: 96.05%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.780947\tNajlepsza wartość straty: 0.067221\tDokładność: 97.34%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.412918\tNajlepsza wartość straty: 0.067221\tDokładność: 97.50%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.271440\tNajlepsza wartość straty: 0.067221\tDokładność: 97.38%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.236158\tNajlepsza wartość straty: 0.067221\tDokładność: 97.97%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.366166\tNajlepsza wartość straty: 0.067221\tDokładność: 98.08%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.01, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  22.8s\n",
      "[CV] n_neurons=50, learning_rate=0.01, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.119908\tNajlepsza wartość straty: 0.119908\tDokładność: 96.36%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.089693\tNajlepsza wartość straty: 0.089693\tDokładność: 97.89%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.100247\tNajlepsza wartość straty: 0.089693\tDokładność: 97.65%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.099505\tNajlepsza wartość straty: 0.089693\tDokładność: 98.05%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.465006\tNajlepsza wartość straty: 0.089693\tDokładność: 92.26%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.071123\tNajlepsza wartość straty: 0.071123\tDokładność: 98.08%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.069868\tNajlepsza wartość straty: 0.069868\tDokładność: 98.12%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.072226\tNajlepsza wartość straty: 0.069868\tDokładność: 98.36%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.082751\tNajlepsza wartość straty: 0.069868\tDokładność: 98.05%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.149023\tNajlepsza wartość straty: 0.069868\tDokładność: 97.50%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.165877\tNajlepsza wartość straty: 0.069868\tDokładność: 97.77%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.091632\tNajlepsza wartość straty: 0.069868\tDokładność: 98.32%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.072210\tNajlepsza wartość straty: 0.069868\tDokładność: 98.51%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.070385\tNajlepsza wartość straty: 0.069868\tDokładność: 98.44%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.125215\tNajlepsza wartość straty: 0.069868\tDokładność: 98.32%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.157806\tNajlepsza wartość straty: 0.069868\tDokładność: 95.93%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.101400\tNajlepsza wartość straty: 0.069868\tDokładność: 98.48%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.096783\tNajlepsza wartość straty: 0.069868\tDokładność: 98.36%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.085091\tNajlepsza wartość straty: 0.069868\tDokładność: 98.51%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.105189\tNajlepsza wartość straty: 0.069868\tDokładność: 98.36%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.149499\tNajlepsza wartość straty: 0.069868\tDokładność: 96.95%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.094884\tNajlepsza wartość straty: 0.069868\tDokładność: 98.08%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.120568\tNajlepsza wartość straty: 0.069868\tDokładność: 98.20%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.180195\tNajlepsza wartość straty: 0.069868\tDokładność: 97.93%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.175157\tNajlepsza wartość straty: 0.069868\tDokładność: 98.08%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.506627\tNajlepsza wartość straty: 0.069868\tDokładność: 97.62%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.224638\tNajlepsza wartość straty: 0.069868\tDokładność: 98.32%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.245384\tNajlepsza wartość straty: 0.069868\tDokładność: 98.28%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.01, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  18.9s\n",
      "[CV] n_neurons=120, learning_rate=0.02, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.382084\tNajlepsza wartość straty: 0.382084\tDokładność: 94.45%\n",
      "1\tF. straty dla zbioru walidacyjnego: 107.048584\tNajlepsza wartość straty: 0.382084\tDokładność: 93.71%\n",
      "2\tF. straty dla zbioru walidacyjnego: 69.264877\tNajlepsza wartość straty: 0.382084\tDokładność: 91.63%\n",
      "3\tF. straty dla zbioru walidacyjnego: 939.496277\tNajlepsza wartość straty: 0.382084\tDokładność: 93.71%\n",
      "4\tF. straty dla zbioru walidacyjnego: 255.264053\tNajlepsza wartość straty: 0.382084\tDokładność: 96.44%\n",
      "5\tF. straty dla zbioru walidacyjnego: 218.873444\tNajlepsza wartość straty: 0.382084\tDokładność: 96.68%\n",
      "6\tF. straty dla zbioru walidacyjnego: 2008.366699\tNajlepsza wartość straty: 0.382084\tDokładność: 95.90%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1259.619385\tNajlepsza wartość straty: 0.382084\tDokładność: 95.50%\n",
      "8\tF. straty dla zbioru walidacyjnego: 11483.472656\tNajlepsza wartość straty: 0.382084\tDokładność: 87.84%\n",
      "9\tF. straty dla zbioru walidacyjnego: 447.115234\tNajlepsza wartość straty: 0.382084\tDokładność: 96.91%\n",
      "10\tF. straty dla zbioru walidacyjnego: 574.205322\tNajlepsza wartość straty: 0.382084\tDokładność: 94.45%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1284.447266\tNajlepsza wartość straty: 0.382084\tDokładność: 96.68%\n",
      "12\tF. straty dla zbioru walidacyjnego: 749.765320\tNajlepsza wartość straty: 0.382084\tDokładność: 96.36%\n",
      "13\tF. straty dla zbioru walidacyjnego: 2453.875977\tNajlepsza wartość straty: 0.382084\tDokładność: 96.33%\n",
      "14\tF. straty dla zbioru walidacyjnego: 4892.819336\tNajlepsza wartość straty: 0.382084\tDokładność: 96.21%\n",
      "15\tF. straty dla zbioru walidacyjnego: 2134.707764\tNajlepsza wartość straty: 0.382084\tDokładność: 95.78%\n",
      "16\tF. straty dla zbioru walidacyjnego: 16816.087891\tNajlepsza wartość straty: 0.382084\tDokładność: 89.60%\n",
      "17\tF. straty dla zbioru walidacyjnego: 2751.794678\tNajlepsza wartość straty: 0.382084\tDokładność: 97.50%\n",
      "18\tF. straty dla zbioru walidacyjnego: 7415.102539\tNajlepsza wartość straty: 0.382084\tDokładność: 96.13%\n",
      "19\tF. straty dla zbioru walidacyjnego: 6835.735840\tNajlepsza wartość straty: 0.382084\tDokładność: 95.35%\n",
      "20\tF. straty dla zbioru walidacyjnego: 3434.847656\tNajlepsza wartość straty: 0.382084\tDokładność: 93.98%\n",
      "21\tF. straty dla zbioru walidacyjnego: 17573.439453\tNajlepsza wartość straty: 0.382084\tDokładność: 97.30%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.02, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total= 1.2min\n",
      "[CV] n_neurons=120, learning_rate=0.02, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.779191\tNajlepsza wartość straty: 1.779191\tDokładność: 95.35%\n",
      "1\tF. straty dla zbioru walidacyjnego: 350.909760\tNajlepsza wartość straty: 1.779191\tDokładność: 93.28%\n",
      "2\tF. straty dla zbioru walidacyjnego: 213.393173\tNajlepsza wartość straty: 1.779191\tDokładność: 91.87%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1099.188477\tNajlepsza wartość straty: 1.779191\tDokładność: 95.86%\n",
      "4\tF. straty dla zbioru walidacyjnego: 362.695160\tNajlepsza wartość straty: 1.779191\tDokładność: 94.45%\n",
      "5\tF. straty dla zbioru walidacyjnego: 25.333813\tNajlepsza wartość straty: 1.779191\tDokładność: 96.83%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1106.991943\tNajlepsza wartość straty: 1.779191\tDokładność: 95.58%\n",
      "7\tF. straty dla zbioru walidacyjnego: 741.379272\tNajlepsza wartość straty: 1.779191\tDokładność: 96.56%\n",
      "8\tF. straty dla zbioru walidacyjnego: 532.852844\tNajlepsza wartość straty: 1.779191\tDokładność: 95.70%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1131.131714\tNajlepsza wartość straty: 1.779191\tDokładność: 94.41%\n",
      "10\tF. straty dla zbioru walidacyjnego: 332.792786\tNajlepsza wartość straty: 1.779191\tDokładność: 94.80%\n",
      "11\tF. straty dla zbioru walidacyjnego: 38853.316406\tNajlepsza wartość straty: 1.779191\tDokładność: 97.11%\n",
      "12\tF. straty dla zbioru walidacyjnego: 184045.453125\tNajlepsza wartość straty: 1.779191\tDokładność: 96.83%\n",
      "13\tF. straty dla zbioru walidacyjnego: 152851.156250\tNajlepsza wartość straty: 1.779191\tDokładność: 96.25%\n",
      "14\tF. straty dla zbioru walidacyjnego: 4360.989258\tNajlepsza wartość straty: 1.779191\tDokładność: 97.46%\n",
      "15\tF. straty dla zbioru walidacyjnego: 5734.652344\tNajlepsza wartość straty: 1.779191\tDokładność: 96.79%\n",
      "16\tF. straty dla zbioru walidacyjnego: 14958.007812\tNajlepsza wartość straty: 1.779191\tDokładność: 81.86%\n",
      "17\tF. straty dla zbioru walidacyjnego: 992.584045\tNajlepsza wartość straty: 1.779191\tDokładność: 97.85%\n",
      "18\tF. straty dla zbioru walidacyjnego: 7236.906250\tNajlepsza wartość straty: 1.779191\tDokładność: 97.11%\n",
      "19\tF. straty dla zbioru walidacyjnego: 4666.975586\tNajlepsza wartość straty: 1.779191\tDokładność: 98.16%\n",
      "20\tF. straty dla zbioru walidacyjnego: 4732.598145\tNajlepsza wartość straty: 1.779191\tDokładność: 97.65%\n",
      "21\tF. straty dla zbioru walidacyjnego: 6202.767578\tNajlepsza wartość straty: 1.779191\tDokładność: 97.11%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.02, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total= 1.2min\n",
      "[CV] n_neurons=120, learning_rate=0.02, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.119167\tNajlepsza wartość straty: 1.119167\tDokładność: 86.28%\n",
      "1\tF. straty dla zbioru walidacyjnego: 169.414001\tNajlepsza wartość straty: 1.119167\tDokładność: 94.64%\n",
      "2\tF. straty dla zbioru walidacyjnego: 49.198154\tNajlepsza wartość straty: 1.119167\tDokładność: 95.07%\n",
      "3\tF. straty dla zbioru walidacyjnego: 27.454670\tNajlepsza wartość straty: 1.119167\tDokładność: 91.67%\n",
      "4\tF. straty dla zbioru walidacyjnego: 322.484863\tNajlepsza wartość straty: 1.119167\tDokładność: 94.02%\n",
      "5\tF. straty dla zbioru walidacyjnego: 131.748810\tNajlepsza wartość straty: 1.119167\tDokładność: 88.58%\n",
      "6\tF. straty dla zbioru walidacyjnego: 702.307983\tNajlepsza wartość straty: 1.119167\tDokładność: 95.19%\n",
      "7\tF. straty dla zbioru walidacyjnego: 479.247498\tNajlepsza wartość straty: 1.119167\tDokładność: 95.27%\n",
      "8\tF. straty dla zbioru walidacyjnego: 187.893845\tNajlepsza wartość straty: 1.119167\tDokładność: 95.07%\n",
      "9\tF. straty dla zbioru walidacyjnego: 460.124329\tNajlepsza wartość straty: 1.119167\tDokładność: 92.96%\n",
      "10\tF. straty dla zbioru walidacyjnego: 275.937256\tNajlepsza wartość straty: 1.119167\tDokładność: 96.44%\n",
      "11\tF. straty dla zbioru walidacyjnego: 3288.386963\tNajlepsza wartość straty: 1.119167\tDokładność: 96.33%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1444.697754\tNajlepsza wartość straty: 1.119167\tDokładność: 96.87%\n",
      "13\tF. straty dla zbioru walidacyjnego: 10344.083008\tNajlepsza wartość straty: 1.119167\tDokładność: 96.87%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1447.777222\tNajlepsza wartość straty: 1.119167\tDokładność: 96.33%\n",
      "15\tF. straty dla zbioru walidacyjnego: 8655.208984\tNajlepsza wartość straty: 1.119167\tDokładność: 96.40%\n",
      "16\tF. straty dla zbioru walidacyjnego: 4238.400879\tNajlepsza wartość straty: 1.119167\tDokładność: 97.58%\n",
      "17\tF. straty dla zbioru walidacyjnego: 6700.694336\tNajlepsza wartość straty: 1.119167\tDokładność: 94.88%\n",
      "18\tF. straty dla zbioru walidacyjnego: 3128.266846\tNajlepsza wartość straty: 1.119167\tDokładność: 97.65%\n",
      "19\tF. straty dla zbioru walidacyjnego: 2102.996826\tNajlepsza wartość straty: 1.119167\tDokładność: 97.34%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1228.831543\tNajlepsza wartość straty: 1.119167\tDokładność: 97.07%\n",
      "21\tF. straty dla zbioru walidacyjnego: 6474.503418\tNajlepsza wartość straty: 1.119167\tDokładność: 96.09%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.02, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total= 1.2min\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.272108\tNajlepsza wartość straty: 1.272108\tDokładność: 64.03%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.580619\tNajlepsza wartość straty: 0.580619\tDokładność: 73.65%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.655595\tNajlepsza wartość straty: 0.580619\tDokładność: 75.57%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.494421\tNajlepsza wartość straty: 0.494421\tDokładność: 85.14%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.411038\tNajlepsza wartość straty: 0.411038\tDokładność: 88.08%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.296308\tNajlepsza wartość straty: 0.296308\tDokładność: 92.34%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.216129\tNajlepsza wartość straty: 0.216129\tDokładność: 94.64%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.245740\tNajlepsza wartość straty: 0.216129\tDokładność: 93.82%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.203502\tNajlepsza wartość straty: 0.203502\tDokładność: 94.49%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.150502\tNajlepsza wartość straty: 0.150502\tDokładność: 96.36%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.152300\tNajlepsza wartość straty: 0.150502\tDokładność: 96.56%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.151420\tNajlepsza wartość straty: 0.150502\tDokładność: 96.60%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.151008\tNajlepsza wartość straty: 0.150502\tDokładność: 96.79%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.148440\tNajlepsza wartość straty: 0.148440\tDokładność: 96.72%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.160798\tNajlepsza wartość straty: 0.148440\tDokładność: 96.48%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.137761\tNajlepsza wartość straty: 0.137761\tDokładność: 97.07%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.129506\tNajlepsza wartość straty: 0.129506\tDokładność: 97.07%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.138746\tNajlepsza wartość straty: 0.129506\tDokładność: 97.19%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.148352\tNajlepsza wartość straty: 0.129506\tDokładność: 96.95%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.143450\tNajlepsza wartość straty: 0.129506\tDokładność: 96.95%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.165780\tNajlepsza wartość straty: 0.129506\tDokładność: 96.76%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.150448\tNajlepsza wartość straty: 0.129506\tDokładność: 96.99%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.142293\tNajlepsza wartość straty: 0.129506\tDokładność: 96.95%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.149169\tNajlepsza wartość straty: 0.129506\tDokładność: 97.07%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.150325\tNajlepsza wartość straty: 0.129506\tDokładność: 96.60%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.149531\tNajlepsza wartość straty: 0.129506\tDokładność: 96.64%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.158175\tNajlepsza wartość straty: 0.129506\tDokładność: 96.76%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.148896\tNajlepsza wartość straty: 0.129506\tDokładność: 97.38%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.154992\tNajlepsza wartość straty: 0.129506\tDokładność: 97.34%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.189601\tNajlepsza wartość straty: 0.129506\tDokładność: 96.91%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.143703\tNajlepsza wartość straty: 0.129506\tDokładność: 97.07%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.153853\tNajlepsza wartość straty: 0.129506\tDokładność: 97.22%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.164142\tNajlepsza wartość straty: 0.129506\tDokładność: 97.22%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.163073\tNajlepsza wartość straty: 0.129506\tDokładność: 97.19%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.173627\tNajlepsza wartość straty: 0.129506\tDokładność: 96.99%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.158202\tNajlepsza wartość straty: 0.129506\tDokładność: 97.22%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.218453\tNajlepsza wartość straty: 0.129506\tDokładność: 97.30%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.184275\tNajlepsza wartość straty: 0.129506\tDokładność: 96.87%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  12.4s\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.767355\tNajlepsza wartość straty: 0.767355\tDokładność: 78.15%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.386712\tNajlepsza wartość straty: 0.386712\tDokładność: 83.39%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.287707\tNajlepsza wartość straty: 0.287707\tDokładność: 89.41%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.196787\tNajlepsza wartość straty: 0.196787\tDokładność: 93.43%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.172127\tNajlepsza wartość straty: 0.172127\tDokładność: 94.61%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.147391\tNajlepsza wartość straty: 0.147391\tDokładność: 95.82%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.175551\tNajlepsza wartość straty: 0.147391\tDokładność: 94.84%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.176078\tNajlepsza wartość straty: 0.147391\tDokładność: 95.11%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.151616\tNajlepsza wartość straty: 0.147391\tDokładność: 95.62%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.126426\tNajlepsza wartość straty: 0.126426\tDokładność: 96.40%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.140218\tNajlepsza wartość straty: 0.126426\tDokładność: 96.17%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.116958\tNajlepsza wartość straty: 0.116958\tDokładność: 96.40%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.136714\tNajlepsza wartość straty: 0.116958\tDokładność: 96.09%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.128911\tNajlepsza wartość straty: 0.116958\tDokładność: 96.76%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.123992\tNajlepsza wartość straty: 0.116958\tDokładność: 96.48%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.120619\tNajlepsza wartość straty: 0.116958\tDokładność: 96.56%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.129562\tNajlepsza wartość straty: 0.116958\tDokładność: 96.60%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.119383\tNajlepsza wartość straty: 0.116958\tDokładność: 96.56%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.127947\tNajlepsza wartość straty: 0.116958\tDokładność: 96.60%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.118663\tNajlepsza wartość straty: 0.116958\tDokładność: 96.83%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.122535\tNajlepsza wartość straty: 0.116958\tDokładność: 96.76%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.114138\tNajlepsza wartość straty: 0.114138\tDokładność: 96.44%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.127844\tNajlepsza wartość straty: 0.114138\tDokładność: 96.64%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.141148\tNajlepsza wartość straty: 0.114138\tDokładność: 96.64%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.121044\tNajlepsza wartość straty: 0.114138\tDokładność: 96.60%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.142495\tNajlepsza wartość straty: 0.114138\tDokładność: 96.56%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.138632\tNajlepsza wartość straty: 0.114138\tDokładność: 96.83%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.126579\tNajlepsza wartość straty: 0.114138\tDokładność: 96.72%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.164515\tNajlepsza wartość straty: 0.114138\tDokładność: 96.05%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.188313\tNajlepsza wartość straty: 0.114138\tDokładność: 95.90%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.139143\tNajlepsza wartość straty: 0.114138\tDokładność: 96.36%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.122659\tNajlepsza wartość straty: 0.114138\tDokładność: 96.83%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.145541\tNajlepsza wartość straty: 0.114138\tDokładność: 96.48%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.135483\tNajlepsza wartość straty: 0.114138\tDokładność: 96.17%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.135048\tNajlepsza wartość straty: 0.114138\tDokładność: 96.76%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.142317\tNajlepsza wartość straty: 0.114138\tDokładność: 96.56%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.156534\tNajlepsza wartość straty: 0.114138\tDokładność: 96.56%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.137742\tNajlepsza wartość straty: 0.114138\tDokładność: 96.79%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.153116\tNajlepsza wartość straty: 0.114138\tDokładność: 96.60%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.158407\tNajlepsza wartość straty: 0.114138\tDokładność: 96.68%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.202634\tNajlepsza wartość straty: 0.114138\tDokładność: 95.78%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.126115\tNajlepsza wartość straty: 0.114138\tDokładność: 96.79%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.140091\tNajlepsza wartość straty: 0.114138\tDokładność: 96.95%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  13.8s\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.870433\tNajlepsza wartość straty: 0.870433\tDokładność: 68.57%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.394312\tNajlepsza wartość straty: 0.394312\tDokładność: 86.67%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.266387\tNajlepsza wartość straty: 0.266387\tDokładność: 92.34%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.203651\tNajlepsza wartość straty: 0.203651\tDokładność: 94.14%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.174143\tNajlepsza wartość straty: 0.174143\tDokładność: 95.07%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.159555\tNajlepsza wartość straty: 0.159555\tDokładność: 95.82%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.160493\tNajlepsza wartość straty: 0.159555\tDokładność: 95.90%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.148725\tNajlepsza wartość straty: 0.148725\tDokładność: 96.52%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.147568\tNajlepsza wartość straty: 0.147568\tDokładność: 96.05%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.127581\tNajlepsza wartość straty: 0.127581\tDokładność: 96.87%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.149854\tNajlepsza wartość straty: 0.127581\tDokładność: 96.33%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.135057\tNajlepsza wartość straty: 0.127581\tDokładność: 96.76%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.137953\tNajlepsza wartość straty: 0.127581\tDokładność: 97.03%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.149482\tNajlepsza wartość straty: 0.127581\tDokładność: 96.52%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.139554\tNajlepsza wartość straty: 0.127581\tDokładność: 96.36%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.118730\tNajlepsza wartość straty: 0.118730\tDokładność: 96.72%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.130080\tNajlepsza wartość straty: 0.118730\tDokładność: 96.72%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.142346\tNajlepsza wartość straty: 0.118730\tDokładność: 96.52%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.137026\tNajlepsza wartość straty: 0.118730\tDokładność: 96.60%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.137784\tNajlepsza wartość straty: 0.118730\tDokładność: 96.64%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.142487\tNajlepsza wartość straty: 0.118730\tDokładność: 96.40%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.127977\tNajlepsza wartość straty: 0.118730\tDokładność: 96.76%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.126880\tNajlepsza wartość straty: 0.118730\tDokładność: 96.91%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.132015\tNajlepsza wartość straty: 0.118730\tDokładność: 96.60%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.120097\tNajlepsza wartość straty: 0.118730\tDokładność: 97.03%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.126119\tNajlepsza wartość straty: 0.118730\tDokładność: 96.87%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.120682\tNajlepsza wartość straty: 0.118730\tDokładność: 96.83%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.141481\tNajlepsza wartość straty: 0.118730\tDokładność: 96.79%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.123841\tNajlepsza wartość straty: 0.118730\tDokładność: 96.76%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.115421\tNajlepsza wartość straty: 0.115421\tDokładność: 97.19%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.128392\tNajlepsza wartość straty: 0.115421\tDokładność: 96.76%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.164271\tNajlepsza wartość straty: 0.115421\tDokładność: 96.68%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.139705\tNajlepsza wartość straty: 0.115421\tDokładność: 96.91%\n",
      "33\tF. straty dla zbioru walidacyjnego: 1.187211\tNajlepsza wartość straty: 0.115421\tDokładność: 51.49%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.835541\tNajlepsza wartość straty: 0.115421\tDokładność: 62.31%\n",
      "35\tF. straty dla zbioru walidacyjnego: 1.692609\tNajlepsza wartość straty: 0.115421\tDokładność: 20.45%\n",
      "36\tF. straty dla zbioru walidacyjnego: 1.628255\tNajlepsza wartość straty: 0.115421\tDokładność: 19.27%\n",
      "37\tF. straty dla zbioru walidacyjnego: 1.625767\tNajlepsza wartość straty: 0.115421\tDokładność: 20.91%\n",
      "38\tF. straty dla zbioru walidacyjnego: 1.652795\tNajlepsza wartość straty: 0.115421\tDokładność: 19.27%\n",
      "39\tF. straty dla zbioru walidacyjnego: 1.611569\tNajlepsza wartość straty: 0.115421\tDokładność: 19.27%\n",
      "40\tF. straty dla zbioru walidacyjnego: 1.617028\tNajlepsza wartość straty: 0.115421\tDokładność: 19.08%\n",
      "41\tF. straty dla zbioru walidacyjnego: 1.614935\tNajlepsza wartość straty: 0.115421\tDokładność: 18.73%\n",
      "42\tF. straty dla zbioru walidacyjnego: 1.617553\tNajlepsza wartość straty: 0.115421\tDokładność: 19.27%\n",
      "43\tF. straty dla zbioru walidacyjnego: 1.643615\tNajlepsza wartość straty: 0.115421\tDokładność: 22.01%\n",
      "44\tF. straty dla zbioru walidacyjnego: 1.612774\tNajlepsza wartość straty: 0.115421\tDokładność: 22.01%\n",
      "45\tF. straty dla zbioru walidacyjnego: 1.625383\tNajlepsza wartość straty: 0.115421\tDokładność: 19.08%\n",
      "46\tF. straty dla zbioru walidacyjnego: 1.615959\tNajlepsza wartość straty: 0.115421\tDokładność: 19.27%\n",
      "47\tF. straty dla zbioru walidacyjnego: 1.635610\tNajlepsza wartość straty: 0.115421\tDokładność: 19.27%\n",
      "48\tF. straty dla zbioru walidacyjnego: 1.622286\tNajlepsza wartość straty: 0.115421\tDokładność: 22.01%\n",
      "49\tF. straty dla zbioru walidacyjnego: 1.632445\tNajlepsza wartość straty: 0.115421\tDokładność: 19.08%\n",
      "50\tF. straty dla zbioru walidacyjnego: 1.623318\tNajlepsza wartość straty: 0.115421\tDokładność: 20.91%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  16.2s\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 8273.743164\tNajlepsza wartość straty: 8273.743164\tDokładność: 19.08%\n",
      "1\tF. straty dla zbioru walidacyjnego: 636.334595\tNajlepsza wartość straty: 636.334595\tDokładność: 49.57%\n",
      "2\tF. straty dla zbioru walidacyjnego: 197.580826\tNajlepsza wartość straty: 197.580826\tDokładność: 55.59%\n",
      "3\tF. straty dla zbioru walidacyjnego: 81.907646\tNajlepsza wartość straty: 81.907646\tDokładność: 61.38%\n",
      "4\tF. straty dla zbioru walidacyjnego: 165.526901\tNajlepsza wartość straty: 81.907646\tDokładność: 52.19%\n",
      "5\tF. straty dla zbioru walidacyjnego: 35.362957\tNajlepsza wartość straty: 35.362957\tDokładność: 69.59%\n",
      "6\tF. straty dla zbioru walidacyjnego: 31.626640\tNajlepsza wartość straty: 31.626640\tDokładność: 68.06%\n",
      "7\tF. straty dla zbioru walidacyjnego: 48.981812\tNajlepsza wartość straty: 31.626640\tDokładność: 51.09%\n",
      "8\tF. straty dla zbioru walidacyjnego: 490241.406250\tNajlepsza wartość straty: 31.626640\tDokładność: 25.10%\n",
      "9\tF. straty dla zbioru walidacyjnego: 8408.346680\tNajlepsza wartość straty: 31.626640\tDokładność: 55.00%\n",
      "10\tF. straty dla zbioru walidacyjnego: 14100.883789\tNajlepsza wartość straty: 31.626640\tDokładność: 50.20%\n",
      "11\tF. straty dla zbioru walidacyjnego: 10084.949219\tNajlepsza wartość straty: 31.626640\tDokładność: 54.53%\n",
      "12\tF. straty dla zbioru walidacyjnego: 4333.422363\tNajlepsza wartość straty: 31.626640\tDokładność: 60.05%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1872.422363\tNajlepsza wartość straty: 31.626640\tDokładność: 73.69%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1441.795166\tNajlepsza wartość straty: 31.626640\tDokładność: 84.36%\n",
      "15\tF. straty dla zbioru walidacyjnego: 547448.937500\tNajlepsza wartość straty: 31.626640\tDokładność: 19.08%\n",
      "16\tF. straty dla zbioru walidacyjnego: 15565.327148\tNajlepsza wartość straty: 31.626640\tDokładność: 53.09%\n",
      "17\tF. straty dla zbioru walidacyjnego: 9036.083984\tNajlepsza wartość straty: 31.626640\tDokładność: 64.82%\n",
      "18\tF. straty dla zbioru walidacyjnego: 5295.022461\tNajlepsza wartość straty: 31.626640\tDokładność: 73.57%\n",
      "19\tF. straty dla zbioru walidacyjnego: 9402.191406\tNajlepsza wartość straty: 31.626640\tDokładność: 68.92%\n",
      "20\tF. straty dla zbioru walidacyjnego: 11275.438477\tNajlepsza wartość straty: 31.626640\tDokładność: 72.17%\n",
      "21\tF. straty dla zbioru walidacyjnego: 3117.811279\tNajlepsza wartość straty: 31.626640\tDokładność: 80.41%\n",
      "22\tF. straty dla zbioru walidacyjnego: 4956.238770\tNajlepsza wartość straty: 31.626640\tDokładność: 77.68%\n",
      "23\tF. straty dla zbioru walidacyjnego: 4824.919434\tNajlepsza wartość straty: 31.626640\tDokładność: 82.06%\n",
      "24\tF. straty dla zbioru walidacyjnego: 3486.780273\tNajlepsza wartość straty: 31.626640\tDokładność: 87.18%\n",
      "25\tF. straty dla zbioru walidacyjnego: 2174.831543\tNajlepsza wartość straty: 31.626640\tDokładność: 88.74%\n",
      "26\tF. straty dla zbioru walidacyjnego: 2863.056396\tNajlepsza wartość straty: 31.626640\tDokładność: 90.23%\n",
      "27\tF. straty dla zbioru walidacyjnego: 58898.527344\tNajlepsza wartość straty: 31.626640\tDokładność: 46.91%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  29.2s\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 19.521868\tNajlepsza wartość straty: 19.521868\tDokładność: 25.45%\n",
      "1\tF. straty dla zbioru walidacyjnego: 803.072327\tNajlepsza wartość straty: 19.521868\tDokładność: 80.88%\n",
      "2\tF. straty dla zbioru walidacyjnego: 400.914551\tNajlepsza wartość straty: 19.521868\tDokładność: 90.27%\n",
      "3\tF. straty dla zbioru walidacyjnego: 208.038162\tNajlepsza wartość straty: 19.521868\tDokładność: 93.86%\n",
      "4\tF. straty dla zbioru walidacyjnego: 238.519058\tNajlepsza wartość straty: 19.521868\tDokładność: 94.14%\n",
      "5\tF. straty dla zbioru walidacyjnego: 163.471420\tNajlepsza wartość straty: 19.521868\tDokładność: 95.15%\n",
      "6\tF. straty dla zbioru walidacyjnego: 177.158844\tNajlepsza wartość straty: 19.521868\tDokładność: 93.08%\n",
      "7\tF. straty dla zbioru walidacyjnego: 239.672668\tNajlepsza wartość straty: 19.521868\tDokładność: 89.68%\n",
      "8\tF. straty dla zbioru walidacyjnego: 79.808556\tNajlepsza wartość straty: 19.521868\tDokładność: 95.82%\n",
      "9\tF. straty dla zbioru walidacyjnego: 4410.033203\tNajlepsza wartość straty: 19.521868\tDokładność: 53.60%\n",
      "10\tF. straty dla zbioru walidacyjnego: 128.682251\tNajlepsza wartość straty: 19.521868\tDokładność: 93.63%\n",
      "11\tF. straty dla zbioru walidacyjnego: 54.512333\tNajlepsza wartość straty: 19.521868\tDokładność: 94.72%\n",
      "12\tF. straty dla zbioru walidacyjnego: 86.131371\tNajlepsza wartość straty: 19.521868\tDokładność: 95.04%\n",
      "13\tF. straty dla zbioru walidacyjnego: 295.327209\tNajlepsza wartość straty: 19.521868\tDokładność: 74.16%\n",
      "14\tF. straty dla zbioru walidacyjnego: 76.091858\tNajlepsza wartość straty: 19.521868\tDokładność: 93.94%\n",
      "15\tF. straty dla zbioru walidacyjnego: 37.805061\tNajlepsza wartość straty: 19.521868\tDokładność: 94.18%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1279052.250000\tNajlepsza wartość straty: 19.521868\tDokładność: 26.00%\n",
      "17\tF. straty dla zbioru walidacyjnego: 38052.441406\tNajlepsza wartość straty: 19.521868\tDokładność: 74.63%\n",
      "18\tF. straty dla zbioru walidacyjnego: 19866.259766\tNajlepsza wartość straty: 19.521868\tDokładność: 74.82%\n",
      "19\tF. straty dla zbioru walidacyjnego: 9415.537109\tNajlepsza wartość straty: 19.521868\tDokładność: 84.21%\n",
      "20\tF. straty dla zbioru walidacyjnego: 7583.271484\tNajlepsza wartość straty: 19.521868\tDokładność: 88.39%\n",
      "21\tF. straty dla zbioru walidacyjnego: 5911.254883\tNajlepsza wartość straty: 19.521868\tDokładność: 90.07%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  21.3s\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1259.743408\tNajlepsza wartość straty: 1259.743408\tDokładność: 20.17%\n",
      "1\tF. straty dla zbioru walidacyjnego: 96.390083\tNajlepsza wartość straty: 96.390083\tDokładność: 57.00%\n",
      "2\tF. straty dla zbioru walidacyjnego: 17.642414\tNajlepsza wartość straty: 17.642414\tDokładność: 74.16%\n",
      "3\tF. straty dla zbioru walidacyjnego: 7.617156\tNajlepsza wartość straty: 7.617156\tDokładność: 80.14%\n",
      "4\tF. straty dla zbioru walidacyjnego: 13011.362305\tNajlepsza wartość straty: 7.617156\tDokładność: 42.57%\n",
      "5\tF. straty dla zbioru walidacyjnego: 3004.953369\tNajlepsza wartość straty: 7.617156\tDokładność: 68.14%\n",
      "6\tF. straty dla zbioru walidacyjnego: 2943.410889\tNajlepsza wartość straty: 7.617156\tDokładność: 65.68%\n",
      "7\tF. straty dla zbioru walidacyjnego: 3154.546387\tNajlepsza wartość straty: 7.617156\tDokładność: 68.80%\n",
      "8\tF. straty dla zbioru walidacyjnego: 3968.066162\tNajlepsza wartość straty: 7.617156\tDokładność: 62.04%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1418.524780\tNajlepsza wartość straty: 7.617156\tDokładność: 72.83%\n",
      "10\tF. straty dla zbioru walidacyjnego: 10261.757812\tNajlepsza wartość straty: 7.617156\tDokładność: 56.37%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1404.711182\tNajlepsza wartość straty: 7.617156\tDokładność: 81.39%\n",
      "12\tF. straty dla zbioru walidacyjnego: 2874.049316\tNajlepsza wartość straty: 7.617156\tDokładność: 73.34%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1350.321899\tNajlepsza wartość straty: 7.617156\tDokładność: 69.98%\n",
      "14\tF. straty dla zbioru walidacyjnego: 532769.875000\tNajlepsza wartość straty: 7.617156\tDokładność: 39.17%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1386.314087\tNajlepsza wartość straty: 7.617156\tDokładność: 74.08%\n",
      "16\tF. straty dla zbioru walidacyjnego: 935.704651\tNajlepsza wartość straty: 7.617156\tDokładność: 81.51%\n",
      "17\tF. straty dla zbioru walidacyjnego: 824.253723\tNajlepsza wartość straty: 7.617156\tDokładność: 75.49%\n",
      "18\tF. straty dla zbioru walidacyjnego: 430.043884\tNajlepsza wartość straty: 7.617156\tDokładność: 86.08%\n",
      "19\tF. straty dla zbioru walidacyjnego: 502.657776\tNajlepsza wartość straty: 7.617156\tDokładność: 83.07%\n",
      "20\tF. straty dla zbioru walidacyjnego: 502.037628\tNajlepsza wartość straty: 7.617156\tDokładność: 78.58%\n",
      "21\tF. straty dla zbioru walidacyjnego: 282.033417\tNajlepsza wartość straty: 7.617156\tDokładność: 87.65%\n",
      "22\tF. straty dla zbioru walidacyjnego: 111667.054688\tNajlepsza wartość straty: 7.617156\tDokładność: 37.92%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1033.415283\tNajlepsza wartość straty: 7.617156\tDokładność: 72.83%\n",
      "24\tF. straty dla zbioru walidacyjnego: 361.013916\tNajlepsza wartość straty: 7.617156\tDokładność: 94.10%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  24.1s\n",
      "[CV] n_neurons=70, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1085.492432\tNajlepsza wartość straty: 1085.492432\tDokładność: 43.04%\n",
      "1\tF. straty dla zbioru walidacyjnego: 61.980904\tNajlepsza wartość straty: 61.980904\tDokładność: 86.16%\n",
      "2\tF. straty dla zbioru walidacyjnego: 50.793068\tNajlepsza wartość straty: 50.793068\tDokładność: 90.23%\n",
      "3\tF. straty dla zbioru walidacyjnego: 33.262905\tNajlepsza wartość straty: 33.262905\tDokładność: 92.96%\n",
      "4\tF. straty dla zbioru walidacyjnego: 31.587818\tNajlepsza wartość straty: 31.587818\tDokładność: 93.75%\n",
      "5\tF. straty dla zbioru walidacyjnego: 23.225235\tNajlepsza wartość straty: 23.225235\tDokładność: 95.04%\n",
      "6\tF. straty dla zbioru walidacyjnego: 16.086853\tNajlepsza wartość straty: 16.086853\tDokładność: 94.29%\n",
      "7\tF. straty dla zbioru walidacyjnego: 11.932909\tNajlepsza wartość straty: 11.932909\tDokładność: 95.35%\n",
      "8\tF. straty dla zbioru walidacyjnego: 14.826177\tNajlepsza wartość straty: 11.932909\tDokładność: 95.97%\n",
      "9\tF. straty dla zbioru walidacyjnego: 15.783033\tNajlepsza wartość straty: 11.932909\tDokładność: 94.68%\n",
      "10\tF. straty dla zbioru walidacyjnego: 9.483222\tNajlepsza wartość straty: 9.483222\tDokładność: 95.97%\n",
      "11\tF. straty dla zbioru walidacyjnego: 9.919328\tNajlepsza wartość straty: 9.483222\tDokładność: 95.27%\n",
      "12\tF. straty dla zbioru walidacyjnego: 13.991247\tNajlepsza wartość straty: 9.483222\tDokładność: 94.57%\n",
      "13\tF. straty dla zbioru walidacyjnego: 11.743505\tNajlepsza wartość straty: 9.483222\tDokładność: 93.63%\n",
      "14\tF. straty dla zbioru walidacyjnego: 6.521685\tNajlepsza wartość straty: 6.521685\tDokładność: 96.72%\n",
      "15\tF. straty dla zbioru walidacyjnego: 6.994628\tNajlepsza wartość straty: 6.521685\tDokładność: 96.29%\n",
      "16\tF. straty dla zbioru walidacyjnego: 6.224855\tNajlepsza wartość straty: 6.224855\tDokładność: 95.90%\n",
      "17\tF. straty dla zbioru walidacyjnego: 6.172121\tNajlepsza wartość straty: 6.172121\tDokładność: 97.50%\n",
      "18\tF. straty dla zbioru walidacyjnego: 6.701324\tNajlepsza wartość straty: 6.172121\tDokładność: 95.43%\n",
      "19\tF. straty dla zbioru walidacyjnego: 5.307819\tNajlepsza wartość straty: 5.307819\tDokładność: 96.68%\n",
      "20\tF. straty dla zbioru walidacyjnego: 140.866089\tNajlepsza wartość straty: 5.307819\tDokładność: 94.18%\n",
      "21\tF. straty dla zbioru walidacyjnego: 18.890720\tNajlepsza wartość straty: 5.307819\tDokładność: 95.43%\n",
      "22\tF. straty dla zbioru walidacyjnego: 11.474499\tNajlepsza wartość straty: 5.307819\tDokładność: 96.36%\n",
      "23\tF. straty dla zbioru walidacyjnego: 7.277597\tNajlepsza wartość straty: 5.307819\tDokładność: 97.50%\n",
      "24\tF. straty dla zbioru walidacyjnego: 11.094499\tNajlepsza wartość straty: 5.307819\tDokładność: 97.15%\n",
      "25\tF. straty dla zbioru walidacyjnego: 6.953103\tNajlepsza wartość straty: 5.307819\tDokładność: 96.40%\n",
      "26\tF. straty dla zbioru walidacyjnego: 5.259970\tNajlepsza wartość straty: 5.259970\tDokładność: 97.22%\n",
      "27\tF. straty dla zbioru walidacyjnego: 5.176294\tNajlepsza wartość straty: 5.176294\tDokładność: 96.91%\n",
      "28\tF. straty dla zbioru walidacyjnego: 3.589055\tNajlepsza wartość straty: 3.589055\tDokładność: 97.19%\n",
      "29\tF. straty dla zbioru walidacyjnego: 4.230726\tNajlepsza wartość straty: 3.589055\tDokładność: 97.07%\n",
      "30\tF. straty dla zbioru walidacyjnego: 5.269845\tNajlepsza wartość straty: 3.589055\tDokładność: 96.76%\n",
      "31\tF. straty dla zbioru walidacyjnego: 4.043087\tNajlepsza wartość straty: 3.589055\tDokładność: 97.34%\n",
      "32\tF. straty dla zbioru walidacyjnego: 4.207315\tNajlepsza wartość straty: 3.589055\tDokładność: 96.87%\n",
      "33\tF. straty dla zbioru walidacyjnego: 5.681274\tNajlepsza wartość straty: 3.589055\tDokładność: 96.60%\n",
      "34\tF. straty dla zbioru walidacyjnego: 12.783940\tNajlepsza wartość straty: 3.589055\tDokładność: 96.48%\n",
      "35\tF. straty dla zbioru walidacyjnego: 53066432.000000\tNajlepsza wartość straty: 3.589055\tDokładność: 25.06%\n",
      "36\tF. straty dla zbioru walidacyjnego: 44094.468750\tNajlepsza wartość straty: 3.589055\tDokładność: 72.40%\n",
      "37\tF. straty dla zbioru walidacyjnego: 13290.783203\tNajlepsza wartość straty: 3.589055\tDokładność: 83.31%\n",
      "38\tF. straty dla zbioru walidacyjnego: 8267.738281\tNajlepsza wartość straty: 3.589055\tDokładność: 87.41%\n",
      "39\tF. straty dla zbioru walidacyjnego: 9458.904297\tNajlepsza wartość straty: 3.589055\tDokładność: 84.52%\n",
      "40\tF. straty dla zbioru walidacyjnego: 6941.626465\tNajlepsza wartość straty: 3.589055\tDokładność: 87.65%\n",
      "41\tF. straty dla zbioru walidacyjnego: 6092.196289\tNajlepsza wartość straty: 3.589055\tDokładność: 87.37%\n",
      "42\tF. straty dla zbioru walidacyjnego: 4298.978027\tNajlepsza wartość straty: 3.589055\tDokładność: 90.30%\n",
      "43\tF. straty dla zbioru walidacyjnego: 2972.131592\tNajlepsza wartość straty: 3.589055\tDokładność: 92.22%\n",
      "44\tF. straty dla zbioru walidacyjnego: 3134.027344\tNajlepsza wartość straty: 3.589055\tDokładność: 90.50%\n",
      "45\tF. straty dla zbioru walidacyjnego: 4365.207031\tNajlepsza wartość straty: 3.589055\tDokładność: 90.50%\n",
      "46\tF. straty dla zbioru walidacyjnego: 4435.889648\tNajlepsza wartość straty: 3.589055\tDokładność: 89.68%\n",
      "47\tF. straty dla zbioru walidacyjnego: 2906.322510\tNajlepsza wartość straty: 3.589055\tDokładność: 91.28%\n",
      "48\tF. straty dla zbioru walidacyjnego: 3817.938965\tNajlepsza wartość straty: 3.589055\tDokładność: 90.77%\n",
      "49\tF. straty dla zbioru walidacyjnego: 3428.085205\tNajlepsza wartość straty: 3.589055\tDokładność: 92.06%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  27.7s\n",
      "[CV] n_neurons=70, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.012399\tNajlepsza wartość straty: 1.012399\tDokładność: 93.04%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.306262\tNajlepsza wartość straty: 0.306262\tDokładność: 94.57%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.305961\tNajlepsza wartość straty: 0.305961\tDokładność: 94.88%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.286834\tNajlepsza wartość straty: 0.286834\tDokładność: 92.89%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.155009\tNajlepsza wartość straty: 0.155009\tDokładność: 96.64%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.201951\tNajlepsza wartość straty: 0.155009\tDokładność: 95.74%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.418912\tNajlepsza wartość straty: 0.155009\tDokładność: 91.87%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.146386\tNajlepsza wartość straty: 0.146386\tDokładność: 96.60%\n",
      "8\tF. straty dla zbioru walidacyjnego: 73956688.000000\tNajlepsza wartość straty: 0.146386\tDokładność: 19.27%\n",
      "9\tF. straty dla zbioru walidacyjnego: 24822.703125\tNajlepsza wartość straty: 0.146386\tDokładność: 88.39%\n",
      "10\tF. straty dla zbioru walidacyjnego: 21864.197266\tNajlepsza wartość straty: 0.146386\tDokładność: 90.15%\n",
      "11\tF. straty dla zbioru walidacyjnego: 14994.872070\tNajlepsza wartość straty: 0.146386\tDokładność: 90.58%\n",
      "12\tF. straty dla zbioru walidacyjnego: 7590.459961\tNajlepsza wartość straty: 0.146386\tDokładność: 93.67%\n",
      "13\tF. straty dla zbioru walidacyjnego: 7534.372070\tNajlepsza wartość straty: 0.146386\tDokładność: 93.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 8165.729492\tNajlepsza wartość straty: 0.146386\tDokładność: 93.51%\n",
      "15\tF. straty dla zbioru walidacyjnego: 6501.590332\tNajlepsza wartość straty: 0.146386\tDokładność: 92.38%\n",
      "16\tF. straty dla zbioru walidacyjnego: 18532.246094\tNajlepsza wartość straty: 0.146386\tDokładność: 85.77%\n",
      "17\tF. straty dla zbioru walidacyjnego: 11691.464844\tNajlepsza wartość straty: 0.146386\tDokładność: 86.24%\n",
      "18\tF. straty dla zbioru walidacyjnego: 8508.536133\tNajlepsza wartość straty: 0.146386\tDokładność: 93.39%\n",
      "19\tF. straty dla zbioru walidacyjnego: 5481.574707\tNajlepsza wartość straty: 0.146386\tDokładność: 92.89%\n",
      "20\tF. straty dla zbioru walidacyjnego: 6337.166504\tNajlepsza wartość straty: 0.146386\tDokładność: 94.37%\n",
      "21\tF. straty dla zbioru walidacyjnego: 7196.700684\tNajlepsza wartość straty: 0.146386\tDokładność: 94.06%\n",
      "22\tF. straty dla zbioru walidacyjnego: 8643.765625\tNajlepsza wartość straty: 0.146386\tDokładność: 94.72%\n",
      "23\tF. straty dla zbioru walidacyjnego: 4724.966309\tNajlepsza wartość straty: 0.146386\tDokładność: 93.59%\n",
      "24\tF. straty dla zbioru walidacyjnego: 5512.817383\tNajlepsza wartość straty: 0.146386\tDokładność: 94.76%\n",
      "25\tF. straty dla zbioru walidacyjnego: 529764.812500\tNajlepsza wartość straty: 0.146386\tDokładność: 39.25%\n",
      "26\tF. straty dla zbioru walidacyjnego: 13950.208008\tNajlepsza wartość straty: 0.146386\tDokładność: 94.41%\n",
      "27\tF. straty dla zbioru walidacyjnego: 11607.034180\tNajlepsza wartość straty: 0.146386\tDokładność: 94.57%\n",
      "28\tF. straty dla zbioru walidacyjnego: 13522.842773\tNajlepsza wartość straty: 0.146386\tDokładność: 95.15%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  16.6s\n",
      "[CV] n_neurons=70, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.309093\tNajlepsza wartość straty: 1.309093\tDokładność: 95.23%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.397236\tNajlepsza wartość straty: 0.397236\tDokładność: 94.61%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.239165\tNajlepsza wartość straty: 0.239165\tDokładność: 96.09%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.285138\tNajlepsza wartość straty: 0.239165\tDokładność: 94.76%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.176472\tNajlepsza wartość straty: 0.176472\tDokładność: 96.01%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.324212\tNajlepsza wartość straty: 0.176472\tDokładność: 95.58%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.184572\tNajlepsza wartość straty: 0.176472\tDokładność: 96.68%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.285719\tNajlepsza wartość straty: 0.176472\tDokładność: 95.58%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.241373\tNajlepsza wartość straty: 0.176472\tDokładność: 95.50%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.205716\tNajlepsza wartość straty: 0.176472\tDokładność: 97.07%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.172299\tNajlepsza wartość straty: 0.172299\tDokładność: 97.54%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.194180\tNajlepsza wartość straty: 0.172299\tDokładność: 97.22%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.242742\tNajlepsza wartość straty: 0.172299\tDokładność: 95.43%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.461739\tNajlepsza wartość straty: 0.172299\tDokładność: 97.30%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.228085\tNajlepsza wartość straty: 0.172299\tDokładność: 96.72%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.163901\tNajlepsza wartość straty: 0.163901\tDokładność: 97.42%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.182091\tNajlepsza wartość straty: 0.163901\tDokładność: 97.65%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.221363\tNajlepsza wartość straty: 0.163901\tDokładność: 97.58%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.339616\tNajlepsza wartość straty: 0.163901\tDokładność: 96.83%\n",
      "19\tF. straty dla zbioru walidacyjnego: 148964.765625\tNajlepsza wartość straty: 0.163901\tDokładność: 54.03%\n",
      "20\tF. straty dla zbioru walidacyjnego: 37037.527344\tNajlepsza wartość straty: 0.163901\tDokładność: 78.30%\n",
      "21\tF. straty dla zbioru walidacyjnego: 16056.153320\tNajlepsza wartość straty: 0.163901\tDokładność: 85.34%\n",
      "22\tF. straty dla zbioru walidacyjnego: 8606.670898\tNajlepsza wartość straty: 0.163901\tDokładność: 89.09%\n",
      "23\tF. straty dla zbioru walidacyjnego: 9283.068359\tNajlepsza wartość straty: 0.163901\tDokładność: 88.66%\n",
      "24\tF. straty dla zbioru walidacyjnego: 11081.125977\tNajlepsza wartość straty: 0.163901\tDokładność: 84.48%\n",
      "25\tF. straty dla zbioru walidacyjnego: 5893.253906\tNajlepsza wartość straty: 0.163901\tDokładność: 92.69%\n",
      "26\tF. straty dla zbioru walidacyjnego: 9417.307617\tNajlepsza wartość straty: 0.163901\tDokładność: 90.62%\n",
      "27\tF. straty dla zbioru walidacyjnego: 4713.962402\tNajlepsza wartość straty: 0.163901\tDokładność: 93.94%\n",
      "28\tF. straty dla zbioru walidacyjnego: 4085.176758\tNajlepsza wartość straty: 0.163901\tDokładność: 93.67%\n",
      "29\tF. straty dla zbioru walidacyjnego: 3686.613770\tNajlepsza wartość straty: 0.163901\tDokładność: 94.64%\n",
      "30\tF. straty dla zbioru walidacyjnego: 5637.406738\tNajlepsza wartość straty: 0.163901\tDokładność: 90.85%\n",
      "31\tF. straty dla zbioru walidacyjnego: 4478.530762\tNajlepsza wartość straty: 0.163901\tDokładność: 93.32%\n",
      "32\tF. straty dla zbioru walidacyjnego: 3898.302979\tNajlepsza wartość straty: 0.163901\tDokładność: 93.86%\n",
      "33\tF. straty dla zbioru walidacyjnego: 3197.164062\tNajlepsza wartość straty: 0.163901\tDokładność: 95.15%\n",
      "34\tF. straty dla zbioru walidacyjnego: 2957.746094\tNajlepsza wartość straty: 0.163901\tDokładność: 95.31%\n",
      "35\tF. straty dla zbioru walidacyjnego: 14895.190430\tNajlepsza wartość straty: 0.163901\tDokładność: 82.60%\n",
      "36\tF. straty dla zbioru walidacyjnego: 4759.586426\tNajlepsza wartość straty: 0.163901\tDokładność: 93.24%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  20.6s\n",
      "[CV] n_neurons=100, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 2.085928\tNajlepsza wartość straty: 2.085928\tDokładność: 18.73%\n",
      "1\tF. straty dla zbioru walidacyjnego: 2.475007\tNajlepsza wartość straty: 2.085928\tDokładność: 18.73%\n",
      "2\tF. straty dla zbioru walidacyjnego: 2.228853\tNajlepsza wartość straty: 2.085928\tDokładność: 19.27%\n",
      "3\tF. straty dla zbioru walidacyjnego: 2.277519\tNajlepsza wartość straty: 2.085928\tDokładność: 19.27%\n",
      "4\tF. straty dla zbioru walidacyjnego: 2.068843\tNajlepsza wartość straty: 2.068843\tDokładność: 19.27%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.693352\tNajlepsza wartość straty: 1.693352\tDokładność: 20.91%\n",
      "6\tF. straty dla zbioru walidacyjnego: 2.097597\tNajlepsza wartość straty: 1.693352\tDokładność: 18.73%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.991443\tNajlepsza wartość straty: 1.693352\tDokładność: 19.08%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.886129\tNajlepsza wartość straty: 1.693352\tDokładność: 19.08%\n",
      "9\tF. straty dla zbioru walidacyjnego: 2.482885\tNajlepsza wartość straty: 1.693352\tDokładność: 19.27%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.726878\tNajlepsza wartość straty: 1.693352\tDokładność: 20.91%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.677547\tNajlepsza wartość straty: 1.677547\tDokładność: 22.01%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.999842\tNajlepsza wartość straty: 1.677547\tDokładność: 19.08%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.756570\tNajlepsza wartość straty: 1.677547\tDokładność: 19.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 2.995755\tNajlepsza wartość straty: 1.677547\tDokładność: 20.91%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.696044\tNajlepsza wartość straty: 1.677547\tDokładność: 19.27%\n",
      "16\tF. straty dla zbioru walidacyjnego: 2.644185\tNajlepsza wartość straty: 1.677547\tDokładność: 20.91%\n",
      "17\tF. straty dla zbioru walidacyjnego: 2.019509\tNajlepsza wartość straty: 1.677547\tDokładność: 20.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 2.568111\tNajlepsza wartość straty: 1.677547\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 2.833773\tNajlepsza wartość straty: 1.677547\tDokładność: 22.01%\n",
      "20\tF. straty dla zbioru walidacyjnego: 2.283543\tNajlepsza wartość straty: 1.677547\tDokładność: 22.01%\n",
      "21\tF. straty dla zbioru walidacyjnego: 2.377189\tNajlepsza wartość straty: 1.677547\tDokładność: 20.91%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.983360\tNajlepsza wartość straty: 1.677547\tDokładność: 18.73%\n",
      "23\tF. straty dla zbioru walidacyjnego: 2.143960\tNajlepsza wartość straty: 1.677547\tDokładność: 19.27%\n",
      "24\tF. straty dla zbioru walidacyjnego: 2.032373\tNajlepsza wartość straty: 1.677547\tDokładność: 20.91%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.827990\tNajlepsza wartość straty: 1.677547\tDokładność: 20.91%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.893213\tNajlepsza wartość straty: 1.677547\tDokładność: 19.08%\n",
      "27\tF. straty dla zbioru walidacyjnego: 2.163700\tNajlepsza wartość straty: 1.677547\tDokładność: 22.01%\n",
      "28\tF. straty dla zbioru walidacyjnego: 2.621579\tNajlepsza wartość straty: 1.677547\tDokładność: 19.08%\n",
      "29\tF. straty dla zbioru walidacyjnego: 2.281025\tNajlepsza wartość straty: 1.677547\tDokładność: 20.91%\n",
      "30\tF. straty dla zbioru walidacyjnego: 1.913913\tNajlepsza wartość straty: 1.677547\tDokładność: 19.27%\n",
      "31\tF. straty dla zbioru walidacyjnego: 2.141626\tNajlepsza wartość straty: 1.677547\tDokładność: 19.27%\n",
      "32\tF. straty dla zbioru walidacyjnego: 2.017714\tNajlepsza wartość straty: 1.677547\tDokładność: 19.27%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total= 1.3min\n",
      "[CV] n_neurons=100, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 2.034915\tNajlepsza wartość straty: 2.034915\tDokładność: 19.27%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.977479\tNajlepsza wartość straty: 1.977479\tDokładność: 22.01%\n",
      "2\tF. straty dla zbioru walidacyjnego: 2.200789\tNajlepsza wartość straty: 1.977479\tDokładność: 22.01%\n",
      "3\tF. straty dla zbioru walidacyjnego: 2.182319\tNajlepsza wartość straty: 1.977479\tDokładność: 22.01%\n",
      "4\tF. straty dla zbioru walidacyjnego: 2.003921\tNajlepsza wartość straty: 1.977479\tDokładność: 20.91%\n",
      "5\tF. straty dla zbioru walidacyjnego: 2.129631\tNajlepsza wartość straty: 1.977479\tDokładność: 19.27%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.798201\tNajlepsza wartość straty: 1.798201\tDokładność: 19.27%\n",
      "7\tF. straty dla zbioru walidacyjnego: 2.028867\tNajlepsza wartość straty: 1.798201\tDokładność: 19.08%\n",
      "8\tF. straty dla zbioru walidacyjnego: 2.354980\tNajlepsza wartość straty: 1.798201\tDokładność: 18.73%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.772442\tNajlepsza wartość straty: 1.772442\tDokładność: 19.27%\n",
      "10\tF. straty dla zbioru walidacyjnego: 2.146058\tNajlepsza wartość straty: 1.772442\tDokładność: 19.08%\n",
      "11\tF. straty dla zbioru walidacyjnego: 2.738313\tNajlepsza wartość straty: 1.772442\tDokładność: 22.01%\n",
      "12\tF. straty dla zbioru walidacyjnego: 2.141825\tNajlepsza wartość straty: 1.772442\tDokładność: 19.27%\n",
      "13\tF. straty dla zbioru walidacyjnego: 2.503583\tNajlepsza wartość straty: 1.772442\tDokładność: 19.27%\n",
      "14\tF. straty dla zbioru walidacyjnego: 2.296503\tNajlepsza wartość straty: 1.772442\tDokładność: 19.27%\n",
      "15\tF. straty dla zbioru walidacyjnego: 2.013768\tNajlepsza wartość straty: 1.772442\tDokładność: 18.73%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.657808\tNajlepsza wartość straty: 1.657808\tDokładność: 19.08%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.940417\tNajlepsza wartość straty: 1.657808\tDokładność: 18.73%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.874329\tNajlepsza wartość straty: 1.657808\tDokładność: 19.08%\n",
      "19\tF. straty dla zbioru walidacyjnego: 2.003678\tNajlepsza wartość straty: 1.657808\tDokładność: 18.73%\n",
      "20\tF. straty dla zbioru walidacyjnego: 2.406503\tNajlepsza wartość straty: 1.657808\tDokładność: 19.27%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.857486\tNajlepsza wartość straty: 1.657808\tDokładność: 19.27%\n",
      "22\tF. straty dla zbioru walidacyjnego: 2.096707\tNajlepsza wartość straty: 1.657808\tDokładność: 19.27%\n",
      "23\tF. straty dla zbioru walidacyjnego: 2.521005\tNajlepsza wartość straty: 1.657808\tDokładność: 22.01%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.893625\tNajlepsza wartość straty: 1.657808\tDokładność: 19.08%\n",
      "25\tF. straty dla zbioru walidacyjnego: 2.078130\tNajlepsza wartość straty: 1.657808\tDokładność: 22.01%\n",
      "26\tF. straty dla zbioru walidacyjnego: 2.804219\tNajlepsza wartość straty: 1.657808\tDokładność: 19.08%\n",
      "27\tF. straty dla zbioru walidacyjnego: 2.394254\tNajlepsza wartość straty: 1.657808\tDokładność: 20.91%\n",
      "28\tF. straty dla zbioru walidacyjnego: 1.977028\tNajlepsza wartość straty: 1.657808\tDokładność: 20.91%\n",
      "29\tF. straty dla zbioru walidacyjnego: 1.721671\tNajlepsza wartość straty: 1.657808\tDokładność: 19.27%\n",
      "30\tF. straty dla zbioru walidacyjnego: 2.203552\tNajlepsza wartość straty: 1.657808\tDokładność: 19.08%\n",
      "31\tF. straty dla zbioru walidacyjnego: 2.605233\tNajlepsza wartość straty: 1.657808\tDokładność: 22.01%\n",
      "32\tF. straty dla zbioru walidacyjnego: 2.066096\tNajlepsza wartość straty: 1.657808\tDokładność: 18.73%\n",
      "33\tF. straty dla zbioru walidacyjnego: 2.527814\tNajlepsza wartość straty: 1.657808\tDokładność: 19.08%\n",
      "34\tF. straty dla zbioru walidacyjnego: 2.730610\tNajlepsza wartość straty: 1.657808\tDokładność: 18.73%\n",
      "35\tF. straty dla zbioru walidacyjnego: 2.582778\tNajlepsza wartość straty: 1.657808\tDokładność: 22.01%\n",
      "36\tF. straty dla zbioru walidacyjnego: 2.845678\tNajlepsza wartość straty: 1.657808\tDokładność: 19.27%\n",
      "37\tF. straty dla zbioru walidacyjnego: 1.998371\tNajlepsza wartość straty: 1.657808\tDokładność: 19.27%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total= 1.5min\n",
      "[CV] n_neurons=100, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.797444\tNajlepsza wartość straty: 1.797444\tDokładność: 22.01%\n",
      "1\tF. straty dla zbioru walidacyjnego: 2.314173\tNajlepsza wartość straty: 1.797444\tDokładność: 22.01%\n",
      "2\tF. straty dla zbioru walidacyjnego: 2.165308\tNajlepsza wartość straty: 1.797444\tDokładność: 19.08%\n",
      "3\tF. straty dla zbioru walidacyjnego: 2.141412\tNajlepsza wartość straty: 1.797444\tDokładność: 19.27%\n",
      "4\tF. straty dla zbioru walidacyjnego: 3.098778\tNajlepsza wartość straty: 1.797444\tDokładność: 20.91%\n",
      "5\tF. straty dla zbioru walidacyjnego: 2.463478\tNajlepsza wartość straty: 1.797444\tDokładność: 20.91%\n",
      "6\tF. straty dla zbioru walidacyjnego: 2.003131\tNajlepsza wartość straty: 1.797444\tDokładność: 22.01%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.763764\tNajlepsza wartość straty: 1.763764\tDokładność: 22.01%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.700592\tNajlepsza wartość straty: 1.700592\tDokładność: 19.27%\n",
      "9\tF. straty dla zbioru walidacyjnego: 2.334473\tNajlepsza wartość straty: 1.700592\tDokładność: 22.01%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.853945\tNajlepsza wartość straty: 1.700592\tDokładność: 18.73%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.842176\tNajlepsza wartość straty: 1.700592\tDokładność: 19.27%\n",
      "12\tF. straty dla zbioru walidacyjnego: 2.091004\tNajlepsza wartość straty: 1.700592\tDokładność: 20.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 3.280352\tNajlepsza wartość straty: 1.700592\tDokładność: 19.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 2.147494\tNajlepsza wartość straty: 1.700592\tDokładność: 19.08%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.816312\tNajlepsza wartość straty: 1.700592\tDokładność: 19.27%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.802170\tNajlepsza wartość straty: 1.700592\tDokładność: 18.73%\n",
      "17\tF. straty dla zbioru walidacyjnego: 2.435369\tNajlepsza wartość straty: 1.700592\tDokładność: 20.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.970473\tNajlepsza wartość straty: 1.700592\tDokładność: 19.27%\n",
      "19\tF. straty dla zbioru walidacyjnego: 2.368722\tNajlepsza wartość straty: 1.700592\tDokładność: 22.01%\n",
      "20\tF. straty dla zbioru walidacyjnego: 2.211048\tNajlepsza wartość straty: 1.700592\tDokładność: 19.27%\n",
      "21\tF. straty dla zbioru walidacyjnego: 2.549920\tNajlepsza wartość straty: 1.700592\tDokładność: 22.01%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.927104\tNajlepsza wartość straty: 1.700592\tDokładność: 19.27%\n",
      "23\tF. straty dla zbioru walidacyjnego: 2.677983\tNajlepsza wartość straty: 1.700592\tDokładność: 19.08%\n",
      "24\tF. straty dla zbioru walidacyjnego: 2.498909\tNajlepsza wartość straty: 1.700592\tDokładność: 19.27%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.704165\tNajlepsza wartość straty: 1.700592\tDokładność: 22.01%\n",
      "26\tF. straty dla zbioru walidacyjnego: 2.280958\tNajlepsza wartość straty: 1.700592\tDokładność: 18.73%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.996865\tNajlepsza wartość straty: 1.700592\tDokładność: 18.73%\n",
      "28\tF. straty dla zbioru walidacyjnego: 2.451456\tNajlepsza wartość straty: 1.700592\tDokładność: 19.08%\n",
      "29\tF. straty dla zbioru walidacyjnego: 2.784373\tNajlepsza wartość straty: 1.700592\tDokładność: 20.91%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total= 1.2min\n",
      "[CV] n_neurons=90, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.146550\tNajlepsza wartość straty: 0.146550\tDokładność: 94.84%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.094868\tNajlepsza wartość straty: 0.094868\tDokładność: 96.87%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.095423\tNajlepsza wartość straty: 0.094868\tDokładność: 96.56%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.064081\tNajlepsza wartość straty: 0.064081\tDokładność: 98.05%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.064739\tNajlepsza wartość straty: 0.064081\tDokładność: 97.89%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.066326\tNajlepsza wartość straty: 0.064081\tDokładność: 97.89%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.063929\tNajlepsza wartość straty: 0.063929\tDokładność: 98.12%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.064304\tNajlepsza wartość straty: 0.063929\tDokładność: 98.08%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.055562\tNajlepsza wartość straty: 0.055562\tDokładność: 98.36%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.058052\tNajlepsza wartość straty: 0.055562\tDokładność: 98.20%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.069510\tNajlepsza wartość straty: 0.055562\tDokładność: 98.36%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.067441\tNajlepsza wartość straty: 0.055562\tDokładność: 98.24%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.078383\tNajlepsza wartość straty: 0.055562\tDokładność: 98.12%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.067708\tNajlepsza wartość straty: 0.055562\tDokładność: 98.16%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.070639\tNajlepsza wartość straty: 0.055562\tDokładność: 98.32%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.079439\tNajlepsza wartość straty: 0.055562\tDokładność: 98.36%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.060729\tNajlepsza wartość straty: 0.055562\tDokładność: 98.83%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.069207\tNajlepsza wartość straty: 0.055562\tDokładność: 98.36%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.073603\tNajlepsza wartość straty: 0.055562\tDokładność: 98.87%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.058262\tNajlepsza wartość straty: 0.055562\tDokładność: 98.98%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.077970\tNajlepsza wartość straty: 0.055562\tDokładność: 98.55%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.095351\tNajlepsza wartość straty: 0.055562\tDokładność: 98.01%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.058100\tNajlepsza wartość straty: 0.055562\tDokładność: 98.63%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.076161\tNajlepsza wartość straty: 0.055562\tDokładność: 98.55%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.053718\tNajlepsza wartość straty: 0.053718\tDokładność: 98.75%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.076575\tNajlepsza wartość straty: 0.053718\tDokładność: 98.55%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.080678\tNajlepsza wartość straty: 0.053718\tDokładność: 98.71%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.062737\tNajlepsza wartość straty: 0.053718\tDokładność: 98.71%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.078669\tNajlepsza wartość straty: 0.053718\tDokładność: 98.63%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.084042\tNajlepsza wartość straty: 0.053718\tDokładność: 98.83%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.074163\tNajlepsza wartość straty: 0.053718\tDokładność: 98.71%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.073977\tNajlepsza wartość straty: 0.053718\tDokładność: 98.87%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.071130\tNajlepsza wartość straty: 0.053718\tDokładność: 99.10%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.074389\tNajlepsza wartość straty: 0.053718\tDokładność: 98.87%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.071092\tNajlepsza wartość straty: 0.053718\tDokładność: 99.06%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.073508\tNajlepsza wartość straty: 0.053718\tDokładność: 98.98%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.073857\tNajlepsza wartość straty: 0.053718\tDokładność: 98.98%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.074223\tNajlepsza wartość straty: 0.053718\tDokładność: 98.98%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.074710\tNajlepsza wartość straty: 0.053718\tDokładność: 99.02%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.075105\tNajlepsza wartość straty: 0.053718\tDokładność: 99.02%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.075449\tNajlepsza wartość straty: 0.053718\tDokładność: 99.02%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.075796\tNajlepsza wartość straty: 0.053718\tDokładność: 99.02%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.076196\tNajlepsza wartość straty: 0.053718\tDokładność: 99.02%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.076560\tNajlepsza wartość straty: 0.053718\tDokładność: 99.02%\n",
      "44\tF. straty dla zbioru walidacyjnego: 0.076881\tNajlepsza wartość straty: 0.053718\tDokładność: 99.02%\n",
      "45\tF. straty dla zbioru walidacyjnego: 0.077185\tNajlepsza wartość straty: 0.053718\tDokładność: 99.02%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  20.2s\n",
      "[CV] n_neurons=90, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.158880\tNajlepsza wartość straty: 0.158880\tDokładność: 95.23%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.096746\tNajlepsza wartość straty: 0.096746\tDokładność: 96.79%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.076148\tNajlepsza wartość straty: 0.076148\tDokładność: 97.85%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.072823\tNajlepsza wartość straty: 0.072823\tDokładność: 97.77%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.067329\tNajlepsza wartość straty: 0.067329\tDokładność: 97.97%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.059387\tNajlepsza wartość straty: 0.059387\tDokładność: 98.28%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.059225\tNajlepsza wartość straty: 0.059225\tDokładność: 98.36%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.053194\tNajlepsza wartość straty: 0.053194\tDokładność: 98.48%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.058501\tNajlepsza wartość straty: 0.053194\tDokładność: 98.40%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.048032\tNajlepsza wartość straty: 0.048032\tDokładność: 98.63%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.048539\tNajlepsza wartość straty: 0.048032\tDokładność: 98.91%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.072550\tNajlepsza wartość straty: 0.048032\tDokładność: 98.16%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.055154\tNajlepsza wartość straty: 0.048032\tDokładność: 98.51%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.069898\tNajlepsza wartość straty: 0.048032\tDokładność: 98.36%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.055621\tNajlepsza wartość straty: 0.048032\tDokładność: 98.55%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.053794\tNajlepsza wartość straty: 0.048032\tDokładność: 98.87%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.052973\tNajlepsza wartość straty: 0.048032\tDokładność: 98.67%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.065249\tNajlepsza wartość straty: 0.048032\tDokładność: 98.75%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.075137\tNajlepsza wartość straty: 0.048032\tDokładność: 98.59%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.070998\tNajlepsza wartość straty: 0.048032\tDokładność: 98.67%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.076704\tNajlepsza wartość straty: 0.048032\tDokładność: 98.36%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.107624\tNajlepsza wartość straty: 0.048032\tDokładność: 97.93%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.045062\tNajlepsza wartość straty: 0.045062\tDokładność: 98.94%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.058681\tNajlepsza wartość straty: 0.045062\tDokładność: 98.75%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.049419\tNajlepsza wartość straty: 0.045062\tDokładność: 98.87%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.065291\tNajlepsza wartość straty: 0.045062\tDokładność: 98.67%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.056437\tNajlepsza wartość straty: 0.045062\tDokładność: 98.59%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.057872\tNajlepsza wartość straty: 0.045062\tDokładność: 98.87%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.054909\tNajlepsza wartość straty: 0.045062\tDokładność: 98.91%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.050040\tNajlepsza wartość straty: 0.045062\tDokładność: 98.94%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.056478\tNajlepsza wartość straty: 0.045062\tDokładność: 98.94%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.054612\tNajlepsza wartość straty: 0.045062\tDokładność: 98.94%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.054965\tNajlepsza wartość straty: 0.045062\tDokładność: 98.98%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.055858\tNajlepsza wartość straty: 0.045062\tDokładność: 98.98%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.056244\tNajlepsza wartość straty: 0.045062\tDokładność: 98.98%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.056805\tNajlepsza wartość straty: 0.045062\tDokładność: 99.02%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.057319\tNajlepsza wartość straty: 0.045062\tDokładność: 99.02%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.057694\tNajlepsza wartość straty: 0.045062\tDokładność: 99.02%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.058222\tNajlepsza wartość straty: 0.045062\tDokładność: 99.02%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.058722\tNajlepsza wartość straty: 0.045062\tDokładność: 98.98%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.059062\tNajlepsza wartość straty: 0.045062\tDokładność: 98.98%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.059467\tNajlepsza wartość straty: 0.045062\tDokładność: 98.98%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.059826\tNajlepsza wartość straty: 0.045062\tDokładność: 98.98%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.060111\tNajlepsza wartość straty: 0.045062\tDokładność: 98.98%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  19.1s\n",
      "[CV] n_neurons=90, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.154424\tNajlepsza wartość straty: 0.154424\tDokładność: 94.88%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.099753\tNajlepsza wartość straty: 0.099753\tDokładność: 96.91%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.092433\tNajlepsza wartość straty: 0.092433\tDokładność: 97.42%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.067391\tNajlepsza wartość straty: 0.067391\tDokładność: 97.97%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.058411\tNajlepsza wartość straty: 0.058411\tDokładność: 98.16%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.059778\tNajlepsza wartość straty: 0.058411\tDokładność: 98.28%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.055561\tNajlepsza wartość straty: 0.055561\tDokładność: 98.44%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.047203\tNajlepsza wartość straty: 0.047203\tDokładność: 98.44%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.060032\tNajlepsza wartość straty: 0.047203\tDokładność: 98.08%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.046201\tNajlepsza wartość straty: 0.046201\tDokładność: 98.67%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.055269\tNajlepsza wartość straty: 0.046201\tDokładność: 98.51%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.051800\tNajlepsza wartość straty: 0.046201\tDokładność: 98.91%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.054688\tNajlepsza wartość straty: 0.046201\tDokładność: 98.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.065163\tNajlepsza wartość straty: 0.046201\tDokładność: 98.32%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.066003\tNajlepsza wartość straty: 0.046201\tDokładność: 98.83%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.056981\tNajlepsza wartość straty: 0.046201\tDokładność: 98.67%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.054378\tNajlepsza wartość straty: 0.046201\tDokładność: 98.63%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.075630\tNajlepsza wartość straty: 0.046201\tDokładność: 98.28%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.061590\tNajlepsza wartość straty: 0.046201\tDokładność: 98.87%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.081824\tNajlepsza wartość straty: 0.046201\tDokładność: 98.28%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.048667\tNajlepsza wartość straty: 0.046201\tDokładność: 98.98%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.060310\tNajlepsza wartość straty: 0.046201\tDokładność: 98.67%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.067896\tNajlepsza wartość straty: 0.046201\tDokładność: 98.87%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.075802\tNajlepsza wartość straty: 0.046201\tDokładność: 98.75%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.060031\tNajlepsza wartość straty: 0.046201\tDokładność: 98.83%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.046694\tNajlepsza wartość straty: 0.046201\tDokładność: 98.59%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.084330\tNajlepsza wartość straty: 0.046201\tDokładność: 98.44%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.061939\tNajlepsza wartość straty: 0.046201\tDokładność: 98.87%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.094049\tNajlepsza wartość straty: 0.046201\tDokładność: 98.40%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.056648\tNajlepsza wartość straty: 0.046201\tDokładność: 98.94%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.056276\tNajlepsza wartość straty: 0.046201\tDokładność: 98.94%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  14.1s\n",
      "[CV] n_neurons=30, learning_rate=0.05, batch_size=50, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.215732\tNajlepsza wartość straty: 0.215732\tDokładność: 93.04%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.139400\tNajlepsza wartość straty: 0.139400\tDokładność: 96.33%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.433384\tNajlepsza wartość straty: 0.139400\tDokładność: 86.32%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.269325\tNajlepsza wartość straty: 0.139400\tDokładność: 92.53%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.411804\tNajlepsza wartość straty: 0.139400\tDokładność: 88.35%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.517529\tNajlepsza wartość straty: 0.139400\tDokładność: 80.61%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.488262\tNajlepsza wartość straty: 0.139400\tDokładność: 83.03%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.376416\tNajlepsza wartość straty: 0.139400\tDokładność: 89.72%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.282048\tNajlepsza wartość straty: 0.139400\tDokładność: 92.89%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.639582\tNajlepsza wartość straty: 0.139400\tDokładność: 71.11%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.582708\tNajlepsza wartość straty: 0.139400\tDokładność: 80.38%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.507388\tNajlepsza wartość straty: 0.139400\tDokładność: 84.71%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.459569\tNajlepsza wartość straty: 0.139400\tDokładność: 87.69%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.744758\tNajlepsza wartość straty: 0.139400\tDokładność: 79.48%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.473121\tNajlepsza wartość straty: 0.139400\tDokładność: 90.38%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.616843\tNajlepsza wartość straty: 0.139400\tDokładność: 22.05%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.612319\tNajlepsza wartość straty: 0.139400\tDokładność: 22.05%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.628921\tNajlepsza wartość straty: 0.139400\tDokładność: 19.27%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.615917\tNajlepsza wartość straty: 0.139400\tDokładność: 22.05%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.608052\tNajlepsza wartość straty: 0.139400\tDokładność: 22.05%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.618206\tNajlepsza wartość straty: 0.139400\tDokładność: 19.27%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.620479\tNajlepsza wartość straty: 0.139400\tDokładność: 19.27%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.618954\tNajlepsza wartość straty: 0.139400\tDokładność: 19.27%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.05, batch_size=50, activation=<function relu at 0x0000029B12839B70>, total=  10.5s\n",
      "[CV] n_neurons=30, learning_rate=0.05, batch_size=50, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.199152\tNajlepsza wartość straty: 0.199152\tDokładność: 95.31%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.156341\tNajlepsza wartość straty: 0.156341\tDokładność: 95.90%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.433819\tNajlepsza wartość straty: 0.156341\tDokładność: 82.68%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.263666\tNajlepsza wartość straty: 0.156341\tDokładność: 94.25%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.461069\tNajlepsza wartość straty: 0.156341\tDokładność: 91.87%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.255998\tNajlepsza wartość straty: 0.156341\tDokładność: 93.12%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.261599\tNajlepsza wartość straty: 0.156341\tDokładność: 93.90%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.030958\tNajlepsza wartość straty: 0.156341\tDokładność: 49.06%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.700126\tNajlepsza wartość straty: 0.156341\tDokładność: 75.57%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.936526\tNajlepsza wartość straty: 0.156341\tDokładność: 57.90%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.835900\tNajlepsza wartość straty: 0.156341\tDokładność: 67.75%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.823775\tNajlepsza wartość straty: 0.156341\tDokładność: 69.74%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.623954\tNajlepsza wartość straty: 0.156341\tDokładność: 76.74%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.704604\tNajlepsza wartość straty: 0.156341\tDokładność: 73.34%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.610537\tNajlepsza wartość straty: 0.156341\tDokładność: 73.53%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.601821\tNajlepsza wartość straty: 0.156341\tDokładność: 73.38%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.977322\tNajlepsza wartość straty: 0.156341\tDokładność: 56.96%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.015209\tNajlepsza wartość straty: 0.156341\tDokładność: 50.90%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.034930\tNajlepsza wartość straty: 0.156341\tDokładność: 57.39%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.013015\tNajlepsza wartość straty: 0.156341\tDokładność: 53.48%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.919808\tNajlepsza wartość straty: 0.156341\tDokładność: 56.80%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.870505\tNajlepsza wartość straty: 0.156341\tDokładność: 58.33%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.865286\tNajlepsza wartość straty: 0.156341\tDokładność: 58.44%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.05, batch_size=50, activation=<function relu at 0x0000029B12839B70>, total=  10.5s\n",
      "[CV] n_neurons=30, learning_rate=0.05, batch_size=50, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.179547\tNajlepsza wartość straty: 0.179547\tDokładność: 95.00%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.176000\tNajlepsza wartość straty: 0.176000\tDokładność: 95.19%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.239000\tNajlepsza wartość straty: 0.176000\tDokładność: 94.96%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.261125\tNajlepsza wartość straty: 0.176000\tDokładność: 36.71%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.155352\tNajlepsza wartość straty: 0.176000\tDokładność: 41.20%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.875618\tNajlepsza wartość straty: 0.176000\tDokładność: 63.41%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.609346\tNajlepsza wartość straty: 0.176000\tDokładność: 74.43%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.597045\tNajlepsza wartość straty: 0.176000\tDokładność: 72.56%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.603174\tNajlepsza wartość straty: 0.176000\tDokładność: 75.41%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.561451\tNajlepsza wartość straty: 0.176000\tDokładność: 76.70%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.460728\tNajlepsza wartość straty: 0.176000\tDokładność: 28.07%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.270340\tNajlepsza wartość straty: 0.176000\tDokładność: 40.50%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.229373\tNajlepsza wartość straty: 0.176000\tDokładność: 42.46%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.228712\tNajlepsza wartość straty: 0.176000\tDokładność: 36.98%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.236048\tNajlepsza wartość straty: 0.176000\tDokładność: 39.80%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.232804\tNajlepsza wartość straty: 0.176000\tDokładność: 42.46%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.204934\tNajlepsza wartość straty: 0.176000\tDokładność: 39.52%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.168800\tNajlepsza wartość straty: 0.176000\tDokładność: 42.30%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.195128\tNajlepsza wartość straty: 0.176000\tDokładność: 42.49%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.224568\tNajlepsza wartość straty: 0.176000\tDokładność: 42.61%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.222249\tNajlepsza wartość straty: 0.176000\tDokładność: 40.54%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.228347\tNajlepsza wartość straty: 0.176000\tDokładność: 40.54%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.220437\tNajlepsza wartość straty: 0.176000\tDokładność: 42.61%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.05, batch_size=50, activation=<function relu at 0x0000029B12839B70>, total=  10.7s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.110374\tNajlepsza wartość straty: 0.110374\tDokładność: 96.72%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.139323\tNajlepsza wartość straty: 0.110374\tDokładność: 96.21%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.477472\tNajlepsza wartość straty: 0.110374\tDokładność: 93.55%\n",
      "3\tF. straty dla zbioru walidacyjnego: 11.665615\tNajlepsza wartość straty: 0.110374\tDokładność: 60.28%\n",
      "4\tF. straty dla zbioru walidacyjnego: 3.249251\tNajlepsza wartość straty: 0.110374\tDokładność: 84.99%\n",
      "5\tF. straty dla zbioru walidacyjnego: 4.642514\tNajlepsza wartość straty: 0.110374\tDokładność: 80.30%\n",
      "6\tF. straty dla zbioru walidacyjnego: 4.519654\tNajlepsza wartość straty: 0.110374\tDokładność: 74.59%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.942611\tNajlepsza wartość straty: 0.110374\tDokładność: 89.99%\n",
      "8\tF. straty dla zbioru walidacyjnego: 3.112829\tNajlepsza wartość straty: 0.110374\tDokładność: 83.42%\n",
      "9\tF. straty dla zbioru walidacyjnego: 2.736600\tNajlepsza wartość straty: 0.110374\tDokładność: 79.59%\n",
      "10\tF. straty dla zbioru walidacyjnego: 2.817553\tNajlepsza wartość straty: 0.110374\tDokładność: 87.02%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.751824\tNajlepsza wartość straty: 0.110374\tDokładność: 90.34%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.526649\tNajlepsza wartość straty: 0.110374\tDokładność: 90.85%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.954679\tNajlepsza wartość straty: 0.110374\tDokładność: 90.54%\n",
      "14\tF. straty dla zbioru walidacyjnego: 2.019754\tNajlepsza wartość straty: 0.110374\tDokładność: 90.38%\n",
      "15\tF. straty dla zbioru walidacyjnego: 2.482432\tNajlepsza wartość straty: 0.110374\tDokładność: 91.83%\n",
      "16\tF. straty dla zbioru walidacyjnego: 2.355162\tNajlepsza wartość straty: 0.110374\tDokładność: 93.16%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.972294\tNajlepsza wartość straty: 0.110374\tDokładność: 92.77%\n",
      "18\tF. straty dla zbioru walidacyjnego: 2.100486\tNajlepsza wartość straty: 0.110374\tDokładność: 93.20%\n",
      "19\tF. straty dla zbioru walidacyjnego: 2.607072\tNajlepsza wartość straty: 0.110374\tDokładność: 87.53%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.775062\tNajlepsza wartość straty: 0.110374\tDokładność: 93.51%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.657023\tNajlepsza wartość straty: 0.110374\tDokładność: 93.20%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  10.2s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.141686\tNajlepsza wartość straty: 0.141686\tDokładność: 96.09%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.132817\tNajlepsza wartość straty: 0.132817\tDokładność: 96.79%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.095270\tNajlepsza wartość straty: 0.095270\tDokładność: 97.26%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.118041\tNajlepsza wartość straty: 0.095270\tDokładność: 97.34%\n",
      "4\tF. straty dla zbioru walidacyjnego: 13923.013672\tNajlepsza wartość straty: 0.095270\tDokładność: 32.10%\n",
      "5\tF. straty dla zbioru walidacyjnego: 41.863548\tNajlepsza wartość straty: 0.095270\tDokładność: 85.18%\n",
      "6\tF. straty dla zbioru walidacyjnego: 18.386723\tNajlepsza wartość straty: 0.095270\tDokładność: 90.73%\n",
      "7\tF. straty dla zbioru walidacyjnego: 15.095444\tNajlepsza wartość straty: 0.095270\tDokładność: 87.41%\n",
      "8\tF. straty dla zbioru walidacyjnego: 8.250523\tNajlepsza wartość straty: 0.095270\tDokładność: 93.08%\n",
      "9\tF. straty dla zbioru walidacyjnego: 7.918875\tNajlepsza wartość straty: 0.095270\tDokładność: 93.08%\n",
      "10\tF. straty dla zbioru walidacyjnego: 7.496359\tNajlepsza wartość straty: 0.095270\tDokładność: 92.65%\n",
      "11\tF. straty dla zbioru walidacyjnego: 6.901485\tNajlepsza wartość straty: 0.095270\tDokładność: 94.41%\n",
      "12\tF. straty dla zbioru walidacyjnego: 8.097102\tNajlepsza wartość straty: 0.095270\tDokładność: 95.07%\n",
      "13\tF. straty dla zbioru walidacyjnego: 4.866257\tNajlepsza wartość straty: 0.095270\tDokładność: 95.19%\n",
      "14\tF. straty dla zbioru walidacyjnego: 7.846973\tNajlepsza wartość straty: 0.095270\tDokładność: 92.73%\n",
      "15\tF. straty dla zbioru walidacyjnego: 5.789136\tNajlepsza wartość straty: 0.095270\tDokładność: 94.64%\n",
      "16\tF. straty dla zbioru walidacyjnego: 4.249867\tNajlepsza wartość straty: 0.095270\tDokładność: 93.86%\n",
      "17\tF. straty dla zbioru walidacyjnego: 3.706665\tNajlepsza wartość straty: 0.095270\tDokładność: 95.66%\n",
      "18\tF. straty dla zbioru walidacyjnego: 9.913536\tNajlepsza wartość straty: 0.095270\tDokładność: 81.59%\n",
      "19\tF. straty dla zbioru walidacyjnego: 3.257812\tNajlepsza wartość straty: 0.095270\tDokładność: 96.13%\n",
      "20\tF. straty dla zbioru walidacyjnego: 4.865031\tNajlepsza wartość straty: 0.095270\tDokładność: 92.34%\n",
      "21\tF. straty dla zbioru walidacyjnego: 6.557854\tNajlepsza wartość straty: 0.095270\tDokładność: 96.13%\n",
      "22\tF. straty dla zbioru walidacyjnego: 4.382745\tNajlepsza wartość straty: 0.095270\tDokładność: 96.33%\n",
      "23\tF. straty dla zbioru walidacyjnego: 6.169400\tNajlepsza wartość straty: 0.095270\tDokładność: 95.15%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  11.5s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.126084\tNajlepsza wartość straty: 0.126084\tDokładność: 96.40%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.136681\tNajlepsza wartość straty: 0.126084\tDokładność: 96.33%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.165544\tNajlepsza wartość straty: 0.126084\tDokładność: 95.82%\n",
      "3\tF. straty dla zbioru walidacyjnego: 825.825134\tNajlepsza wartość straty: 0.126084\tDokładność: 52.54%\n",
      "4\tF. straty dla zbioru walidacyjnego: 8.862791\tNajlepsza wartość straty: 0.126084\tDokładność: 90.77%\n",
      "5\tF. straty dla zbioru walidacyjnego: 4.116876\tNajlepsza wartość straty: 0.126084\tDokładność: 91.01%\n",
      "6\tF. straty dla zbioru walidacyjnego: 3.721356\tNajlepsza wartość straty: 0.126084\tDokładność: 91.28%\n",
      "7\tF. straty dla zbioru walidacyjnego: 2.750389\tNajlepsza wartość straty: 0.126084\tDokładność: 93.12%\n",
      "8\tF. straty dla zbioru walidacyjnego: 2.648105\tNajlepsza wartość straty: 0.126084\tDokładność: 90.89%\n",
      "9\tF. straty dla zbioru walidacyjnego: 3.036577\tNajlepsza wartość straty: 0.126084\tDokładność: 89.41%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.644020\tNajlepsza wartość straty: 0.126084\tDokładność: 93.94%\n",
      "11\tF. straty dla zbioru walidacyjnego: 2.728987\tNajlepsza wartość straty: 0.126084\tDokładność: 93.08%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.180722\tNajlepsza wartość straty: 0.126084\tDokładność: 94.88%\n",
      "13\tF. straty dla zbioru walidacyjnego: 2.271691\tNajlepsza wartość straty: 0.126084\tDokładność: 89.72%\n",
      "14\tF. straty dla zbioru walidacyjnego: 2.484967\tNajlepsza wartość straty: 0.126084\tDokładność: 92.22%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.003466\tNajlepsza wartość straty: 0.126084\tDokładność: 94.53%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.842463\tNajlepsza wartość straty: 0.126084\tDokładność: 95.54%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.080314\tNajlepsza wartość straty: 0.126084\tDokładność: 95.23%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.723011\tNajlepsza wartość straty: 0.126084\tDokładność: 95.97%\n",
      "19\tF. straty dla zbioru walidacyjnego: 3.615749\tNajlepsza wartość straty: 0.126084\tDokładność: 89.60%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.878952\tNajlepsza wartość straty: 0.126084\tDokładność: 95.86%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.528723\tNajlepsza wartość straty: 0.126084\tDokładność: 94.29%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  10.2s\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.251231\tNajlepsza wartość straty: 0.251231\tDokładność: 93.28%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.181496\tNajlepsza wartość straty: 0.181496\tDokładność: 93.98%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.160846\tNajlepsza wartość straty: 0.160846\tDokładność: 96.25%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.171285\tNajlepsza wartość straty: 0.160846\tDokładność: 95.62%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.163015\tNajlepsza wartość straty: 0.160846\tDokładność: 95.70%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.256955\tNajlepsza wartość straty: 0.160846\tDokładność: 93.75%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.150772\tNajlepsza wartość straty: 0.150772\tDokładność: 95.47%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.147298\tNajlepsza wartość straty: 0.147298\tDokładność: 95.90%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.190088\tNajlepsza wartość straty: 0.147298\tDokładność: 95.66%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.174188\tNajlepsza wartość straty: 0.147298\tDokładność: 96.05%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.159191\tNajlepsza wartość straty: 0.147298\tDokładność: 95.70%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.159187\tNajlepsza wartość straty: 0.147298\tDokładność: 95.27%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.232136\tNajlepsza wartość straty: 0.147298\tDokładność: 95.58%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.179838\tNajlepsza wartość straty: 0.147298\tDokładność: 95.62%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.334639\tNajlepsza wartość straty: 0.147298\tDokładność: 89.37%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.203263\tNajlepsza wartość straty: 0.147298\tDokładność: 95.97%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.424232\tNajlepsza wartość straty: 0.147298\tDokładność: 78.73%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.442037\tNajlepsza wartość straty: 0.147298\tDokładność: 78.38%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.496962\tNajlepsza wartość straty: 0.147298\tDokładność: 76.54%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.345068\tNajlepsza wartość straty: 0.147298\tDokładność: 91.87%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.245976\tNajlepsza wartość straty: 0.147298\tDokładność: 94.49%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.339882\tNajlepsza wartość straty: 0.147298\tDokładność: 90.73%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.256058\tNajlepsza wartość straty: 0.147298\tDokładność: 93.94%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.253395\tNajlepsza wartość straty: 0.147298\tDokładność: 95.19%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.426500\tNajlepsza wartość straty: 0.147298\tDokładność: 83.39%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.442524\tNajlepsza wartość straty: 0.147298\tDokładność: 86.24%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.294521\tNajlepsza wartość straty: 0.147298\tDokładność: 93.98%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.318557\tNajlepsza wartość straty: 0.147298\tDokładność: 91.05%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.477557\tNajlepsza wartość straty: 0.147298\tDokładność: 86.51%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total=  31.9s\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.170099\tNajlepsza wartość straty: 0.170099\tDokładność: 95.43%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.181517\tNajlepsza wartość straty: 0.170099\tDokładność: 95.82%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.375385\tNajlepsza wartość straty: 0.170099\tDokładność: 90.50%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.929977\tNajlepsza wartość straty: 0.170099\tDokładność: 81.98%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.155837\tNajlepsza wartość straty: 0.155837\tDokładność: 95.82%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.199897\tNajlepsza wartość straty: 0.155837\tDokładność: 95.50%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.188019\tNajlepsza wartość straty: 0.155837\tDokładność: 95.62%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.195650\tNajlepsza wartość straty: 0.155837\tDokładność: 95.97%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.303301\tNajlepsza wartość straty: 0.155837\tDokładność: 92.26%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.198584\tNajlepsza wartość straty: 0.155837\tDokładność: 95.90%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.217942\tNajlepsza wartość straty: 0.155837\tDokładność: 96.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.211459\tNajlepsza wartość straty: 0.155837\tDokładność: 95.00%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.200029\tNajlepsza wartość straty: 0.155837\tDokładność: 94.88%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.339233\tNajlepsza wartość straty: 0.155837\tDokładność: 91.01%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.183951\tNajlepsza wartość straty: 0.155837\tDokładność: 95.27%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.269957\tNajlepsza wartość straty: 0.155837\tDokładność: 94.33%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.235046\tNajlepsza wartość straty: 0.155837\tDokładność: 95.31%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.699826\tNajlepsza wartość straty: 0.155837\tDokładność: 65.25%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.444733\tNajlepsza wartość straty: 0.155837\tDokładność: 78.38%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.412999\tNajlepsza wartość straty: 0.155837\tDokładność: 78.69%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.426569\tNajlepsza wartość straty: 0.155837\tDokładność: 78.81%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.407093\tNajlepsza wartość straty: 0.155837\tDokładność: 78.15%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.735427\tNajlepsza wartość straty: 0.155837\tDokładność: 57.23%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.762173\tNajlepsza wartość straty: 0.155837\tDokładność: 58.80%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.699977\tNajlepsza wartość straty: 0.155837\tDokładność: 57.54%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.428770\tNajlepsza wartość straty: 0.155837\tDokładność: 78.93%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total=  27.8s\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.229509\tNajlepsza wartość straty: 0.229509\tDokładność: 93.90%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.168935\tNajlepsza wartość straty: 0.168935\tDokładność: 95.43%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.198185\tNajlepsza wartość straty: 0.168935\tDokładność: 94.53%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.155462\tNajlepsza wartość straty: 0.155462\tDokładność: 96.79%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.162245\tNajlepsza wartość straty: 0.155462\tDokładność: 96.09%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.156478\tNajlepsza wartość straty: 0.155462\tDokładność: 96.56%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.175575\tNajlepsza wartość straty: 0.155462\tDokładność: 96.44%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.136318\tNajlepsza wartość straty: 0.136318\tDokładność: 97.19%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.233459\tNajlepsza wartość straty: 0.136318\tDokładność: 92.92%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.172686\tNajlepsza wartość straty: 0.136318\tDokładność: 96.79%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.170386\tNajlepsza wartość straty: 0.136318\tDokładność: 95.82%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.148925\tNajlepsza wartość straty: 0.136318\tDokładność: 96.56%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.242782\tNajlepsza wartość straty: 0.136318\tDokładność: 96.17%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.320116\tNajlepsza wartość straty: 0.136318\tDokładność: 92.49%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.216614\tNajlepsza wartość straty: 0.136318\tDokładność: 96.83%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.249633\tNajlepsza wartość straty: 0.136318\tDokładność: 93.63%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.184815\tNajlepsza wartość straty: 0.136318\tDokładność: 96.72%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.235723\tNajlepsza wartość straty: 0.136318\tDokładność: 95.93%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.362397\tNajlepsza wartość straty: 0.136318\tDokładność: 94.29%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.196415\tNajlepsza wartość straty: 0.136318\tDokładność: 97.03%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.258364\tNajlepsza wartość straty: 0.136318\tDokładność: 95.04%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.235033\tNajlepsza wartość straty: 0.136318\tDokładność: 95.07%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.328133\tNajlepsza wartość straty: 0.136318\tDokładność: 93.08%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.299661\tNajlepsza wartość straty: 0.136318\tDokładność: 93.67%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.392923\tNajlepsza wartość straty: 0.136318\tDokładność: 76.74%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.612521\tNajlepsza wartość straty: 0.136318\tDokładność: 74.51%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.333263\tNajlepsza wartość straty: 0.136318\tDokładność: 91.01%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.521187\tNajlepsza wartość straty: 0.136318\tDokładność: 93.32%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.397723\tNajlepsza wartość straty: 0.136318\tDokładność: 86.43%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total=  31.4s\n",
      "[CV] n_neurons=140, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.885555\tNajlepsza wartość straty: 0.885555\tDokładność: 87.57%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.277295\tNajlepsza wartość straty: 0.277295\tDokładność: 93.43%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.155696\tNajlepsza wartość straty: 0.155696\tDokładność: 94.92%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.133315\tNajlepsza wartość straty: 0.133315\tDokładność: 96.05%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.113697\tNajlepsza wartość straty: 0.113697\tDokładność: 96.64%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.132807\tNajlepsza wartość straty: 0.113697\tDokładność: 96.56%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.101138\tNajlepsza wartość straty: 0.101138\tDokładność: 97.11%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.090616\tNajlepsza wartość straty: 0.090616\tDokładność: 97.50%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.087892\tNajlepsza wartość straty: 0.087892\tDokładność: 97.65%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.102796\tNajlepsza wartość straty: 0.087892\tDokładność: 97.11%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.093145\tNajlepsza wartość straty: 0.087892\tDokładność: 97.69%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.075636\tNajlepsza wartość straty: 0.075636\tDokładność: 97.73%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.166197\tNajlepsza wartość straty: 0.075636\tDokładność: 95.35%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.083386\tNajlepsza wartość straty: 0.075636\tDokładność: 98.20%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.082009\tNajlepsza wartość straty: 0.075636\tDokładność: 97.93%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.148396\tNajlepsza wartość straty: 0.075636\tDokładność: 97.69%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.073251\tNajlepsza wartość straty: 0.073251\tDokładność: 98.01%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.090398\tNajlepsza wartość straty: 0.073251\tDokładność: 97.54%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.086689\tNajlepsza wartość straty: 0.073251\tDokładność: 97.97%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.104127\tNajlepsza wartość straty: 0.073251\tDokładność: 97.58%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.100290\tNajlepsza wartość straty: 0.073251\tDokładność: 97.62%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.096803\tNajlepsza wartość straty: 0.073251\tDokładność: 97.81%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.086918\tNajlepsza wartość straty: 0.073251\tDokładność: 98.28%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.088679\tNajlepsza wartość straty: 0.073251\tDokładność: 98.08%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.104201\tNajlepsza wartość straty: 0.073251\tDokładność: 97.97%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.100813\tNajlepsza wartość straty: 0.073251\tDokładność: 97.69%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.072568\tNajlepsza wartość straty: 0.072568\tDokładność: 98.51%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.081339\tNajlepsza wartość straty: 0.072568\tDokładność: 98.40%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.086434\tNajlepsza wartość straty: 0.072568\tDokładność: 98.16%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.093168\tNajlepsza wartość straty: 0.072568\tDokładność: 98.16%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.099336\tNajlepsza wartość straty: 0.072568\tDokładność: 98.24%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.080387\tNajlepsza wartość straty: 0.072568\tDokładność: 98.48%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.151889\tNajlepsza wartość straty: 0.072568\tDokładność: 97.42%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.080271\tNajlepsza wartość straty: 0.072568\tDokładność: 98.16%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.082813\tNajlepsza wartość straty: 0.072568\tDokładność: 98.20%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.072009\tNajlepsza wartość straty: 0.072009\tDokładność: 98.51%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.082439\tNajlepsza wartość straty: 0.072009\tDokładność: 98.44%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.086242\tNajlepsza wartość straty: 0.072009\tDokładność: 98.32%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.069229\tNajlepsza wartość straty: 0.069229\tDokładność: 98.44%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.079815\tNajlepsza wartość straty: 0.069229\tDokładność: 98.67%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.084705\tNajlepsza wartość straty: 0.069229\tDokładność: 98.44%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.080542\tNajlepsza wartość straty: 0.069229\tDokładność: 98.59%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.089372\tNajlepsza wartość straty: 0.069229\tDokładność: 98.75%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.094080\tNajlepsza wartość straty: 0.069229\tDokładność: 98.44%\n",
      "44\tF. straty dla zbioru walidacyjnego: 0.128619\tNajlepsza wartość straty: 0.069229\tDokładność: 98.24%\n",
      "45\tF. straty dla zbioru walidacyjnego: 0.165794\tNajlepsza wartość straty: 0.069229\tDokładność: 97.97%\n",
      "46\tF. straty dla zbioru walidacyjnego: 0.104354\tNajlepsza wartość straty: 0.069229\tDokładność: 98.20%\n",
      "47\tF. straty dla zbioru walidacyjnego: 0.098409\tNajlepsza wartość straty: 0.069229\tDokładność: 98.16%\n",
      "48\tF. straty dla zbioru walidacyjnego: 0.092850\tNajlepsza wartość straty: 0.069229\tDokładność: 98.28%\n",
      "49\tF. straty dla zbioru walidacyjnego: 0.086260\tNajlepsza wartość straty: 0.069229\tDokładność: 98.67%\n",
      "50\tF. straty dla zbioru walidacyjnego: 0.147480\tNajlepsza wartość straty: 0.069229\tDokładność: 97.58%\n",
      "51\tF. straty dla zbioru walidacyjnego: 0.111573\tNajlepsza wartość straty: 0.069229\tDokładność: 98.28%\n",
      "52\tF. straty dla zbioru walidacyjnego: 0.104940\tNajlepsza wartość straty: 0.069229\tDokładność: 98.51%\n",
      "53\tF. straty dla zbioru walidacyjnego: 0.098577\tNajlepsza wartość straty: 0.069229\tDokładność: 98.67%\n",
      "54\tF. straty dla zbioru walidacyjnego: 0.113166\tNajlepsza wartość straty: 0.069229\tDokładność: 98.67%\n",
      "55\tF. straty dla zbioru walidacyjnego: 0.117886\tNajlepsza wartość straty: 0.069229\tDokładność: 98.55%\n",
      "56\tF. straty dla zbioru walidacyjnego: 0.147390\tNajlepsza wartość straty: 0.069229\tDokładność: 98.20%\n",
      "57\tF. straty dla zbioru walidacyjnego: 0.150841\tNajlepsza wartość straty: 0.069229\tDokładność: 98.05%\n",
      "58\tF. straty dla zbioru walidacyjnego: 648232.312500\tNajlepsza wartość straty: 0.069229\tDokładność: 22.01%\n",
      "59\tF. straty dla zbioru walidacyjnego: 44421540.000000\tNajlepsza wartość straty: 0.069229\tDokładność: 19.08%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  38.3s\n",
      "[CV] n_neurons=140, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 2.901128\tNajlepsza wartość straty: 2.901128\tDokładność: 64.74%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.289966\tNajlepsza wartość straty: 0.289966\tDokładność: 92.65%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.196209\tNajlepsza wartość straty: 0.196209\tDokładność: 94.33%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.131761\tNajlepsza wartość straty: 0.131761\tDokładność: 96.21%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.162388\tNajlepsza wartość straty: 0.131761\tDokładność: 95.11%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.118322\tNajlepsza wartość straty: 0.118322\tDokładność: 96.68%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.129350\tNajlepsza wartość straty: 0.118322\tDokładność: 96.36%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.134537\tNajlepsza wartość straty: 0.118322\tDokładność: 96.01%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.127696\tNajlepsza wartość straty: 0.118322\tDokładność: 97.07%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.130234\tNajlepsza wartość straty: 0.118322\tDokładność: 96.40%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.124699\tNajlepsza wartość straty: 0.118322\tDokładność: 96.76%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.121282\tNajlepsza wartość straty: 0.118322\tDokładność: 97.19%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.108090\tNajlepsza wartość straty: 0.108090\tDokładność: 97.77%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.128236\tNajlepsza wartość straty: 0.108090\tDokładność: 97.62%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.100385\tNajlepsza wartość straty: 0.100385\tDokładność: 97.58%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.117972\tNajlepsza wartość straty: 0.100385\tDokładność: 97.19%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.099533\tNajlepsza wartość straty: 0.099533\tDokładność: 97.81%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.095016\tNajlepsza wartość straty: 0.095016\tDokładność: 97.93%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.090823\tNajlepsza wartość straty: 0.090823\tDokładność: 98.24%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.169126\tNajlepsza wartość straty: 0.090823\tDokładność: 96.33%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.102346\tNajlepsza wartość straty: 0.090823\tDokładność: 97.97%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.115663\tNajlepsza wartość straty: 0.090823\tDokładność: 97.97%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.134390\tNajlepsza wartość straty: 0.090823\tDokładność: 97.62%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.120632\tNajlepsza wartość straty: 0.090823\tDokładność: 97.62%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.095852\tNajlepsza wartość straty: 0.090823\tDokładność: 98.16%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.125111\tNajlepsza wartość straty: 0.090823\tDokładność: 98.01%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.120941\tNajlepsza wartość straty: 0.090823\tDokładność: 98.12%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.175437\tNajlepsza wartość straty: 0.090823\tDokładność: 97.22%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.127602\tNajlepsza wartość straty: 0.090823\tDokładność: 98.05%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.150040\tNajlepsza wartość straty: 0.090823\tDokładność: 97.89%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.149179\tNajlepsza wartość straty: 0.090823\tDokładność: 97.97%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.157277\tNajlepsza wartość straty: 0.090823\tDokładność: 97.85%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.155046\tNajlepsza wartość straty: 0.090823\tDokładność: 97.65%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.223248\tNajlepsza wartość straty: 0.090823\tDokładność: 97.03%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.165429\tNajlepsza wartość straty: 0.090823\tDokładność: 97.73%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.166987\tNajlepsza wartość straty: 0.090823\tDokładność: 98.05%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.158941\tNajlepsza wartość straty: 0.090823\tDokładność: 98.16%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.171032\tNajlepsza wartość straty: 0.090823\tDokładność: 97.85%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.182900\tNajlepsza wartość straty: 0.090823\tDokładność: 98.01%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.129453\tNajlepsza wartość straty: 0.090823\tDokładność: 97.85%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  26.3s\n",
      "[CV] n_neurons=140, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 2.011704\tNajlepsza wartość straty: 2.011704\tDokładność: 79.63%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.377332\tNajlepsza wartość straty: 0.377332\tDokładność: 92.61%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.339921\tNajlepsza wartość straty: 0.339921\tDokładność: 93.78%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.208505\tNajlepsza wartość straty: 0.208505\tDokładność: 94.72%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.185562\tNajlepsza wartość straty: 0.185562\tDokładność: 95.23%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.123060\tNajlepsza wartość straty: 0.123060\tDokładność: 97.07%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.120585\tNajlepsza wartość straty: 0.120585\tDokładność: 96.95%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.122644\tNajlepsza wartość straty: 0.120585\tDokładność: 96.60%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.151848\tNajlepsza wartość straty: 0.120585\tDokładność: 96.40%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.110026\tNajlepsza wartość straty: 0.110026\tDokładność: 96.95%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.093443\tNajlepsza wartość straty: 0.093443\tDokładność: 97.26%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.097560\tNajlepsza wartość straty: 0.093443\tDokładność: 97.26%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.103248\tNajlepsza wartość straty: 0.093443\tDokładność: 97.19%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.128481\tNajlepsza wartość straty: 0.093443\tDokładność: 96.72%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.088036\tNajlepsza wartość straty: 0.088036\tDokładność: 97.77%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.119146\tNajlepsza wartość straty: 0.088036\tDokładność: 97.38%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.078963\tNajlepsza wartość straty: 0.078963\tDokładność: 97.65%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.120020\tNajlepsza wartość straty: 0.078963\tDokładność: 97.11%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.077677\tNajlepsza wartość straty: 0.077677\tDokładność: 97.93%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.090471\tNajlepsza wartość straty: 0.077677\tDokładność: 97.93%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.085294\tNajlepsza wartość straty: 0.077677\tDokładność: 98.05%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.015760\tNajlepsza wartość straty: 0.077677\tDokładność: 92.22%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.198181\tNajlepsza wartość straty: 0.077677\tDokładność: 96.36%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.106597\tNajlepsza wartość straty: 0.077677\tDokładność: 97.26%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.095509\tNajlepsza wartość straty: 0.077677\tDokładność: 97.30%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.098533\tNajlepsza wartość straty: 0.077677\tDokładność: 97.73%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.095375\tNajlepsza wartość straty: 0.077677\tDokładność: 97.77%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.092673\tNajlepsza wartość straty: 0.077677\tDokładność: 97.97%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.088631\tNajlepsza wartość straty: 0.077677\tDokładność: 97.85%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.101466\tNajlepsza wartość straty: 0.077677\tDokładność: 97.97%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.091870\tNajlepsza wartość straty: 0.077677\tDokładność: 98.01%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.094027\tNajlepsza wartość straty: 0.077677\tDokładność: 98.05%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.092561\tNajlepsza wartość straty: 0.077677\tDokładność: 98.16%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.107988\tNajlepsza wartość straty: 0.077677\tDokładność: 98.12%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.099924\tNajlepsza wartość straty: 0.077677\tDokładność: 98.16%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.104527\tNajlepsza wartość straty: 0.077677\tDokładność: 98.20%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.110419\tNajlepsza wartość straty: 0.077677\tDokładność: 98.16%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.134155\tNajlepsza wartość straty: 0.077677\tDokładność: 97.85%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.169738\tNajlepsza wartość straty: 0.077677\tDokładność: 97.97%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.136022\tNajlepsza wartość straty: 0.077677\tDokładność: 97.93%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  26.0s\n",
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.170474\tNajlepsza wartość straty: 0.170474\tDokładność: 95.78%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.146371\tNajlepsza wartość straty: 0.146371\tDokładność: 96.52%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.150955\tNajlepsza wartość straty: 0.146371\tDokładność: 96.56%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.160110\tNajlepsza wartość straty: 0.146371\tDokładność: 96.33%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.128113\tNajlepsza wartość straty: 0.128113\tDokładność: 96.99%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.586638\tNajlepsza wartość straty: 0.128113\tDokładność: 90.34%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.311918\tNajlepsza wartość straty: 0.128113\tDokładność: 93.00%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.498029\tNajlepsza wartość straty: 0.128113\tDokładność: 82.96%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.593628\tNajlepsza wartość straty: 0.128113\tDokładność: 75.29%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.338772\tNajlepsza wartość straty: 0.128113\tDokładność: 92.77%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.278458\tNajlepsza wartość straty: 0.128113\tDokładność: 93.47%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.455578\tNajlepsza wartość straty: 0.128113\tDokładność: 78.11%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.919116\tNajlepsza wartość straty: 0.128113\tDokładność: 92.69%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.368621\tNajlepsza wartość straty: 0.128113\tDokładność: 92.85%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.291389\tNajlepsza wartość straty: 0.128113\tDokładność: 95.11%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.330318\tNajlepsza wartość straty: 0.128113\tDokładność: 91.24%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.523563\tNajlepsza wartość straty: 0.128113\tDokładność: 77.25%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.562240\tNajlepsza wartość straty: 0.128113\tDokładność: 76.08%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.972188\tNajlepsza wartość straty: 0.128113\tDokładność: 59.73%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.805574\tNajlepsza wartość straty: 0.128113\tDokładność: 58.37%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.790685\tNajlepsza wartość straty: 0.128113\tDokładność: 59.62%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.922358\tNajlepsza wartość straty: 0.128113\tDokładność: 57.78%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.199813\tNajlepsza wartość straty: 0.128113\tDokładność: 37.49%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.344235\tNajlepsza wartość straty: 0.128113\tDokładność: 40.70%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.301918\tNajlepsza wartość straty: 0.128113\tDokładność: 40.73%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.317369\tNajlepsza wartość straty: 0.128113\tDokładność: 34.36%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=   6.3s\n",
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.758972\tNajlepsza wartość straty: 0.758972\tDokładność: 62.47%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.712293\tNajlepsza wartość straty: 0.712293\tDokładność: 68.88%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.660499\tNajlepsza wartość straty: 0.660499\tDokładność: 69.62%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.504557\tNajlepsza wartość straty: 0.504557\tDokładność: 81.47%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.426271\tNajlepsza wartość straty: 0.426271\tDokładność: 85.81%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.375548\tNajlepsza wartość straty: 0.375548\tDokładność: 87.96%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.408547\tNajlepsza wartość straty: 0.375548\tDokładność: 86.08%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.481642\tNajlepsza wartość straty: 0.375548\tDokładność: 84.68%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.457134\tNajlepsza wartość straty: 0.375548\tDokładność: 85.46%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.453440\tNajlepsza wartość straty: 0.375548\tDokładność: 84.68%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.419131\tNajlepsza wartość straty: 0.375548\tDokładność: 87.65%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.363298\tNajlepsza wartość straty: 0.363298\tDokładność: 89.13%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.395534\tNajlepsza wartość straty: 0.363298\tDokładność: 87.57%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.355166\tNajlepsza wartość straty: 0.355166\tDokładność: 88.90%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.365801\tNajlepsza wartość straty: 0.355166\tDokładność: 89.21%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.348468\tNajlepsza wartość straty: 0.348468\tDokładność: 88.82%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.348618\tNajlepsza wartość straty: 0.348468\tDokładność: 88.94%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.397291\tNajlepsza wartość straty: 0.348468\tDokładność: 89.01%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.352113\tNajlepsza wartość straty: 0.348468\tDokładność: 88.82%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.356764\tNajlepsza wartość straty: 0.348468\tDokładność: 88.12%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.381102\tNajlepsza wartość straty: 0.348468\tDokładność: 88.70%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.385127\tNajlepsza wartość straty: 0.348468\tDokładność: 88.15%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.443021\tNajlepsza wartość straty: 0.348468\tDokładność: 84.64%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.376504\tNajlepsza wartość straty: 0.348468\tDokładność: 87.96%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.405745\tNajlepsza wartość straty: 0.348468\tDokładność: 89.05%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.381359\tNajlepsza wartość straty: 0.348468\tDokładność: 88.66%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.349922\tNajlepsza wartość straty: 0.348468\tDokładność: 88.90%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.365997\tNajlepsza wartość straty: 0.348468\tDokładność: 88.74%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.342024\tNajlepsza wartość straty: 0.342024\tDokładność: 89.52%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.347647\tNajlepsza wartość straty: 0.342024\tDokładność: 90.03%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.393049\tNajlepsza wartość straty: 0.342024\tDokładność: 88.27%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.363485\tNajlepsza wartość straty: 0.342024\tDokładność: 90.19%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.346115\tNajlepsza wartość straty: 0.342024\tDokładność: 90.23%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.417754\tNajlepsza wartość straty: 0.342024\tDokładność: 85.81%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.345079\tNajlepsza wartość straty: 0.342024\tDokładność: 89.60%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.347902\tNajlepsza wartość straty: 0.342024\tDokładność: 89.48%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.401176\tNajlepsza wartość straty: 0.342024\tDokładność: 88.58%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.377315\tNajlepsza wartość straty: 0.342024\tDokładność: 88.39%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.590922\tNajlepsza wartość straty: 0.342024\tDokładność: 85.30%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.466982\tNajlepsza wartość straty: 0.342024\tDokładność: 87.37%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.459692\tNajlepsza wartość straty: 0.342024\tDokładność: 83.50%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.351623\tNajlepsza wartość straty: 0.342024\tDokładność: 89.91%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.409178\tNajlepsza wartość straty: 0.342024\tDokładność: 87.96%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.362301\tNajlepsza wartość straty: 0.342024\tDokładność: 89.87%\n",
      "44\tF. straty dla zbioru walidacyjnego: 0.393341\tNajlepsza wartość straty: 0.342024\tDokładność: 88.58%\n",
      "45\tF. straty dla zbioru walidacyjnego: 0.406688\tNajlepsza wartość straty: 0.342024\tDokładność: 86.86%\n",
      "46\tF. straty dla zbioru walidacyjnego: 0.362794\tNajlepsza wartość straty: 0.342024\tDokładność: 90.07%\n",
      "47\tF. straty dla zbioru walidacyjnego: 0.387565\tNajlepsza wartość straty: 0.342024\tDokładność: 88.86%\n",
      "48\tF. straty dla zbioru walidacyjnego: 0.413557\tNajlepsza wartość straty: 0.342024\tDokładność: 85.69%\n",
      "49\tF. straty dla zbioru walidacyjnego: 0.361268\tNajlepsza wartość straty: 0.342024\tDokładność: 89.29%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=  10.6s\n",
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.689934\tNajlepsza wartość straty: 0.689934\tDokładność: 67.20%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.630774\tNajlepsza wartość straty: 0.630774\tDokładność: 77.44%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.301046\tNajlepsza wartość straty: 0.301046\tDokładność: 89.87%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.376197\tNajlepsza wartość straty: 0.301046\tDokładność: 87.69%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.389894\tNajlepsza wartość straty: 0.301046\tDokładność: 86.98%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.286243\tNajlepsza wartość straty: 0.286243\tDokładność: 90.73%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.301413\tNajlepsza wartość straty: 0.286243\tDokładność: 90.19%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.389475\tNajlepsza wartość straty: 0.286243\tDokładność: 85.42%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.290606\tNajlepsza wartość straty: 0.286243\tDokładność: 91.13%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.366500\tNajlepsza wartość straty: 0.286243\tDokładność: 88.12%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.820078\tNajlepsza wartość straty: 0.286243\tDokładność: 59.54%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.983414\tNajlepsza wartość straty: 0.286243\tDokładność: 54.38%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.985894\tNajlepsza wartość straty: 0.286243\tDokładność: 58.29%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.791761\tNajlepsza wartość straty: 0.286243\tDokładność: 62.16%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.665473\tNajlepsza wartość straty: 0.286243\tDokładność: 69.70%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.668921\tNajlepsza wartość straty: 0.286243\tDokładność: 69.82%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.626769\tNajlepsza wartość straty: 0.286243\tDokładność: 72.95%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.582220\tNajlepsza wartość straty: 0.286243\tDokładność: 73.42%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.635232\tNajlepsza wartość straty: 0.286243\tDokładność: 70.88%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.581867\tNajlepsza wartość straty: 0.286243\tDokładność: 73.53%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.711130\tNajlepsza wartość straty: 0.286243\tDokładność: 70.60%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.824245\tNajlepsza wartość straty: 0.286243\tDokładność: 63.10%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.700296\tNajlepsza wartość straty: 0.286243\tDokładność: 70.13%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.679674\tNajlepsza wartość straty: 0.286243\tDokładność: 71.31%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.753570\tNajlepsza wartość straty: 0.286243\tDokładność: 63.84%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.778410\tNajlepsza wartość straty: 0.286243\tDokładność: 58.84%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.825266\tNajlepsza wartość straty: 0.286243\tDokładność: 56.45%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=   6.2s\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.167856\tNajlepsza wartość straty: 0.167856\tDokładność: 95.62%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.108457\tNajlepsza wartość straty: 0.108457\tDokładność: 96.64%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.115244\tNajlepsza wartość straty: 0.108457\tDokładność: 96.68%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.103707\tNajlepsza wartość straty: 0.103707\tDokładność: 97.11%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.088900\tNajlepsza wartość straty: 0.088900\tDokładność: 97.07%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.108242\tNajlepsza wartość straty: 0.088900\tDokładność: 96.44%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.102633\tNajlepsza wartość straty: 0.088900\tDokładność: 96.60%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.115583\tNajlepsza wartość straty: 0.088900\tDokładność: 96.44%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.115068\tNajlepsza wartość straty: 0.088900\tDokładność: 96.48%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.088219\tNajlepsza wartość straty: 0.088219\tDokładność: 97.50%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.087714\tNajlepsza wartość straty: 0.087714\tDokładność: 97.19%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.094173\tNajlepsza wartość straty: 0.087714\tDokładność: 97.22%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.096110\tNajlepsza wartość straty: 0.087714\tDokładność: 97.26%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.103788\tNajlepsza wartość straty: 0.087714\tDokładność: 97.03%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.103117\tNajlepsza wartość straty: 0.087714\tDokładność: 97.26%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.096846\tNajlepsza wartość straty: 0.087714\tDokładność: 97.26%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.093926\tNajlepsza wartość straty: 0.087714\tDokładność: 97.50%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.100891\tNajlepsza wartość straty: 0.087714\tDokładność: 97.50%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.111737\tNajlepsza wartość straty: 0.087714\tDokładność: 97.19%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.101377\tNajlepsza wartość straty: 0.087714\tDokładność: 97.26%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.103936\tNajlepsza wartość straty: 0.087714\tDokładność: 97.22%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.114787\tNajlepsza wartość straty: 0.087714\tDokładność: 97.22%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.106312\tNajlepsza wartość straty: 0.087714\tDokładność: 97.58%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.104593\tNajlepsza wartość straty: 0.087714\tDokładność: 97.46%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.112813\tNajlepsza wartość straty: 0.087714\tDokładność: 97.58%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.115383\tNajlepsza wartość straty: 0.087714\tDokładność: 97.26%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.122886\tNajlepsza wartość straty: 0.087714\tDokładność: 97.46%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.114413\tNajlepsza wartość straty: 0.087714\tDokładność: 97.19%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.103462\tNajlepsza wartość straty: 0.087714\tDokładność: 97.73%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.121505\tNajlepsza wartość straty: 0.087714\tDokładność: 97.19%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.111080\tNajlepsza wartość straty: 0.087714\tDokładność: 97.34%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.124049\tNajlepsza wartość straty: 0.087714\tDokładność: 97.22%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=   7.2s\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.213472\tNajlepsza wartość straty: 0.213472\tDokładność: 94.06%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.145661\tNajlepsza wartość straty: 0.145661\tDokładność: 96.05%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.123632\tNajlepsza wartość straty: 0.123632\tDokładność: 96.36%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.124052\tNajlepsza wartość straty: 0.123632\tDokładność: 96.44%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.144890\tNajlepsza wartość straty: 0.123632\tDokładność: 96.13%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.110220\tNajlepsza wartość straty: 0.110220\tDokładność: 97.11%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.120329\tNajlepsza wartość straty: 0.110220\tDokładność: 96.83%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.108252\tNajlepsza wartość straty: 0.108252\tDokładność: 97.07%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.112362\tNajlepsza wartość straty: 0.108252\tDokładność: 96.95%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.105016\tNajlepsza wartość straty: 0.105016\tDokładność: 96.91%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.132372\tNajlepsza wartość straty: 0.105016\tDokładność: 96.56%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.126517\tNajlepsza wartość straty: 0.105016\tDokładność: 96.79%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.112029\tNajlepsza wartość straty: 0.105016\tDokładność: 97.19%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.107584\tNajlepsza wartość straty: 0.105016\tDokładność: 96.99%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.105112\tNajlepsza wartość straty: 0.105016\tDokładność: 97.07%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.116186\tNajlepsza wartość straty: 0.105016\tDokładność: 96.99%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.117755\tNajlepsza wartość straty: 0.105016\tDokładność: 97.03%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.094675\tNajlepsza wartość straty: 0.094675\tDokładność: 97.22%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.102539\tNajlepsza wartość straty: 0.094675\tDokładność: 97.58%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.107347\tNajlepsza wartość straty: 0.094675\tDokładność: 97.15%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.111494\tNajlepsza wartość straty: 0.094675\tDokładność: 97.50%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.109670\tNajlepsza wartość straty: 0.094675\tDokładność: 97.19%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.108203\tNajlepsza wartość straty: 0.094675\tDokładność: 97.19%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.104844\tNajlepsza wartość straty: 0.094675\tDokładność: 97.30%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.130722\tNajlepsza wartość straty: 0.094675\tDokładność: 97.34%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.096134\tNajlepsza wartość straty: 0.094675\tDokładność: 97.50%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.112334\tNajlepsza wartość straty: 0.094675\tDokładność: 97.07%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.122940\tNajlepsza wartość straty: 0.094675\tDokładność: 97.07%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.125409\tNajlepsza wartość straty: 0.094675\tDokładność: 96.87%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.108948\tNajlepsza wartość straty: 0.094675\tDokładność: 97.34%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.112125\tNajlepsza wartość straty: 0.094675\tDokładność: 96.87%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.126527\tNajlepsza wartość straty: 0.094675\tDokładność: 97.22%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.106123\tNajlepsza wartość straty: 0.094675\tDokładność: 97.30%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.118133\tNajlepsza wartość straty: 0.094675\tDokładność: 97.03%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.124480\tNajlepsza wartość straty: 0.094675\tDokładność: 97.19%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.127079\tNajlepsza wartość straty: 0.094675\tDokładność: 97.26%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.106997\tNajlepsza wartość straty: 0.094675\tDokładność: 97.46%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.120236\tNajlepsza wartość straty: 0.094675\tDokładność: 97.15%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.104174\tNajlepsza wartość straty: 0.094675\tDokładność: 97.22%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=   8.7s\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.229440\tNajlepsza wartość straty: 0.229440\tDokładność: 94.02%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.148883\tNajlepsza wartość straty: 0.148883\tDokładność: 95.58%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.141299\tNajlepsza wartość straty: 0.141299\tDokładność: 96.29%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.124889\tNajlepsza wartość straty: 0.124889\tDokładność: 96.48%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.135961\tNajlepsza wartość straty: 0.124889\tDokładność: 96.79%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.119037\tNajlepsza wartość straty: 0.119037\tDokładność: 96.87%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.109765\tNajlepsza wartość straty: 0.109765\tDokładność: 97.03%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.119925\tNajlepsza wartość straty: 0.109765\tDokładność: 96.76%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.108674\tNajlepsza wartość straty: 0.108674\tDokładność: 97.15%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.104904\tNajlepsza wartość straty: 0.104904\tDokładność: 96.99%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.114967\tNajlepsza wartość straty: 0.104904\tDokładność: 96.99%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.093642\tNajlepsza wartość straty: 0.093642\tDokładność: 97.85%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.088363\tNajlepsza wartość straty: 0.088363\tDokładność: 97.81%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.088819\tNajlepsza wartość straty: 0.088363\tDokładność: 97.77%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.089590\tNajlepsza wartość straty: 0.088363\tDokładność: 97.73%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.084302\tNajlepsza wartość straty: 0.084302\tDokładność: 97.97%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.099357\tNajlepsza wartość straty: 0.084302\tDokładność: 97.50%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.086152\tNajlepsza wartość straty: 0.084302\tDokładność: 97.54%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.090644\tNajlepsza wartość straty: 0.084302\tDokładność: 97.93%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.100587\tNajlepsza wartość straty: 0.084302\tDokładność: 97.38%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.108705\tNajlepsza wartość straty: 0.084302\tDokładność: 97.58%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.094549\tNajlepsza wartość straty: 0.084302\tDokładność: 97.54%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.088551\tNajlepsza wartość straty: 0.084302\tDokładność: 97.62%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.121881\tNajlepsza wartość straty: 0.084302\tDokładność: 97.15%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.095039\tNajlepsza wartość straty: 0.084302\tDokładność: 97.73%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.084652\tNajlepsza wartość straty: 0.084302\tDokładność: 97.97%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.112332\tNajlepsza wartość straty: 0.084302\tDokładność: 97.50%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.100713\tNajlepsza wartość straty: 0.084302\tDokładność: 97.50%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.115726\tNajlepsza wartość straty: 0.084302\tDokładność: 97.65%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.101663\tNajlepsza wartość straty: 0.084302\tDokładność: 97.81%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.109195\tNajlepsza wartość straty: 0.084302\tDokładność: 97.54%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.089981\tNajlepsza wartość straty: 0.084302\tDokładność: 97.89%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.096196\tNajlepsza wartość straty: 0.084302\tDokładność: 97.69%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.090680\tNajlepsza wartość straty: 0.084302\tDokładność: 97.97%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.092241\tNajlepsza wartość straty: 0.084302\tDokładność: 97.73%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.092533\tNajlepsza wartość straty: 0.084302\tDokładność: 98.20%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.098719\tNajlepsza wartość straty: 0.084302\tDokładność: 97.81%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=   8.1s\n",
      "[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.087748\tNajlepsza wartość straty: 0.087748\tDokładność: 97.38%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.081187\tNajlepsza wartość straty: 0.081187\tDokładność: 97.73%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.078718\tNajlepsza wartość straty: 0.078718\tDokładność: 97.97%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.996712\tNajlepsza wartość straty: 0.078718\tDokładność: 80.49%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.098560\tNajlepsza wartość straty: 0.078718\tDokładność: 97.69%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.106743\tNajlepsza wartość straty: 0.078718\tDokładność: 98.01%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.082764\tNajlepsza wartość straty: 0.078718\tDokładność: 97.81%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.051687\tNajlepsza wartość straty: 0.051687\tDokładność: 98.67%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.062863\tNajlepsza wartość straty: 0.051687\tDokładność: 98.44%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.109466\tNajlepsza wartość straty: 0.051687\tDokładność: 98.63%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.294863\tNajlepsza wartość straty: 0.051687\tDokładność: 79.67%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.636340\tNajlepsza wartość straty: 0.051687\tDokładność: 22.01%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.629150\tNajlepsza wartość straty: 0.051687\tDokładność: 20.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.610085\tNajlepsza wartość straty: 0.051687\tDokładność: 20.91%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.643361\tNajlepsza wartość straty: 0.051687\tDokładność: 19.08%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.624260\tNajlepsza wartość straty: 0.051687\tDokładność: 22.01%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.630033\tNajlepsza wartość straty: 0.051687\tDokładność: 22.01%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.627744\tNajlepsza wartość straty: 0.051687\tDokładność: 19.27%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.691903\tNajlepsza wartość straty: 0.051687\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.623106\tNajlepsza wartość straty: 0.051687\tDokładność: 22.01%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.647248\tNajlepsza wartość straty: 0.051687\tDokładność: 19.27%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.649978\tNajlepsza wartość straty: 0.051687\tDokładność: 19.08%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.636020\tNajlepsza wartość straty: 0.051687\tDokładność: 22.01%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.615931\tNajlepsza wartość straty: 0.051687\tDokładność: 19.08%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.660141\tNajlepsza wartość straty: 0.051687\tDokładność: 19.27%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.665775\tNajlepsza wartość straty: 0.051687\tDokładność: 22.01%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.719179\tNajlepsza wartość straty: 0.051687\tDokładność: 20.91%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.697971\tNajlepsza wartość straty: 0.051687\tDokładność: 19.08%\n",
      "28\tF. straty dla zbioru walidacyjnego: 1.652521\tNajlepsza wartość straty: 0.051687\tDokładność: 19.08%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=  15.2s\n",
      "[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.100436\tNajlepsza wartość straty: 0.100436\tDokładność: 97.34%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.082984\tNajlepsza wartość straty: 0.082984\tDokładność: 97.46%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.109492\tNajlepsza wartość straty: 0.082984\tDokładność: 96.76%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.060599\tNajlepsza wartość straty: 0.060599\tDokładność: 98.20%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.061442\tNajlepsza wartość straty: 0.060599\tDokładność: 98.28%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.069166\tNajlepsza wartość straty: 0.060599\tDokładność: 98.12%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.080251\tNajlepsza wartość straty: 0.060599\tDokładność: 97.81%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.083831\tNajlepsza wartość straty: 0.060599\tDokładność: 98.32%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.098979\tNajlepsza wartość straty: 0.060599\tDokładność: 98.12%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.056946\tNajlepsza wartość straty: 0.056946\tDokładność: 98.67%\n",
      "10\tF. straty dla zbioru walidacyjnego: 3.896351\tNajlepsza wartość straty: 0.056946\tDokładność: 20.91%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.616500\tNajlepsza wartość straty: 0.056946\tDokładność: 19.08%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.608680\tNajlepsza wartość straty: 0.056946\tDokładność: 22.01%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.611189\tNajlepsza wartość straty: 0.056946\tDokładność: 20.91%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.617403\tNajlepsza wartość straty: 0.056946\tDokładność: 22.01%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.619442\tNajlepsza wartość straty: 0.056946\tDokładność: 19.08%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.629762\tNajlepsza wartość straty: 0.056946\tDokładność: 18.73%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.629232\tNajlepsza wartość straty: 0.056946\tDokładność: 20.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.651712\tNajlepsza wartość straty: 0.056946\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.623236\tNajlepsza wartość straty: 0.056946\tDokładność: 19.27%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.628258\tNajlepsza wartość straty: 0.056946\tDokładność: 19.27%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.715391\tNajlepsza wartość straty: 0.056946\tDokładność: 22.01%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.641515\tNajlepsza wartość straty: 0.056946\tDokładność: 19.27%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.658927\tNajlepsza wartość straty: 0.056946\tDokładność: 22.01%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.688920\tNajlepsza wartość straty: 0.056946\tDokładność: 22.01%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.619269\tNajlepsza wartość straty: 0.056946\tDokładność: 19.27%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.636429\tNajlepsza wartość straty: 0.056946\tDokładność: 19.08%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.630284\tNajlepsza wartość straty: 0.056946\tDokładność: 19.27%\n",
      "28\tF. straty dla zbioru walidacyjnego: 1.611157\tNajlepsza wartość straty: 0.056946\tDokładność: 22.01%\n",
      "29\tF. straty dla zbioru walidacyjnego: 1.630368\tNajlepsza wartość straty: 0.056946\tDokładność: 18.73%\n",
      "30\tF. straty dla zbioru walidacyjnego: 1.631085\tNajlepsza wartość straty: 0.056946\tDokładność: 22.01%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=  16.7s\n",
      "[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.093557\tNajlepsza wartość straty: 0.093557\tDokładność: 98.08%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.083387\tNajlepsza wartość straty: 0.083387\tDokładność: 97.89%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.090065\tNajlepsza wartość straty: 0.083387\tDokładność: 97.50%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.068677\tNajlepsza wartość straty: 0.068677\tDokładność: 98.24%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.083533\tNajlepsza wartość straty: 0.068677\tDokładność: 97.77%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.087899\tNajlepsza wartość straty: 0.068677\tDokładność: 98.05%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.076249\tNajlepsza wartość straty: 0.068677\tDokładność: 98.12%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.105916\tNajlepsza wartość straty: 0.068677\tDokładność: 97.77%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.208663\tNajlepsza wartość straty: 0.068677\tDokładność: 97.11%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.655652\tNajlepsza wartość straty: 0.068677\tDokładność: 19.08%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.609612\tNajlepsza wartość straty: 0.068677\tDokładność: 22.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.626743\tNajlepsza wartość straty: 0.068677\tDokładność: 19.27%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.664769\tNajlepsza wartość straty: 0.068677\tDokładność: 20.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.650111\tNajlepsza wartość straty: 0.068677\tDokładność: 19.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.631884\tNajlepsza wartość straty: 0.068677\tDokładność: 19.08%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.645576\tNajlepsza wartość straty: 0.068677\tDokładność: 22.01%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.685346\tNajlepsza wartość straty: 0.068677\tDokładność: 18.73%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.630290\tNajlepsza wartość straty: 0.068677\tDokładność: 20.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.639349\tNajlepsza wartość straty: 0.068677\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.650071\tNajlepsza wartość straty: 0.068677\tDokładność: 20.91%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.643313\tNajlepsza wartość straty: 0.068677\tDokładność: 19.27%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.723223\tNajlepsza wartość straty: 0.068677\tDokładność: 19.27%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.623018\tNajlepsza wartość straty: 0.068677\tDokładność: 18.73%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.614361\tNajlepsza wartość straty: 0.068677\tDokładność: 22.01%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.671116\tNajlepsza wartość straty: 0.068677\tDokładność: 20.91%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=  13.2s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.101148\tNajlepsza wartość straty: 0.101148\tDokładność: 97.07%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.172148\tNajlepsza wartość straty: 0.101148\tDokładność: 95.50%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.105509\tNajlepsza wartość straty: 0.101148\tDokładność: 97.15%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.081293\tNajlepsza wartość straty: 0.081293\tDokładność: 97.54%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.312333\tNajlepsza wartość straty: 0.081293\tDokładność: 94.02%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.092525\tNajlepsza wartość straty: 0.081293\tDokładność: 97.62%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.083483\tNajlepsza wartość straty: 0.081293\tDokładność: 97.58%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.207208\tNajlepsza wartość straty: 0.081293\tDokładność: 97.38%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.083394\tNajlepsza wartość straty: 0.081293\tDokładność: 97.73%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.175250\tNajlepsza wartość straty: 0.081293\tDokładność: 97.62%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.097859\tNajlepsza wartość straty: 0.081293\tDokładność: 98.12%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.107117\tNajlepsza wartość straty: 0.081293\tDokładność: 97.26%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.105163\tNajlepsza wartość straty: 0.081293\tDokładność: 97.50%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.116576\tNajlepsza wartość straty: 0.081293\tDokładność: 97.42%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.101296\tNajlepsza wartość straty: 0.081293\tDokładność: 97.62%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.142922\tNajlepsza wartość straty: 0.081293\tDokładność: 97.07%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.156520\tNajlepsza wartość straty: 0.081293\tDokładność: 96.72%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.138988\tNajlepsza wartość straty: 0.081293\tDokładność: 97.65%\n",
      "18\tF. straty dla zbioru walidacyjnego: 3.262866\tNajlepsza wartość straty: 0.081293\tDokładność: 88.55%\n",
      "19\tF. straty dla zbioru walidacyjnego: 5.570070\tNajlepsza wartość straty: 0.081293\tDokładność: 93.00%\n",
      "20\tF. straty dla zbioru walidacyjnego: 3.466961\tNajlepsza wartość straty: 0.081293\tDokładność: 95.93%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.763105\tNajlepsza wartość straty: 0.081293\tDokładność: 96.91%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.709509\tNajlepsza wartość straty: 0.081293\tDokładność: 97.38%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.522372\tNajlepsza wartość straty: 0.081293\tDokładność: 96.33%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.534245\tNajlepsza wartość straty: 0.081293\tDokładność: 97.22%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=   9.4s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.128931\tNajlepsza wartość straty: 0.128931\tDokładność: 96.44%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.096025\tNajlepsza wartość straty: 0.096025\tDokładność: 97.50%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.070108\tNajlepsza wartość straty: 0.070108\tDokładność: 98.08%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.099106\tNajlepsza wartość straty: 0.070108\tDokładność: 97.58%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.069380\tNajlepsza wartość straty: 0.069380\tDokładność: 97.69%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.090373\tNajlepsza wartość straty: 0.069380\tDokładność: 97.54%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.095254\tNajlepsza wartość straty: 0.069380\tDokładność: 97.65%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.294960\tNajlepsza wartość straty: 0.069380\tDokładność: 93.67%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.194309\tNajlepsza wartość straty: 0.069380\tDokładność: 95.35%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.171960\tNajlepsza wartość straty: 0.069380\tDokładność: 96.21%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.150022\tNajlepsza wartość straty: 0.069380\tDokładność: 96.17%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.161224\tNajlepsza wartość straty: 0.069380\tDokładność: 96.79%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.167953\tNajlepsza wartość straty: 0.069380\tDokładność: 96.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.131126\tNajlepsza wartość straty: 0.069380\tDokładność: 97.11%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.209527\tNajlepsza wartość straty: 0.069380\tDokładność: 96.79%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.157937\tNajlepsza wartość straty: 0.069380\tDokładność: 97.50%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.208552\tNajlepsza wartość straty: 0.069380\tDokładność: 97.58%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.138353\tNajlepsza wartość straty: 0.069380\tDokładność: 97.15%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.100935\tNajlepsza wartość straty: 0.069380\tDokładność: 97.73%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.116350\tNajlepsza wartość straty: 0.069380\tDokładność: 97.11%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.112213\tNajlepsza wartość straty: 0.069380\tDokładność: 97.50%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.191542\tNajlepsza wartość straty: 0.069380\tDokładność: 97.38%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.149784\tNajlepsza wartość straty: 0.069380\tDokładność: 97.97%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.163074\tNajlepsza wartość straty: 0.069380\tDokładność: 97.34%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.191671\tNajlepsza wartość straty: 0.069380\tDokładność: 96.13%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.137559\tNajlepsza wartość straty: 0.069380\tDokładność: 97.07%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=   9.5s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.114279\tNajlepsza wartość straty: 0.114279\tDokładność: 96.44%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.087768\tNajlepsza wartość straty: 0.087768\tDokładność: 97.30%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.086912\tNajlepsza wartość straty: 0.086912\tDokładność: 97.54%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.084269\tNajlepsza wartość straty: 0.084269\tDokładność: 98.01%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.092843\tNajlepsza wartość straty: 0.084269\tDokładność: 97.54%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.136506\tNajlepsza wartość straty: 0.084269\tDokładność: 97.26%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.254653\tNajlepsza wartość straty: 0.084269\tDokładność: 96.36%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.171380\tNajlepsza wartość straty: 0.084269\tDokładność: 96.87%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.167416\tNajlepsza wartość straty: 0.084269\tDokładność: 96.76%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.169040\tNajlepsza wartość straty: 0.084269\tDokładność: 97.30%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.180098\tNajlepsza wartość straty: 0.084269\tDokładność: 97.42%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.182409\tNajlepsza wartość straty: 0.084269\tDokładność: 97.73%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.231386\tNajlepsza wartość straty: 0.084269\tDokładność: 97.81%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.244978\tNajlepsza wartość straty: 0.084269\tDokładność: 97.73%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.234075\tNajlepsza wartość straty: 0.084269\tDokładność: 97.62%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.299513\tNajlepsza wartość straty: 0.084269\tDokładność: 97.73%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.202003\tNajlepsza wartość straty: 0.084269\tDokładność: 97.77%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.239913\tNajlepsza wartość straty: 0.084269\tDokładność: 97.65%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.237417\tNajlepsza wartość straty: 0.084269\tDokładność: 97.54%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.254710\tNajlepsza wartość straty: 0.084269\tDokładność: 97.81%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.386830\tNajlepsza wartość straty: 0.084269\tDokładność: 97.65%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.200112\tNajlepsza wartość straty: 0.084269\tDokładność: 97.81%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.834171\tNajlepsza wartość straty: 0.084269\tDokładność: 90.38%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.171077\tNajlepsza wartość straty: 0.084269\tDokładność: 96.99%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.134054\tNajlepsza wartość straty: 0.084269\tDokładność: 97.58%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=   9.5s\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 2.076442\tNajlepsza wartość straty: 2.076442\tDokładność: 18.73%\n",
      "1\tF. straty dla zbioru walidacyjnego: 2.428190\tNajlepsza wartość straty: 2.076442\tDokładność: 19.08%\n",
      "2\tF. straty dla zbioru walidacyjnego: 2.307039\tNajlepsza wartość straty: 2.076442\tDokładność: 19.27%\n",
      "3\tF. straty dla zbioru walidacyjnego: 2.756636\tNajlepsza wartość straty: 2.076442\tDokładność: 22.01%\n",
      "4\tF. straty dla zbioru walidacyjnego: 2.369078\tNajlepsza wartość straty: 2.076442\tDokładność: 18.73%\n",
      "5\tF. straty dla zbioru walidacyjnego: 2.553977\tNajlepsza wartość straty: 2.076442\tDokładność: 20.91%\n",
      "6\tF. straty dla zbioru walidacyjnego: 2.177732\tNajlepsza wartość straty: 2.076442\tDokładność: 19.27%\n",
      "7\tF. straty dla zbioru walidacyjnego: 2.358694\tNajlepsza wartość straty: 2.076442\tDokładność: 19.08%\n",
      "8\tF. straty dla zbioru walidacyjnego: 2.946320\tNajlepsza wartość straty: 2.076442\tDokładność: 19.08%\n",
      "9\tF. straty dla zbioru walidacyjnego: 2.138835\tNajlepsza wartość straty: 2.076442\tDokładność: 19.27%\n",
      "10\tF. straty dla zbioru walidacyjnego: 2.189744\tNajlepsza wartość straty: 2.076442\tDokładność: 19.27%\n",
      "11\tF. straty dla zbioru walidacyjnego: 3.824192\tNajlepsza wartość straty: 2.076442\tDokładność: 19.08%\n",
      "12\tF. straty dla zbioru walidacyjnego: 2.287050\tNajlepsza wartość straty: 2.076442\tDokładność: 22.01%\n",
      "13\tF. straty dla zbioru walidacyjnego: 2.921958\tNajlepsza wartość straty: 2.076442\tDokładność: 19.27%\n",
      "14\tF. straty dla zbioru walidacyjnego: 3.607187\tNajlepsza wartość straty: 2.076442\tDokładność: 18.73%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.990910\tNajlepsza wartość straty: 1.990910\tDokładność: 19.08%\n",
      "16\tF. straty dla zbioru walidacyjnego: 2.753008\tNajlepsza wartość straty: 1.990910\tDokładność: 22.01%\n",
      "17\tF. straty dla zbioru walidacyjnego: 3.354759\tNajlepsza wartość straty: 1.990910\tDokładność: 19.08%\n",
      "18\tF. straty dla zbioru walidacyjnego: 3.922930\tNajlepsza wartość straty: 1.990910\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 3.308564\tNajlepsza wartość straty: 1.990910\tDokładność: 22.01%\n",
      "20\tF. straty dla zbioru walidacyjnego: 3.115769\tNajlepsza wartość straty: 1.990910\tDokładność: 20.91%\n",
      "21\tF. straty dla zbioru walidacyjnego: 2.433603\tNajlepsza wartość straty: 1.990910\tDokładność: 20.91%\n",
      "22\tF. straty dla zbioru walidacyjnego: 2.151388\tNajlepsza wartość straty: 1.990910\tDokładność: 22.01%\n",
      "23\tF. straty dla zbioru walidacyjnego: 2.456521\tNajlepsza wartość straty: 1.990910\tDokładność: 20.91%\n",
      "24\tF. straty dla zbioru walidacyjnego: 2.226642\tNajlepsza wartość straty: 1.990910\tDokładność: 20.91%\n",
      "25\tF. straty dla zbioru walidacyjnego: 2.193438\tNajlepsza wartość straty: 1.990910\tDokładność: 19.08%\n",
      "26\tF. straty dla zbioru walidacyjnego: 2.799143\tNajlepsza wartość straty: 1.990910\tDokładność: 20.91%\n",
      "27\tF. straty dla zbioru walidacyjnego: 2.042701\tNajlepsza wartość straty: 1.990910\tDokładność: 22.01%\n",
      "28\tF. straty dla zbioru walidacyjnego: 4.522389\tNajlepsza wartość straty: 1.990910\tDokładność: 19.08%\n",
      "29\tF. straty dla zbioru walidacyjnego: 2.374831\tNajlepsza wartość straty: 1.990910\tDokładność: 19.08%\n",
      "30\tF. straty dla zbioru walidacyjnego: 3.447889\tNajlepsza wartość straty: 1.990910\tDokładność: 20.91%\n",
      "31\tF. straty dla zbioru walidacyjnego: 2.633521\tNajlepsza wartość straty: 1.990910\tDokładność: 19.08%\n",
      "32\tF. straty dla zbioru walidacyjnego: 2.296706\tNajlepsza wartość straty: 1.990910\tDokładność: 22.01%\n",
      "33\tF. straty dla zbioru walidacyjnego: 1.987826\tNajlepsza wartość straty: 1.987826\tDokładność: 22.01%\n",
      "34\tF. straty dla zbioru walidacyjnego: 1.698200\tNajlepsza wartość straty: 1.698200\tDokładność: 19.27%\n",
      "35\tF. straty dla zbioru walidacyjnego: 1.921056\tNajlepsza wartość straty: 1.698200\tDokładność: 19.27%\n",
      "36\tF. straty dla zbioru walidacyjnego: 3.619339\tNajlepsza wartość straty: 1.698200\tDokładność: 22.01%\n",
      "37\tF. straty dla zbioru walidacyjnego: 1.977261\tNajlepsza wartość straty: 1.698200\tDokładność: 18.73%\n",
      "38\tF. straty dla zbioru walidacyjnego: 2.175638\tNajlepsza wartość straty: 1.698200\tDokładność: 22.01%\n",
      "39\tF. straty dla zbioru walidacyjnego: 4.466838\tNajlepsza wartość straty: 1.698200\tDokładność: 18.73%\n",
      "40\tF. straty dla zbioru walidacyjnego: 4.394022\tNajlepsza wartość straty: 1.698200\tDokładność: 18.73%\n",
      "41\tF. straty dla zbioru walidacyjnego: 3.069252\tNajlepsza wartość straty: 1.698200\tDokładność: 19.27%\n",
      "42\tF. straty dla zbioru walidacyjnego: 2.522177\tNajlepsza wartość straty: 1.698200\tDokładność: 19.27%\n",
      "43\tF. straty dla zbioru walidacyjnego: 2.563700\tNajlepsza wartość straty: 1.698200\tDokładność: 19.27%\n",
      "44\tF. straty dla zbioru walidacyjnego: 2.195197\tNajlepsza wartość straty: 1.698200\tDokładność: 19.08%\n",
      "45\tF. straty dla zbioru walidacyjnego: 2.543791\tNajlepsza wartość straty: 1.698200\tDokładność: 18.73%\n",
      "46\tF. straty dla zbioru walidacyjnego: 3.404180\tNajlepsza wartość straty: 1.698200\tDokładność: 18.73%\n",
      "47\tF. straty dla zbioru walidacyjnego: 2.296740\tNajlepsza wartość straty: 1.698200\tDokładność: 22.01%\n",
      "48\tF. straty dla zbioru walidacyjnego: 2.188519\tNajlepsza wartość straty: 1.698200\tDokładność: 19.08%\n",
      "49\tF. straty dla zbioru walidacyjnego: 2.347929\tNajlepsza wartość straty: 1.698200\tDokładność: 22.01%\n",
      "50\tF. straty dla zbioru walidacyjnego: 3.556148\tNajlepsza wartość straty: 1.698200\tDokładność: 19.27%\n",
      "51\tF. straty dla zbioru walidacyjnego: 3.182601\tNajlepsza wartość straty: 1.698200\tDokładność: 22.01%\n",
      "52\tF. straty dla zbioru walidacyjnego: 2.342917\tNajlepsza wartość straty: 1.698200\tDokładność: 19.27%\n",
      "53\tF. straty dla zbioru walidacyjnego: 1.673205\tNajlepsza wartość straty: 1.673205\tDokładność: 22.01%\n",
      "54\tF. straty dla zbioru walidacyjnego: 3.453861\tNajlepsza wartość straty: 1.673205\tDokładność: 19.27%\n",
      "55\tF. straty dla zbioru walidacyjnego: 2.349484\tNajlepsza wartość straty: 1.673205\tDokładność: 19.27%\n",
      "56\tF. straty dla zbioru walidacyjnego: 3.106301\tNajlepsza wartość straty: 1.673205\tDokładność: 19.08%\n",
      "57\tF. straty dla zbioru walidacyjnego: 1.906423\tNajlepsza wartość straty: 1.673205\tDokładność: 18.73%\n",
      "58\tF. straty dla zbioru walidacyjnego: 2.991432\tNajlepsza wartość straty: 1.673205\tDokładność: 19.27%\n",
      "59\tF. straty dla zbioru walidacyjnego: 2.658706\tNajlepsza wartość straty: 1.673205\tDokładność: 22.01%\n",
      "60\tF. straty dla zbioru walidacyjnego: 3.324445\tNajlepsza wartość straty: 1.673205\tDokładność: 19.27%\n",
      "61\tF. straty dla zbioru walidacyjnego: 3.002789\tNajlepsza wartość straty: 1.673205\tDokładność: 19.08%\n",
      "62\tF. straty dla zbioru walidacyjnego: 3.436943\tNajlepsza wartość straty: 1.673205\tDokładność: 20.91%\n",
      "63\tF. straty dla zbioru walidacyjnego: 2.181111\tNajlepsza wartość straty: 1.673205\tDokładność: 19.27%\n",
      "64\tF. straty dla zbioru walidacyjnego: 2.536390\tNajlepsza wartość straty: 1.673205\tDokładność: 19.27%\n",
      "65\tF. straty dla zbioru walidacyjnego: 2.101936\tNajlepsza wartość straty: 1.673205\tDokładność: 22.01%\n",
      "66\tF. straty dla zbioru walidacyjnego: 2.098545\tNajlepsza wartość straty: 1.673205\tDokładność: 20.91%\n",
      "67\tF. straty dla zbioru walidacyjnego: 3.876142\tNajlepsza wartość straty: 1.673205\tDokładność: 18.73%\n",
      "68\tF. straty dla zbioru walidacyjnego: 3.883032\tNajlepsza wartość straty: 1.673205\tDokładność: 18.73%\n",
      "69\tF. straty dla zbioru walidacyjnego: 1.946106\tNajlepsza wartość straty: 1.673205\tDokładność: 18.73%\n",
      "70\tF. straty dla zbioru walidacyjnego: 3.507969\tNajlepsza wartość straty: 1.673205\tDokładność: 22.01%\n",
      "71\tF. straty dla zbioru walidacyjnego: 1.821180\tNajlepsza wartość straty: 1.673205\tDokładność: 20.91%\n",
      "72\tF. straty dla zbioru walidacyjnego: 2.230544\tNajlepsza wartość straty: 1.673205\tDokładność: 19.08%\n",
      "73\tF. straty dla zbioru walidacyjnego: 2.909559\tNajlepsza wartość straty: 1.673205\tDokładność: 20.91%\n",
      "74\tF. straty dla zbioru walidacyjnego: 3.413930\tNajlepsza wartość straty: 1.673205\tDokładność: 18.73%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total= 3.0min\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.838628\tNajlepsza wartość straty: 1.838628\tDokładność: 20.91%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.696655\tNajlepsza wartość straty: 1.696655\tDokładność: 19.08%\n",
      "2\tF. straty dla zbioru walidacyjnego: 2.123399\tNajlepsza wartość straty: 1.696655\tDokładność: 22.01%\n",
      "3\tF. straty dla zbioru walidacyjnego: 2.421900\tNajlepsza wartość straty: 1.696655\tDokładność: 22.01%\n",
      "4\tF. straty dla zbioru walidacyjnego: 2.267316\tNajlepsza wartość straty: 1.696655\tDokładność: 22.01%\n",
      "5\tF. straty dla zbioru walidacyjnego: 2.374305\tNajlepsza wartość straty: 1.696655\tDokładność: 18.73%\n",
      "6\tF. straty dla zbioru walidacyjnego: 2.521422\tNajlepsza wartość straty: 1.696655\tDokładność: 18.73%\n",
      "7\tF. straty dla zbioru walidacyjnego: 2.586689\tNajlepsza wartość straty: 1.696655\tDokładność: 19.08%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.833712\tNajlepsza wartość straty: 1.696655\tDokładność: 20.91%\n",
      "9\tF. straty dla zbioru walidacyjnego: 3.433898\tNajlepsza wartość straty: 1.696655\tDokładność: 22.01%\n",
      "10\tF. straty dla zbioru walidacyjnego: 3.025983\tNajlepsza wartość straty: 1.696655\tDokładność: 19.08%\n",
      "11\tF. straty dla zbioru walidacyjnego: 2.974210\tNajlepsza wartość straty: 1.696655\tDokładność: 19.08%\n",
      "12\tF. straty dla zbioru walidacyjnego: 2.647062\tNajlepsza wartość straty: 1.696655\tDokładność: 19.27%\n",
      "13\tF. straty dla zbioru walidacyjnego: 4.668605\tNajlepsza wartość straty: 1.696655\tDokładność: 19.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.663861\tNajlepsza wartość straty: 1.663861\tDokładność: 22.01%\n",
      "15\tF. straty dla zbioru walidacyjnego: 2.915455\tNajlepsza wartość straty: 1.663861\tDokładność: 20.91%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.861268\tNajlepsza wartość straty: 1.663861\tDokładność: 20.91%\n",
      "17\tF. straty dla zbioru walidacyjnego: 2.694765\tNajlepsza wartość straty: 1.663861\tDokładność: 20.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 2.126588\tNajlepsza wartość straty: 1.663861\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 3.068683\tNajlepsza wartość straty: 1.663861\tDokładność: 18.73%\n",
      "20\tF. straty dla zbioru walidacyjnego: 2.758964\tNajlepsza wartość straty: 1.663861\tDokładność: 18.73%\n",
      "21\tF. straty dla zbioru walidacyjnego: 2.177196\tNajlepsza wartość straty: 1.663861\tDokładność: 20.91%\n",
      "22\tF. straty dla zbioru walidacyjnego: 3.347433\tNajlepsza wartość straty: 1.663861\tDokładność: 19.27%\n",
      "23\tF. straty dla zbioru walidacyjnego: 3.344348\tNajlepsza wartość straty: 1.663861\tDokładność: 19.08%\n",
      "24\tF. straty dla zbioru walidacyjnego: 2.567679\tNajlepsza wartość straty: 1.663861\tDokładność: 19.08%\n",
      "25\tF. straty dla zbioru walidacyjnego: 2.473506\tNajlepsza wartość straty: 1.663861\tDokładność: 22.01%\n",
      "26\tF. straty dla zbioru walidacyjnego: 6.275393\tNajlepsza wartość straty: 1.663861\tDokładność: 18.73%\n",
      "27\tF. straty dla zbioru walidacyjnego: 2.501013\tNajlepsza wartość straty: 1.663861\tDokładność: 18.73%\n",
      "28\tF. straty dla zbioru walidacyjnego: 2.875358\tNajlepsza wartość straty: 1.663861\tDokładność: 19.27%\n",
      "29\tF. straty dla zbioru walidacyjnego: 1.914296\tNajlepsza wartość straty: 1.663861\tDokładność: 18.73%\n",
      "30\tF. straty dla zbioru walidacyjnego: 3.092057\tNajlepsza wartość straty: 1.663861\tDokładność: 22.01%\n",
      "31\tF. straty dla zbioru walidacyjnego: 2.832247\tNajlepsza wartość straty: 1.663861\tDokładność: 18.73%\n",
      "32\tF. straty dla zbioru walidacyjnego: 3.034756\tNajlepsza wartość straty: 1.663861\tDokładność: 19.27%\n",
      "33\tF. straty dla zbioru walidacyjnego: 3.690392\tNajlepsza wartość straty: 1.663861\tDokładność: 19.08%\n",
      "34\tF. straty dla zbioru walidacyjnego: 2.846368\tNajlepsza wartość straty: 1.663861\tDokładność: 20.91%\n",
      "35\tF. straty dla zbioru walidacyjnego: 3.884599\tNajlepsza wartość straty: 1.663861\tDokładność: 22.01%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total= 1.4min\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.793579\tNajlepsza wartość straty: 1.793579\tDokładność: 22.01%\n",
      "1\tF. straty dla zbioru walidacyjnego: 2.631318\tNajlepsza wartość straty: 1.793579\tDokładność: 22.01%\n",
      "2\tF. straty dla zbioru walidacyjnego: 2.647305\tNajlepsza wartość straty: 1.793579\tDokładność: 19.08%\n",
      "3\tF. straty dla zbioru walidacyjnego: 3.142425\tNajlepsza wartość straty: 1.793579\tDokładność: 22.01%\n",
      "4\tF. straty dla zbioru walidacyjnego: 2.180204\tNajlepsza wartość straty: 1.793579\tDokładność: 19.08%\n",
      "5\tF. straty dla zbioru walidacyjnego: 2.422339\tNajlepsza wartość straty: 1.793579\tDokładność: 20.91%\n",
      "6\tF. straty dla zbioru walidacyjnego: 2.968391\tNajlepsza wartość straty: 1.793579\tDokładność: 20.91%\n",
      "7\tF. straty dla zbioru walidacyjnego: 2.272239\tNajlepsza wartość straty: 1.793579\tDokładność: 22.01%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.776087\tNajlepsza wartość straty: 1.776087\tDokładność: 18.73%\n",
      "9\tF. straty dla zbioru walidacyjnego: 2.764297\tNajlepsza wartość straty: 1.776087\tDokładność: 22.01%\n",
      "10\tF. straty dla zbioru walidacyjnego: 3.420413\tNajlepsza wartość straty: 1.776087\tDokładność: 19.27%\n",
      "11\tF. straty dla zbioru walidacyjnego: 2.616637\tNajlepsza wartość straty: 1.776087\tDokładność: 18.73%\n",
      "12\tF. straty dla zbioru walidacyjnego: 3.104215\tNajlepsza wartość straty: 1.776087\tDokładność: 22.01%\n",
      "13\tF. straty dla zbioru walidacyjnego: 4.496893\tNajlepsza wartość straty: 1.776087\tDokładność: 19.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 2.561239\tNajlepsza wartość straty: 1.776087\tDokładność: 19.27%\n",
      "15\tF. straty dla zbioru walidacyjnego: 2.251834\tNajlepsza wartość straty: 1.776087\tDokładność: 19.27%\n",
      "16\tF. straty dla zbioru walidacyjnego: 2.265980\tNajlepsza wartość straty: 1.776087\tDokładność: 20.91%\n",
      "17\tF. straty dla zbioru walidacyjnego: 2.629085\tNajlepsza wartość straty: 1.776087\tDokładność: 20.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 2.920799\tNajlepsza wartość straty: 1.776087\tDokładność: 19.27%\n",
      "19\tF. straty dla zbioru walidacyjnego: 2.067248\tNajlepsza wartość straty: 1.776087\tDokładność: 22.01%\n",
      "20\tF. straty dla zbioru walidacyjnego: 2.164310\tNajlepsza wartość straty: 1.776087\tDokładność: 20.91%\n",
      "21\tF. straty dla zbioru walidacyjnego: 3.457067\tNajlepsza wartość straty: 1.776087\tDokładność: 22.01%\n",
      "22\tF. straty dla zbioru walidacyjnego: 2.324863\tNajlepsza wartość straty: 1.776087\tDokładność: 22.01%\n",
      "23\tF. straty dla zbioru walidacyjnego: 3.668049\tNajlepsza wartość straty: 1.776087\tDokładność: 19.08%\n",
      "24\tF. straty dla zbioru walidacyjnego: 2.673012\tNajlepsza wartość straty: 1.776087\tDokładność: 19.08%\n",
      "25\tF. straty dla zbioru walidacyjnego: 2.491214\tNajlepsza wartość straty: 1.776087\tDokładność: 22.01%\n",
      "26\tF. straty dla zbioru walidacyjnego: 2.664450\tNajlepsza wartość straty: 1.776087\tDokładność: 19.27%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.825970\tNajlepsza wartość straty: 1.776087\tDokładność: 20.91%\n",
      "28\tF. straty dla zbioru walidacyjnego: 2.968775\tNajlepsza wartość straty: 1.776087\tDokładność: 19.27%\n",
      "29\tF. straty dla zbioru walidacyjnego: 2.958214\tNajlepsza wartość straty: 1.776087\tDokładność: 20.91%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total= 1.2min\n",
      "[CV] n_neurons=90, learning_rate=0.1, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.613390\tNajlepsza wartość straty: 1.613390\tDokładność: 22.01%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.629813\tNajlepsza wartość straty: 1.613390\tDokładność: 22.01%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.615969\tNajlepsza wartość straty: 1.613390\tDokładność: 22.01%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.616293\tNajlepsza wartość straty: 1.613390\tDokładność: 19.27%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.619931\tNajlepsza wartość straty: 1.613390\tDokładność: 22.01%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.615359\tNajlepsza wartość straty: 1.613390\tDokładność: 19.27%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.645818\tNajlepsza wartość straty: 1.613390\tDokładność: 22.01%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.647322\tNajlepsza wartość straty: 1.613390\tDokładność: 22.01%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.619548\tNajlepsza wartość straty: 1.613390\tDokładność: 19.27%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.675719\tNajlepsza wartość straty: 1.613390\tDokładność: 22.01%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.628715\tNajlepsza wartość straty: 1.613390\tDokładność: 18.73%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.622288\tNajlepsza wartość straty: 1.613390\tDokładność: 19.08%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.630509\tNajlepsza wartość straty: 1.613390\tDokładność: 20.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.609838\tNajlepsza wartość straty: 1.609838\tDokładność: 19.27%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.623431\tNajlepsza wartość straty: 1.609838\tDokładność: 19.08%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.659688\tNajlepsza wartość straty: 1.609838\tDokładność: 22.01%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.659542\tNajlepsza wartość straty: 1.609838\tDokładność: 20.91%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.700314\tNajlepsza wartość straty: 1.609838\tDokładność: 20.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.639765\tNajlepsza wartość straty: 1.609838\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.636775\tNajlepsza wartość straty: 1.609838\tDokładność: 20.91%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.643269\tNajlepsza wartość straty: 1.609838\tDokładność: 19.08%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.666451\tNajlepsza wartość straty: 1.609838\tDokładność: 22.01%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.620915\tNajlepsza wartość straty: 1.609838\tDokładność: 22.01%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.631680\tNajlepsza wartość straty: 1.609838\tDokładność: 22.01%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.658319\tNajlepsza wartość straty: 1.609838\tDokładność: 22.01%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.711110\tNajlepsza wartość straty: 1.609838\tDokładność: 22.01%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.867949\tNajlepsza wartość straty: 1.609838\tDokładność: 19.08%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.806390\tNajlepsza wartość straty: 1.609838\tDokładność: 19.27%\n",
      "28\tF. straty dla zbioru walidacyjnego: 1.914339\tNajlepsza wartość straty: 1.609838\tDokładność: 22.01%\n",
      "29\tF. straty dla zbioru walidacyjnego: 1.623009\tNajlepsza wartość straty: 1.609838\tDokładność: 22.01%\n",
      "30\tF. straty dla zbioru walidacyjnego: 1.784733\tNajlepsza wartość straty: 1.609838\tDokładność: 19.08%\n",
      "31\tF. straty dla zbioru walidacyjnego: 1.760049\tNajlepsza wartość straty: 1.609838\tDokładność: 18.73%\n",
      "32\tF. straty dla zbioru walidacyjnego: 1.821810\tNajlepsza wartość straty: 1.609838\tDokładność: 19.08%\n",
      "33\tF. straty dla zbioru walidacyjnego: 1.894667\tNajlepsza wartość straty: 1.609838\tDokładność: 22.01%\n",
      "34\tF. straty dla zbioru walidacyjnego: 1.615224\tNajlepsza wartość straty: 1.609838\tDokładność: 22.01%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.1, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=  21.0s\n",
      "[CV] n_neurons=90, learning_rate=0.1, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.653298\tNajlepsza wartość straty: 0.653298\tDokładność: 82.21%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.373107\tNajlepsza wartość straty: 0.373107\tDokładność: 89.01%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.337496\tNajlepsza wartość straty: 0.337496\tDokładność: 90.15%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.731426\tNajlepsza wartość straty: 0.337496\tDokładność: 77.25%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.207622\tNajlepsza wartość straty: 0.207622\tDokładność: 95.27%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.180605\tNajlepsza wartość straty: 0.180605\tDokładność: 95.47%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.221899\tNajlepsza wartość straty: 0.180605\tDokładność: 95.78%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.342754\tNajlepsza wartość straty: 0.180605\tDokładność: 95.07%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.626741\tNajlepsza wartość straty: 0.180605\tDokładność: 22.01%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.620573\tNajlepsza wartość straty: 0.180605\tDokładność: 22.01%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.610480\tNajlepsza wartość straty: 0.180605\tDokładność: 22.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.628510\tNajlepsza wartość straty: 0.180605\tDokładność: 19.08%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.637670\tNajlepsza wartość straty: 0.180605\tDokładność: 19.08%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.630513\tNajlepsza wartość straty: 0.180605\tDokładność: 19.27%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.620266\tNajlepsza wartość straty: 0.180605\tDokładność: 22.01%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.614792\tNajlepsza wartość straty: 0.180605\tDokładność: 22.01%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.629104\tNajlepsza wartość straty: 0.180605\tDokładność: 18.73%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.628850\tNajlepsza wartość straty: 0.180605\tDokładność: 19.08%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.703194\tNajlepsza wartość straty: 0.180605\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.650998\tNajlepsza wartość straty: 0.180605\tDokładność: 20.91%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.613480\tNajlepsza wartość straty: 0.180605\tDokładność: 22.01%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.714501\tNajlepsza wartość straty: 0.180605\tDokładność: 22.01%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.642129\tNajlepsza wartość straty: 0.180605\tDokładność: 19.08%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.698435\tNajlepsza wartość straty: 0.180605\tDokładność: 22.01%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.776942\tNajlepsza wartość straty: 0.180605\tDokładność: 22.01%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.694765\tNajlepsza wartość straty: 0.180605\tDokładność: 19.08%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.626983\tNajlepsza wartość straty: 0.180605\tDokładność: 19.27%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.1, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=  16.3s\n",
      "[CV] n_neurons=90, learning_rate=0.1, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.610351\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.622865\tNajlepsza wartość straty: 1.610351\tDokładność: 19.08%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.612650\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.663413\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.619371\tNajlepsza wartość straty: 1.610351\tDokładność: 19.08%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.683074\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.612572\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.657455\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.660426\tNajlepsza wartość straty: 1.610351\tDokładność: 18.73%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.685470\tNajlepsza wartość straty: 1.610351\tDokładność: 19.27%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.633783\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.654966\tNajlepsza wartość straty: 1.610351\tDokładność: 19.27%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.624559\tNajlepsza wartość straty: 1.610351\tDokładność: 20.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.683939\tNajlepsza wartość straty: 1.610351\tDokładność: 19.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.649799\tNajlepsza wartość straty: 1.610351\tDokładność: 18.73%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.661326\tNajlepsza wartość straty: 1.610351\tDokładność: 18.73%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.726479\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.628972\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.641633\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.657076\tNajlepsza wartość straty: 1.610351\tDokładność: 19.27%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.649873\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "21\tF. straty dla zbioru walidacyjnego: 2.166503\tNajlepsza wartość straty: 1.610351\tDokładność: 22.01%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.1, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=  13.8s\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.531188\tNajlepsza wartość straty: 0.531188\tDokładność: 74.43%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.472329\tNajlepsza wartość straty: 0.472329\tDokładność: 86.28%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.337535\tNajlepsza wartość straty: 0.337535\tDokładność: 89.29%\n",
      "3\tF. straty dla zbioru walidacyjnego: 6.962605\tNajlepsza wartość straty: 0.337535\tDokładność: 57.39%\n",
      "4\tF. straty dla zbioru walidacyjnego: 252.665283\tNajlepsza wartość straty: 0.337535\tDokładność: 32.33%\n",
      "5\tF. straty dla zbioru walidacyjnego: 62.504368\tNajlepsza wartość straty: 0.337535\tDokładność: 54.65%\n",
      "6\tF. straty dla zbioru walidacyjnego: 78.805275\tNajlepsza wartość straty: 0.337535\tDokładność: 31.98%\n",
      "7\tF. straty dla zbioru walidacyjnego: 42.913044\tNajlepsza wartość straty: 0.337535\tDokładność: 59.97%\n",
      "8\tF. straty dla zbioru walidacyjnego: 4.657725\tNajlepsza wartość straty: 0.337535\tDokładność: 69.08%\n",
      "9\tF. straty dla zbioru walidacyjnego: 3.930391\tNajlepsza wartość straty: 0.337535\tDokładność: 67.71%\n",
      "10\tF. straty dla zbioru walidacyjnego: 3.528152\tNajlepsza wartość straty: 0.337535\tDokładność: 74.67%\n",
      "11\tF. straty dla zbioru walidacyjnego: 2.352865\tNajlepsza wartość straty: 0.337535\tDokładność: 77.37%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.502918\tNajlepsza wartość straty: 0.337535\tDokładność: 83.82%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.355175\tNajlepsza wartość straty: 0.337535\tDokładność: 81.94%\n",
      "14\tF. straty dla zbioru walidacyjnego: 2.377819\tNajlepsza wartość straty: 0.337535\tDokładność: 77.21%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.920967\tNajlepsza wartość straty: 0.337535\tDokładność: 85.73%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.851882\tNajlepsza wartość straty: 0.337535\tDokładność: 85.14%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.043197\tNajlepsza wartość straty: 0.337535\tDokładność: 81.35%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.878488\tNajlepsza wartość straty: 0.337535\tDokładność: 83.42%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.551096\tNajlepsza wartość straty: 0.337535\tDokładność: 76.97%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.811408\tNajlepsza wartość straty: 0.337535\tDokładność: 84.83%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.648286\tNajlepsza wartość straty: 0.337535\tDokładność: 86.59%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.615781\tNajlepsza wartość straty: 0.337535\tDokładność: 85.81%\n",
      "23\tF. straty dla zbioru walidacyjnego: 2.176587\tNajlepsza wartość straty: 0.337535\tDokładność: 60.75%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=   9.4s\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.417181\tNajlepsza wartość straty: 0.417181\tDokładność: 86.04%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.244960\tNajlepsza wartość straty: 0.244960\tDokładność: 93.55%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.177202\tNajlepsza wartość straty: 0.177202\tDokładność: 94.88%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.136269\tNajlepsza wartość straty: 0.136269\tDokładność: 95.97%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.137971\tNajlepsza wartość straty: 0.136269\tDokładność: 95.93%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.129093\tNajlepsza wartość straty: 0.129093\tDokładność: 96.13%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.130427\tNajlepsza wartość straty: 0.129093\tDokładność: 95.90%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.151624\tNajlepsza wartość straty: 0.129093\tDokładność: 95.90%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.139118\tNajlepsza wartość straty: 0.129093\tDokładność: 95.93%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.110948\tNajlepsza wartość straty: 0.110948\tDokładność: 96.44%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.120890\tNajlepsza wartość straty: 0.110948\tDokładność: 96.91%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.116735\tNajlepsza wartość straty: 0.110948\tDokładność: 96.52%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.133294\tNajlepsza wartość straty: 0.110948\tDokładność: 96.36%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.126263\tNajlepsza wartość straty: 0.110948\tDokładność: 96.13%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.113065\tNajlepsza wartość straty: 0.110948\tDokładność: 96.91%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.138944\tNajlepsza wartość straty: 0.110948\tDokładność: 96.79%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.124277\tNajlepsza wartość straty: 0.110948\tDokładność: 96.36%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.135481\tNajlepsza wartość straty: 0.110948\tDokładność: 96.25%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.104853\tNajlepsza wartość straty: 0.104853\tDokładność: 96.64%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.113207\tNajlepsza wartość straty: 0.104853\tDokładność: 96.83%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.122555\tNajlepsza wartość straty: 0.104853\tDokładność: 96.99%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.109412\tNajlepsza wartość straty: 0.104853\tDokładność: 96.56%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.103096\tNajlepsza wartość straty: 0.103096\tDokładność: 96.91%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.120885\tNajlepsza wartość straty: 0.103096\tDokładność: 96.83%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.104020\tNajlepsza wartość straty: 0.103096\tDokładność: 96.95%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.116017\tNajlepsza wartość straty: 0.103096\tDokładność: 96.60%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.134520\tNajlepsza wartość straty: 0.103096\tDokładność: 96.36%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.124247\tNajlepsza wartość straty: 0.103096\tDokładność: 96.56%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.116691\tNajlepsza wartość straty: 0.103096\tDokładność: 96.79%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.104824\tNajlepsza wartość straty: 0.103096\tDokładność: 96.64%\n",
      "30\tF. straty dla zbioru walidacyjnego: 353.997833\tNajlepsza wartość straty: 0.103096\tDokładność: 17.20%\n",
      "31\tF. straty dla zbioru walidacyjnego: 98176.835938\tNajlepsza wartość straty: 0.103096\tDokładność: 20.91%\n",
      "32\tF. straty dla zbioru walidacyjnego: 6175.351562\tNajlepsza wartość straty: 0.103096\tDokładność: 18.26%\n",
      "33\tF. straty dla zbioru walidacyjnego: 445.187805\tNajlepsza wartość straty: 0.103096\tDokładność: 26.78%\n",
      "34\tF. straty dla zbioru walidacyjnego: 213.763580\tNajlepsza wartość straty: 0.103096\tDokładność: 29.36%\n",
      "35\tF. straty dla zbioru walidacyjnego: 135.304626\tNajlepsza wartość straty: 0.103096\tDokładność: 34.64%\n",
      "36\tF. straty dla zbioru walidacyjnego: 109.004425\tNajlepsza wartość straty: 0.103096\tDokładność: 41.44%\n",
      "37\tF. straty dla zbioru walidacyjnego: 57.065449\tNajlepsza wartość straty: 0.103096\tDokładność: 51.68%\n",
      "38\tF. straty dla zbioru walidacyjnego: 115.991203\tNajlepsza wartość straty: 0.103096\tDokładność: 40.42%\n",
      "39\tF. straty dla zbioru walidacyjnego: 50.006496\tNajlepsza wartość straty: 0.103096\tDokładność: 50.66%\n",
      "40\tF. straty dla zbioru walidacyjnego: 68.972687\tNajlepsza wartość straty: 0.103096\tDokładność: 53.71%\n",
      "41\tF. straty dla zbioru walidacyjnego: 75.305069\tNajlepsza wartość straty: 0.103096\tDokładność: 38.82%\n",
      "42\tF. straty dla zbioru walidacyjnego: 30.548677\tNajlepsza wartość straty: 0.103096\tDokładność: 56.96%\n",
      "43\tF. straty dla zbioru walidacyjnego: 22.504190\tNajlepsza wartość straty: 0.103096\tDokładność: 64.50%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=  14.7s\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.352686\tNajlepsza wartość straty: 0.352686\tDokładność: 89.80%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.205184\tNajlepsza wartość straty: 0.205184\tDokładność: 94.68%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.197973\tNajlepsza wartość straty: 0.197973\tDokładność: 95.31%\n",
      "3\tF. straty dla zbioru walidacyjnego: 14.931022\tNajlepsza wartość straty: 0.197973\tDokładność: 61.26%\n",
      "4\tF. straty dla zbioru walidacyjnego: 2.782717\tNajlepsza wartość straty: 0.197973\tDokładność: 32.13%\n",
      "5\tF. straty dla zbioru walidacyjnego: 2370.571045\tNajlepsza wartość straty: 0.197973\tDokładność: 19.27%\n",
      "6\tF. straty dla zbioru walidacyjnego: 386.063660\tNajlepsza wartość straty: 0.197973\tDokładność: 19.27%\n",
      "7\tF. straty dla zbioru walidacyjnego: 170.423172\tNajlepsza wartość straty: 0.197973\tDokładność: 17.40%\n",
      "8\tF. straty dla zbioru walidacyjnego: 70.671249\tNajlepsza wartość straty: 0.197973\tDokładność: 21.81%\n",
      "9\tF. straty dla zbioru walidacyjnego: 59.427807\tNajlepsza wartość straty: 0.197973\tDokładność: 19.08%\n",
      "10\tF. straty dla zbioru walidacyjnego: 15.789738\tNajlepsza wartość straty: 0.197973\tDokładność: 34.25%\n",
      "11\tF. straty dla zbioru walidacyjnego: 7.596352\tNajlepsza wartość straty: 0.197973\tDokładność: 32.49%\n",
      "12\tF. straty dla zbioru walidacyjnego: 4.809263\tNajlepsza wartość straty: 0.197973\tDokładność: 40.97%\n",
      "13\tF. straty dla zbioru walidacyjnego: 81.503601\tNajlepsza wartość straty: 0.197973\tDokładność: 35.77%\n",
      "14\tF. straty dla zbioru walidacyjnego: 20.359934\tNajlepsza wartość straty: 0.197973\tDokładność: 40.03%\n",
      "15\tF. straty dla zbioru walidacyjnego: 6.119676\tNajlepsza wartość straty: 0.197973\tDokładność: 39.52%\n",
      "16\tF. straty dla zbioru walidacyjnego: 2.568221\tNajlepsza wartość straty: 0.197973\tDokładność: 48.79%\n",
      "17\tF. straty dla zbioru walidacyjnego: 2.684112\tNajlepsza wartość straty: 0.197973\tDokładność: 51.29%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.770383\tNajlepsza wartość straty: 0.197973\tDokładność: 57.39%\n",
      "19\tF. straty dla zbioru walidacyjnego: 4.001692\tNajlepsza wartość straty: 0.197973\tDokładność: 48.36%\n",
      "20\tF. straty dla zbioru walidacyjnego: 2.625732\tNajlepsza wartość straty: 0.197973\tDokładność: 52.42%\n",
      "21\tF. straty dla zbioru walidacyjnego: 3.242819\tNajlepsza wartość straty: 0.197973\tDokładność: 57.04%\n",
      "22\tF. straty dla zbioru walidacyjnego: 4.992995\tNajlepsza wartość straty: 0.197973\tDokładność: 52.07%\n",
      "23\tF. straty dla zbioru walidacyjnego: 2.203268\tNajlepsza wartość straty: 0.197973\tDokładność: 59.85%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0>, total=   8.8s\n",
      "[CV] n_neurons=140, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.239395\tNajlepsza wartość straty: 0.239395\tDokładność: 92.81%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.114068\tNajlepsza wartość straty: 0.114068\tDokładność: 96.36%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.101603\tNajlepsza wartość straty: 0.101603\tDokładność: 96.87%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.082091\tNajlepsza wartość straty: 0.082091\tDokładność: 97.15%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.088103\tNajlepsza wartość straty: 0.082091\tDokładność: 97.26%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.072697\tNajlepsza wartość straty: 0.072697\tDokładność: 97.65%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.071647\tNajlepsza wartość straty: 0.071647\tDokładność: 97.93%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.070782\tNajlepsza wartość straty: 0.070782\tDokładność: 97.81%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.085142\tNajlepsza wartość straty: 0.070782\tDokładność: 98.05%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.067414\tNajlepsza wartość straty: 0.067414\tDokładność: 98.16%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.067348\tNajlepsza wartość straty: 0.067348\tDokładność: 98.28%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.058871\tNajlepsza wartość straty: 0.058871\tDokładność: 98.44%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.066780\tNajlepsza wartość straty: 0.058871\tDokładność: 98.16%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.074179\tNajlepsza wartość straty: 0.058871\tDokładność: 98.36%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.064165\tNajlepsza wartość straty: 0.058871\tDokładność: 98.51%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.073275\tNajlepsza wartość straty: 0.058871\tDokładność: 98.20%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.078089\tNajlepsza wartość straty: 0.058871\tDokładność: 98.24%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.062321\tNajlepsza wartość straty: 0.058871\tDokładność: 98.75%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.092339\tNajlepsza wartość straty: 0.058871\tDokładność: 98.36%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.094809\tNajlepsza wartość straty: 0.058871\tDokładność: 98.36%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.092627\tNajlepsza wartość straty: 0.058871\tDokładność: 98.28%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.068684\tNajlepsza wartość straty: 0.058871\tDokładność: 98.51%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.070838\tNajlepsza wartość straty: 0.058871\tDokładność: 98.83%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.085479\tNajlepsza wartość straty: 0.058871\tDokładność: 98.71%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.085686\tNajlepsza wartość straty: 0.058871\tDokładność: 98.63%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.103558\tNajlepsza wartość straty: 0.058871\tDokładność: 98.40%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.098006\tNajlepsza wartość straty: 0.058871\tDokładność: 98.36%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.103493\tNajlepsza wartość straty: 0.058871\tDokładność: 98.32%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.088476\tNajlepsza wartość straty: 0.058871\tDokładność: 98.40%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.111094\tNajlepsza wartość straty: 0.058871\tDokładność: 98.36%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.107060\tNajlepsza wartość straty: 0.058871\tDokładność: 98.55%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.079433\tNajlepsza wartość straty: 0.058871\tDokładność: 98.67%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.080235\tNajlepsza wartość straty: 0.058871\tDokładność: 98.71%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  20.9s\n",
      "[CV] n_neurons=140, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.197095\tNajlepsza wartość straty: 0.197095\tDokładność: 94.18%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.106369\tNajlepsza wartość straty: 0.106369\tDokładność: 96.40%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.091567\tNajlepsza wartość straty: 0.091567\tDokładność: 97.07%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.074419\tNajlepsza wartość straty: 0.074419\tDokładność: 97.81%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.068379\tNajlepsza wartość straty: 0.068379\tDokładność: 97.93%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.061854\tNajlepsza wartość straty: 0.061854\tDokładność: 98.16%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.055008\tNajlepsza wartość straty: 0.055008\tDokładność: 98.20%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.054343\tNajlepsza wartość straty: 0.054343\tDokładność: 98.40%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.081942\tNajlepsza wartość straty: 0.054343\tDokładność: 97.89%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.049486\tNajlepsza wartość straty: 0.049486\tDokładność: 98.51%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.062519\tNajlepsza wartość straty: 0.049486\tDokładność: 98.44%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.048105\tNajlepsza wartość straty: 0.048105\tDokładność: 98.51%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.055243\tNajlepsza wartość straty: 0.048105\tDokładność: 98.40%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.045036\tNajlepsza wartość straty: 0.045036\tDokładność: 98.71%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.046572\tNajlepsza wartość straty: 0.045036\tDokładność: 98.91%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.062225\tNajlepsza wartość straty: 0.045036\tDokładność: 98.51%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.057976\tNajlepsza wartość straty: 0.045036\tDokładność: 98.59%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.067202\tNajlepsza wartość straty: 0.045036\tDokładność: 98.36%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.062199\tNajlepsza wartość straty: 0.045036\tDokładność: 98.28%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.053873\tNajlepsza wartość straty: 0.045036\tDokładność: 98.98%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.054930\tNajlepsza wartość straty: 0.045036\tDokładność: 98.71%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.072046\tNajlepsza wartość straty: 0.045036\tDokładność: 98.51%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.064340\tNajlepsza wartość straty: 0.045036\tDokładność: 98.75%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.072231\tNajlepsza wartość straty: 0.045036\tDokładność: 98.55%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.074557\tNajlepsza wartość straty: 0.045036\tDokładność: 98.71%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.081295\tNajlepsza wartość straty: 0.045036\tDokładność: 98.32%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.060807\tNajlepsza wartość straty: 0.045036\tDokładność: 98.63%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.069235\tNajlepsza wartość straty: 0.045036\tDokładność: 98.59%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.074125\tNajlepsza wartość straty: 0.045036\tDokładność: 98.48%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.061127\tNajlepsza wartość straty: 0.045036\tDokładność: 98.63%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.064538\tNajlepsza wartość straty: 0.045036\tDokładność: 98.79%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.057730\tNajlepsza wartość straty: 0.045036\tDokładność: 98.71%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.058835\tNajlepsza wartość straty: 0.045036\tDokładność: 98.94%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.066599\tNajlepsza wartość straty: 0.045036\tDokładność: 98.87%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.084574\tNajlepsza wartość straty: 0.045036\tDokładność: 98.55%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  22.1s\n",
      "[CV] n_neurons=140, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.176326\tNajlepsza wartość straty: 0.176326\tDokładność: 94.37%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.106079\tNajlepsza wartość straty: 0.106079\tDokładność: 96.40%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.068325\tNajlepsza wartość straty: 0.068325\tDokładność: 98.01%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.058481\tNajlepsza wartość straty: 0.058481\tDokładność: 98.05%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.053894\tNajlepsza wartość straty: 0.053894\tDokładność: 98.51%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.061963\tNajlepsza wartość straty: 0.053894\tDokładność: 98.24%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.053127\tNajlepsza wartość straty: 0.053127\tDokładność: 98.67%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.044661\tNajlepsza wartość straty: 0.044661\tDokładność: 98.91%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.059098\tNajlepsza wartość straty: 0.044661\tDokładność: 98.44%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.045812\tNajlepsza wartość straty: 0.044661\tDokładność: 98.91%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.055575\tNajlepsza wartość straty: 0.044661\tDokładność: 98.75%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.072596\tNajlepsza wartość straty: 0.044661\tDokładność: 98.05%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.059499\tNajlepsza wartość straty: 0.044661\tDokładność: 98.48%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.065756\tNajlepsza wartość straty: 0.044661\tDokładność: 98.48%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.058327\tNajlepsza wartość straty: 0.044661\tDokładność: 98.83%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.049000\tNajlepsza wartość straty: 0.044661\tDokładność: 98.71%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.055118\tNajlepsza wartość straty: 0.044661\tDokładność: 98.91%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.067828\tNajlepsza wartość straty: 0.044661\tDokładność: 98.67%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.064043\tNajlepsza wartość straty: 0.044661\tDokładność: 98.59%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.070464\tNajlepsza wartość straty: 0.044661\tDokładność: 98.55%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.067148\tNajlepsza wartość straty: 0.044661\tDokładność: 98.55%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.076731\tNajlepsza wartość straty: 0.044661\tDokładność: 98.32%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.061095\tNajlepsza wartość straty: 0.044661\tDokładność: 98.71%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.046849\tNajlepsza wartość straty: 0.044661\tDokładność: 98.91%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.043914\tNajlepsza wartość straty: 0.043914\tDokładność: 98.91%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.056966\tNajlepsza wartość straty: 0.043914\tDokładność: 98.94%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.078432\tNajlepsza wartość straty: 0.043914\tDokładność: 98.87%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.055749\tNajlepsza wartość straty: 0.043914\tDokładność: 98.67%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.068338\tNajlepsza wartość straty: 0.043914\tDokładność: 98.44%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.071804\tNajlepsza wartość straty: 0.043914\tDokładność: 98.79%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.059278\tNajlepsza wartość straty: 0.043914\tDokładność: 98.67%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.065913\tNajlepsza wartość straty: 0.043914\tDokładność: 98.87%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.072590\tNajlepsza wartość straty: 0.043914\tDokładność: 98.59%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.064470\tNajlepsza wartość straty: 0.043914\tDokładność: 98.79%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.066571\tNajlepsza wartość straty: 0.043914\tDokładność: 98.71%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.067991\tNajlepsza wartość straty: 0.043914\tDokładność: 98.75%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.057799\tNajlepsza wartość straty: 0.043914\tDokładność: 99.02%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.064647\tNajlepsza wartość straty: 0.043914\tDokładność: 98.94%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.065677\tNajlepsza wartość straty: 0.043914\tDokładność: 98.91%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.065363\tNajlepsza wartość straty: 0.043914\tDokładność: 98.83%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.063081\tNajlepsza wartość straty: 0.043914\tDokładność: 98.87%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.063625\tNajlepsza wartość straty: 0.043914\tDokładność: 98.87%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.064049\tNajlepsza wartość straty: 0.043914\tDokładność: 98.87%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.064563\tNajlepsza wartość straty: 0.043914\tDokładność: 98.87%\n",
      "44\tF. straty dla zbioru walidacyjnego: 0.065151\tNajlepsza wartość straty: 0.043914\tDokładność: 98.91%\n",
      "45\tF. straty dla zbioru walidacyjnego: 0.065620\tNajlepsza wartość straty: 0.043914\tDokładność: 98.91%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, activation=<function elu at 0x0000029B128289D8>, total=  28.4s\n",
      "[CV] n_neurons=30, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.080899\tNajlepsza wartość straty: 0.080899\tDokładność: 97.62%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.081879\tNajlepsza wartość straty: 0.080899\tDokładność: 97.42%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.082327\tNajlepsza wartość straty: 0.080899\tDokładność: 97.62%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.071056\tNajlepsza wartość straty: 0.071056\tDokładność: 98.01%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.076751\tNajlepsza wartość straty: 0.071056\tDokładność: 97.93%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.066322\tNajlepsza wartość straty: 0.066322\tDokładność: 98.48%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.059979\tNajlepsza wartość straty: 0.059979\tDokładność: 98.44%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.076130\tNajlepsza wartość straty: 0.059979\tDokładność: 98.01%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.074430\tNajlepsza wartość straty: 0.059979\tDokładność: 98.12%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.067761\tNajlepsza wartość straty: 0.059979\tDokładność: 98.12%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.069020\tNajlepsza wartość straty: 0.059979\tDokładność: 98.32%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.087139\tNajlepsza wartość straty: 0.059979\tDokładność: 98.59%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.062663\tNajlepsza wartość straty: 0.059979\tDokładność: 98.36%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.076832\tNajlepsza wartość straty: 0.059979\tDokładność: 98.44%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.070633\tNajlepsza wartość straty: 0.059979\tDokładność: 98.40%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.068416\tNajlepsza wartość straty: 0.059979\tDokładność: 98.63%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.084713\tNajlepsza wartość straty: 0.059979\tDokładność: 98.36%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.097619\tNajlepsza wartość straty: 0.059979\tDokładność: 98.32%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.090083\tNajlepsza wartość straty: 0.059979\tDokładność: 98.32%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.072367\tNajlepsza wartość straty: 0.059979\tDokładność: 98.36%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.112561\tNajlepsza wartość straty: 0.059979\tDokładność: 97.97%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.086277\tNajlepsza wartość straty: 0.059979\tDokładność: 98.36%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.090535\tNajlepsza wartość straty: 0.059979\tDokładność: 98.40%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.092966\tNajlepsza wartość straty: 0.059979\tDokładność: 98.51%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.137959\tNajlepsza wartość straty: 0.059979\tDokładność: 98.44%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.122397\tNajlepsza wartość straty: 0.059979\tDokładność: 97.50%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.078179\tNajlepsza wartość straty: 0.059979\tDokładność: 98.40%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.091669\tNajlepsza wartość straty: 0.059979\tDokładność: 98.32%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=   8.8s\n",
      "[CV] n_neurons=30, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.133944\tNajlepsza wartość straty: 0.133944\tDokładność: 96.29%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.071065\tNajlepsza wartość straty: 0.071065\tDokładność: 97.77%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.077478\tNajlepsza wartość straty: 0.071065\tDokładność: 97.58%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.076450\tNajlepsza wartość straty: 0.071065\tDokładność: 97.73%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.061516\tNajlepsza wartość straty: 0.061516\tDokładność: 98.20%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.056325\tNajlepsza wartość straty: 0.056325\tDokładność: 98.40%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.061890\tNajlepsza wartość straty: 0.056325\tDokładność: 97.93%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.118141\tNajlepsza wartość straty: 0.056325\tDokładność: 97.22%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.089199\tNajlepsza wartość straty: 0.056325\tDokładność: 97.93%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.069719\tNajlepsza wartość straty: 0.056325\tDokładność: 98.08%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.061531\tNajlepsza wartość straty: 0.056325\tDokładność: 98.40%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.085135\tNajlepsza wartość straty: 0.056325\tDokładność: 97.93%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.084788\tNajlepsza wartość straty: 0.056325\tDokładność: 97.50%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.077269\tNajlepsza wartość straty: 0.056325\tDokładność: 97.62%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.096325\tNajlepsza wartość straty: 0.056325\tDokładność: 97.42%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.078117\tNajlepsza wartość straty: 0.056325\tDokładność: 98.12%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.116060\tNajlepsza wartość straty: 0.056325\tDokładność: 97.65%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.078819\tNajlepsza wartość straty: 0.056325\tDokładność: 97.89%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.070354\tNajlepsza wartość straty: 0.056325\tDokładność: 98.16%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.111785\tNajlepsza wartość straty: 0.056325\tDokładność: 97.42%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.073726\tNajlepsza wartość straty: 0.056325\tDokładność: 98.08%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.080655\tNajlepsza wartość straty: 0.056325\tDokładność: 98.63%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.079580\tNajlepsza wartość straty: 0.056325\tDokładność: 98.51%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.061159\tNajlepsza wartość straty: 0.056325\tDokładność: 98.55%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.089409\tNajlepsza wartość straty: 0.056325\tDokładność: 98.44%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.110296\tNajlepsza wartość straty: 0.056325\tDokładność: 98.20%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.118295\tNajlepsza wartość straty: 0.056325\tDokładność: 98.24%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=   8.5s\n",
      "[CV] n_neurons=30, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.083999\tNajlepsza wartość straty: 0.083999\tDokładność: 97.73%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.079724\tNajlepsza wartość straty: 0.079724\tDokładność: 97.77%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.072404\tNajlepsza wartość straty: 0.072404\tDokładność: 98.16%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.081537\tNajlepsza wartość straty: 0.072404\tDokładność: 97.73%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.059428\tNajlepsza wartość straty: 0.059428\tDokładność: 98.40%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.070597\tNajlepsza wartość straty: 0.059428\tDokładność: 97.97%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.070878\tNajlepsza wartość straty: 0.059428\tDokładność: 98.32%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.060568\tNajlepsza wartość straty: 0.059428\tDokładność: 98.24%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.056695\tNajlepsza wartość straty: 0.056695\tDokładność: 98.67%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.091262\tNajlepsza wartość straty: 0.056695\tDokładność: 98.08%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.079710\tNajlepsza wartość straty: 0.056695\tDokładność: 98.12%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.089205\tNajlepsza wartość straty: 0.056695\tDokładność: 97.65%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.078834\tNajlepsza wartość straty: 0.056695\tDokładność: 98.59%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.085639\tNajlepsza wartość straty: 0.056695\tDokładność: 98.44%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.055908\tNajlepsza wartość straty: 0.055908\tDokładność: 98.83%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.066165\tNajlepsza wartość straty: 0.055908\tDokładność: 98.67%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.076572\tNajlepsza wartość straty: 0.055908\tDokładność: 98.40%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.071210\tNajlepsza wartość straty: 0.055908\tDokładność: 98.67%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.067831\tNajlepsza wartość straty: 0.055908\tDokładność: 98.55%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.074001\tNajlepsza wartość straty: 0.055908\tDokładność: 98.44%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.065027\tNajlepsza wartość straty: 0.055908\tDokładność: 98.44%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.074873\tNajlepsza wartość straty: 0.055908\tDokładność: 98.63%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.076369\tNajlepsza wartość straty: 0.055908\tDokładność: 98.79%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.074685\tNajlepsza wartość straty: 0.055908\tDokładność: 98.63%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.068004\tNajlepsza wartość straty: 0.055908\tDokładność: 98.67%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.085745\tNajlepsza wartość straty: 0.055908\tDokładność: 98.48%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.073165\tNajlepsza wartość straty: 0.055908\tDokładność: 98.59%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.073267\tNajlepsza wartość straty: 0.055908\tDokładność: 98.48%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.069135\tNajlepsza wartość straty: 0.055908\tDokładność: 98.98%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.062626\tNajlepsza wartość straty: 0.055908\tDokładność: 98.63%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.066487\tNajlepsza wartość straty: 0.055908\tDokładność: 98.55%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.072857\tNajlepsza wartość straty: 0.055908\tDokładność: 98.63%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.083791\tNajlepsza wartość straty: 0.055908\tDokładność: 98.75%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.081282\tNajlepsza wartość straty: 0.055908\tDokładność: 98.24%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.057014\tNajlepsza wartość straty: 0.055908\tDokładność: 98.94%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.087753\tNajlepsza wartość straty: 0.055908\tDokładność: 98.87%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=  10.9s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.190557\tNajlepsza wartość straty: 0.190557\tDokładność: 94.33%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.103523\tNajlepsza wartość straty: 0.103523\tDokładność: 96.76%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.086624\tNajlepsza wartość straty: 0.086624\tDokładność: 97.07%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.077607\tNajlepsza wartość straty: 0.077607\tDokładność: 97.65%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.082079\tNajlepsza wartość straty: 0.077607\tDokładność: 97.42%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.083545\tNajlepsza wartość straty: 0.077607\tDokładność: 97.54%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.068354\tNajlepsza wartość straty: 0.068354\tDokładność: 97.69%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.094264\tNajlepsza wartość straty: 0.068354\tDokładność: 97.65%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.063316\tNajlepsza wartość straty: 0.063316\tDokładność: 97.89%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.072044\tNajlepsza wartość straty: 0.063316\tDokładność: 98.16%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.084418\tNajlepsza wartość straty: 0.063316\tDokładność: 97.85%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.064840\tNajlepsza wartość straty: 0.063316\tDokładność: 98.28%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.071073\tNajlepsza wartość straty: 0.063316\tDokładność: 97.89%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.096432\tNajlepsza wartość straty: 0.063316\tDokładność: 98.05%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.075315\tNajlepsza wartość straty: 0.063316\tDokładność: 98.28%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.097017\tNajlepsza wartość straty: 0.063316\tDokładność: 97.81%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.086086\tNajlepsza wartość straty: 0.063316\tDokładność: 98.05%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.077768\tNajlepsza wartość straty: 0.063316\tDokładność: 98.08%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.117368\tNajlepsza wartość straty: 0.063316\tDokładność: 97.97%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.064661\tNajlepsza wartość straty: 0.063316\tDokładność: 98.32%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.100890\tNajlepsza wartość straty: 0.063316\tDokładność: 97.93%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.075777\tNajlepsza wartość straty: 0.063316\tDokładność: 98.32%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.075962\tNajlepsza wartość straty: 0.063316\tDokładność: 97.93%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.098416\tNajlepsza wartość straty: 0.063316\tDokładność: 97.97%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.086014\tNajlepsza wartość straty: 0.063316\tDokładność: 98.16%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.089585\tNajlepsza wartość straty: 0.063316\tDokładność: 98.28%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.090668\tNajlepsza wartość straty: 0.063316\tDokładność: 98.32%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.102665\tNajlepsza wartość straty: 0.063316\tDokładność: 98.01%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.148556\tNajlepsza wartość straty: 0.063316\tDokładność: 97.34%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.088985\tNajlepsza wartość straty: 0.063316\tDokładność: 98.05%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  10.3s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.185461\tNajlepsza wartość straty: 0.185461\tDokładność: 94.53%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.095314\tNajlepsza wartość straty: 0.095314\tDokładność: 97.54%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.100743\tNajlepsza wartość straty: 0.095314\tDokładność: 97.15%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.088476\tNajlepsza wartość straty: 0.088476\tDokładność: 97.54%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.082171\tNajlepsza wartość straty: 0.082171\tDokładność: 98.01%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.082559\tNajlepsza wartość straty: 0.082171\tDokładność: 97.77%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.105714\tNajlepsza wartość straty: 0.082171\tDokładność: 97.11%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.070113\tNajlepsza wartość straty: 0.070113\tDokładność: 97.97%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.097103\tNajlepsza wartość straty: 0.070113\tDokładność: 97.26%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.090261\tNajlepsza wartość straty: 0.070113\tDokładność: 97.89%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.075113\tNajlepsza wartość straty: 0.070113\tDokładność: 97.73%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.082157\tNajlepsza wartość straty: 0.070113\tDokładność: 97.93%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.090223\tNajlepsza wartość straty: 0.070113\tDokładność: 98.05%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.071476\tNajlepsza wartość straty: 0.070113\tDokładność: 98.12%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.085178\tNajlepsza wartość straty: 0.070113\tDokładność: 98.16%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.066748\tNajlepsza wartość straty: 0.066748\tDokładność: 98.32%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.099642\tNajlepsza wartość straty: 0.066748\tDokładność: 97.81%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.086891\tNajlepsza wartość straty: 0.066748\tDokładność: 98.05%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.121513\tNajlepsza wartość straty: 0.066748\tDokładność: 97.42%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.085458\tNajlepsza wartość straty: 0.066748\tDokładność: 98.05%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.096431\tNajlepsza wartość straty: 0.066748\tDokładność: 98.16%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.134715\tNajlepsza wartość straty: 0.066748\tDokładność: 97.15%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.096339\tNajlepsza wartość straty: 0.066748\tDokładność: 97.85%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.081181\tNajlepsza wartość straty: 0.066748\tDokładność: 98.28%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.084465\tNajlepsza wartość straty: 0.066748\tDokładność: 98.16%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.073502\tNajlepsza wartość straty: 0.066748\tDokładność: 98.36%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.095363\tNajlepsza wartość straty: 0.066748\tDokładność: 98.16%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.090096\tNajlepsza wartość straty: 0.066748\tDokładność: 98.55%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.119113\tNajlepsza wartość straty: 0.066748\tDokładność: 97.89%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.106815\tNajlepsza wartość straty: 0.066748\tDokładność: 98.51%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.124527\tNajlepsza wartość straty: 0.066748\tDokładność: 98.05%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.085261\tNajlepsza wartość straty: 0.066748\tDokładność: 98.24%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.112267\tNajlepsza wartość straty: 0.066748\tDokładność: 98.20%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.100824\tNajlepsza wartość straty: 0.066748\tDokładność: 98.16%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.163821\tNajlepsza wartość straty: 0.066748\tDokładność: 97.65%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.116763\tNajlepsza wartość straty: 0.066748\tDokładność: 98.24%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.103522\tNajlepsza wartość straty: 0.066748\tDokładność: 98.20%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  12.4s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.329158\tNajlepsza wartość straty: 0.329158\tDokładność: 91.20%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.130137\tNajlepsza wartość straty: 0.130137\tDokładność: 96.29%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.100900\tNajlepsza wartość straty: 0.100900\tDokładność: 96.72%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.099145\tNajlepsza wartość straty: 0.099145\tDokładność: 96.76%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.075413\tNajlepsza wartość straty: 0.075413\tDokładność: 97.65%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.066473\tNajlepsza wartość straty: 0.066473\tDokładność: 97.77%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.077221\tNajlepsza wartość straty: 0.066473\tDokładność: 97.97%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.077926\tNajlepsza wartość straty: 0.066473\tDokładność: 97.77%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.070145\tNajlepsza wartość straty: 0.066473\tDokładność: 98.24%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.057348\tNajlepsza wartość straty: 0.057348\tDokładność: 98.44%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.060379\tNajlepsza wartość straty: 0.057348\tDokładność: 98.36%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.057347\tNajlepsza wartość straty: 0.057347\tDokładność: 98.63%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.076436\tNajlepsza wartość straty: 0.057347\tDokładność: 98.32%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.073165\tNajlepsza wartość straty: 0.057347\tDokładność: 97.89%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.063858\tNajlepsza wartość straty: 0.057347\tDokładność: 98.63%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.071868\tNajlepsza wartość straty: 0.057347\tDokładność: 98.28%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.072516\tNajlepsza wartość straty: 0.057347\tDokładność: 98.28%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.078202\tNajlepsza wartość straty: 0.057347\tDokładność: 98.28%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.073927\tNajlepsza wartość straty: 0.057347\tDokładność: 98.48%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.064325\tNajlepsza wartość straty: 0.057347\tDokładność: 98.71%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.071925\tNajlepsza wartość straty: 0.057347\tDokładność: 98.40%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.080814\tNajlepsza wartość straty: 0.057347\tDokładność: 98.24%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.084403\tNajlepsza wartość straty: 0.057347\tDokładność: 98.20%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.050585\tNajlepsza wartość straty: 0.050585\tDokładność: 98.67%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.079313\tNajlepsza wartość straty: 0.050585\tDokładność: 98.75%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.091623\tNajlepsza wartość straty: 0.050585\tDokładność: 98.24%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.056478\tNajlepsza wartość straty: 0.050585\tDokładność: 98.87%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.077741\tNajlepsza wartość straty: 0.050585\tDokładność: 98.63%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.086408\tNajlepsza wartość straty: 0.050585\tDokładność: 98.67%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.066272\tNajlepsza wartość straty: 0.050585\tDokładność: 98.91%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.088580\tNajlepsza wartość straty: 0.050585\tDokładność: 98.59%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.071754\tNajlepsza wartość straty: 0.050585\tDokładność: 98.36%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.073199\tNajlepsza wartość straty: 0.050585\tDokładność: 98.59%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.067363\tNajlepsza wartość straty: 0.050585\tDokładność: 98.91%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.104046\tNajlepsza wartość straty: 0.050585\tDokładność: 98.55%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.119255\tNajlepsza wartość straty: 0.050585\tDokładność: 97.97%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.101143\tNajlepsza wartość straty: 0.050585\tDokładność: 98.05%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.057161\tNajlepsza wartość straty: 0.050585\tDokładność: 98.59%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.080985\tNajlepsza wartość straty: 0.050585\tDokładność: 98.67%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.070094\tNajlepsza wartość straty: 0.050585\tDokładność: 98.59%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.062876\tNajlepsza wartość straty: 0.050585\tDokładność: 98.79%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.064937\tNajlepsza wartość straty: 0.050585\tDokładność: 98.59%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.071860\tNajlepsza wartość straty: 0.050585\tDokładność: 98.59%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.073201\tNajlepsza wartość straty: 0.050585\tDokładność: 98.83%\n",
      "44\tF. straty dla zbioru walidacyjnego: 0.155687\tNajlepsza wartość straty: 0.050585\tDokładność: 97.85%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  14.8s\n",
      "[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.124040\tNajlepsza wartość straty: 0.124040\tDokładność: 96.29%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.100613\tNajlepsza wartość straty: 0.100613\tDokładność: 97.65%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.084793\tNajlepsza wartość straty: 0.084793\tDokładność: 97.81%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.089322\tNajlepsza wartość straty: 0.084793\tDokładność: 97.73%\n",
      "4\tF. straty dla zbioru walidacyjnego: 62.759548\tNajlepsza wartość straty: 0.084793\tDokładność: 45.27%\n",
      "5\tF. straty dla zbioru walidacyjnego: 3.213766\tNajlepsza wartość straty: 0.084793\tDokładność: 94.80%\n",
      "6\tF. straty dla zbioru walidacyjnego: 2.041977\tNajlepsza wartość straty: 0.084793\tDokładność: 91.44%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.743550\tNajlepsza wartość straty: 0.084793\tDokładność: 95.62%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.099823\tNajlepsza wartość straty: 0.084793\tDokładność: 94.02%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.410001\tNajlepsza wartość straty: 0.084793\tDokładność: 95.66%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.351155\tNajlepsza wartość straty: 0.084793\tDokładność: 96.05%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.237297\tNajlepsza wartość straty: 0.084793\tDokładność: 96.36%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.319192\tNajlepsza wartość straty: 0.084793\tDokładność: 95.74%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.190420\tNajlepsza wartość straty: 0.084793\tDokładność: 96.29%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.217655\tNajlepsza wartość straty: 0.084793\tDokładność: 97.11%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.139083\tNajlepsza wartość straty: 0.084793\tDokładność: 97.38%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.172265\tNajlepsza wartość straty: 0.084793\tDokładność: 96.56%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.155043\tNajlepsza wartość straty: 0.084793\tDokładność: 96.99%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.130122\tNajlepsza wartość straty: 0.084793\tDokładność: 97.50%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.144255\tNajlepsza wartość straty: 0.084793\tDokładność: 97.03%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.117824\tNajlepsza wartość straty: 0.084793\tDokładność: 97.58%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.147707\tNajlepsza wartość straty: 0.084793\tDokładność: 97.50%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.459714\tNajlepsza wartość straty: 0.084793\tDokładność: 95.50%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.135748\tNajlepsza wartość straty: 0.084793\tDokładność: 96.87%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  13.5s\n",
      "[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.106948\tNajlepsza wartość straty: 0.106948\tDokładność: 96.72%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.075767\tNajlepsza wartość straty: 0.075767\tDokładność: 97.97%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.065525\tNajlepsza wartość straty: 0.065525\tDokładność: 98.16%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.083183\tNajlepsza wartość straty: 0.065525\tDokładność: 97.93%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.102783\tNajlepsza wartość straty: 0.065525\tDokładność: 97.77%\n",
      "5\tF. straty dla zbioru walidacyjnego: 2.856409\tNajlepsza wartość straty: 0.065525\tDokładność: 94.61%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.298540\tNajlepsza wartość straty: 0.065525\tDokładność: 95.23%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.369392\tNajlepsza wartość straty: 0.065525\tDokładność: 94.06%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.190505\tNajlepsza wartość straty: 0.065525\tDokładność: 96.36%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.210806\tNajlepsza wartość straty: 0.065525\tDokładność: 96.72%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.139061\tNajlepsza wartość straty: 0.065525\tDokładność: 97.22%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.148204\tNajlepsza wartość straty: 0.065525\tDokładność: 96.91%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.113563\tNajlepsza wartość straty: 0.065525\tDokładność: 97.50%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.230344\tNajlepsza wartość straty: 0.065525\tDokładność: 97.69%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.297927\tNajlepsza wartość straty: 0.065525\tDokładność: 97.42%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.111113\tNajlepsza wartość straty: 0.065525\tDokładność: 97.42%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.109697\tNajlepsza wartość straty: 0.065525\tDokładność: 97.81%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.110296\tNajlepsza wartość straty: 0.065525\tDokładność: 97.34%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.126561\tNajlepsza wartość straty: 0.065525\tDokładność: 97.89%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.170036\tNajlepsza wartość straty: 0.065525\tDokładność: 97.19%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.154560\tNajlepsza wartość straty: 0.065525\tDokładność: 97.65%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.398110\tNajlepsza wartość straty: 0.065525\tDokładność: 97.58%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.762818\tNajlepsza wartość straty: 0.065525\tDokładność: 95.27%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.185959\tNajlepsza wartość straty: 0.065525\tDokładność: 97.42%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  13.5s\n",
      "[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.111403\tNajlepsza wartość straty: 0.111403\tDokładność: 96.99%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.116951\tNajlepsza wartość straty: 0.111403\tDokładność: 96.91%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.108808\tNajlepsza wartość straty: 0.108808\tDokładność: 97.15%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.086976\tNajlepsza wartość straty: 0.086976\tDokładność: 97.93%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.611509\tNajlepsza wartość straty: 0.086976\tDokładność: 91.48%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.698746\tNajlepsza wartość straty: 0.086976\tDokładność: 93.82%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.458561\tNajlepsza wartość straty: 0.086976\tDokładność: 94.76%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.327412\tNajlepsza wartość straty: 0.086976\tDokładność: 95.62%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.268244\tNajlepsza wartość straty: 0.086976\tDokładność: 96.36%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.232033\tNajlepsza wartość straty: 0.086976\tDokładność: 96.72%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.180060\tNajlepsza wartość straty: 0.086976\tDokładność: 96.79%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.275530\tNajlepsza wartość straty: 0.086976\tDokładność: 94.80%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.163108\tNajlepsza wartość straty: 0.086976\tDokładność: 96.68%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.129818\tNajlepsza wartość straty: 0.086976\tDokładność: 97.30%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.141787\tNajlepsza wartość straty: 0.086976\tDokładność: 96.76%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.140970\tNajlepsza wartość straty: 0.086976\tDokładność: 97.38%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.142348\tNajlepsza wartość straty: 0.086976\tDokładność: 96.56%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.120059\tNajlepsza wartość straty: 0.086976\tDokładność: 97.22%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.139976\tNajlepsza wartość straty: 0.086976\tDokładność: 96.52%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.107196\tNajlepsza wartość straty: 0.086976\tDokładność: 97.97%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.145475\tNajlepsza wartość straty: 0.086976\tDokładność: 97.50%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.256773\tNajlepsza wartość straty: 0.086976\tDokładność: 95.54%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.156183\tNajlepsza wartość straty: 0.086976\tDokładność: 97.19%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.107218\tNajlepsza wartość straty: 0.086976\tDokładność: 97.62%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.133363\tNajlepsza wartość straty: 0.086976\tDokładność: 97.50%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  14.0s\n",
      "[CV] n_neurons=90, learning_rate=0.01, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.237459\tNajlepsza wartość straty: 0.237459\tDokładność: 93.59%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.898603\tNajlepsza wartość straty: 0.237459\tDokładność: 94.14%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.792648\tNajlepsza wartość straty: 0.237459\tDokładność: 94.96%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.648746\tNajlepsza wartość straty: 0.237459\tDokładność: 94.18%\n",
      "4\tF. straty dla zbioru walidacyjnego: 46.380909\tNajlepsza wartość straty: 0.237459\tDokładność: 91.87%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.119799\tNajlepsza wartość straty: 0.237459\tDokładność: 95.47%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.937512\tNajlepsza wartość straty: 0.237459\tDokładność: 95.27%\n",
      "7\tF. straty dla zbioru walidacyjnego: 18.597952\tNajlepsza wartość straty: 0.237459\tDokładność: 93.98%\n",
      "8\tF. straty dla zbioru walidacyjnego: 5.723521\tNajlepsza wartość straty: 0.237459\tDokładność: 96.29%\n",
      "9\tF. straty dla zbioru walidacyjnego: 4.215297\tNajlepsza wartość straty: 0.237459\tDokładność: 95.04%\n",
      "10\tF. straty dla zbioru walidacyjnego: 13.529040\tNajlepsza wartość straty: 0.237459\tDokładność: 96.48%\n",
      "11\tF. straty dla zbioru walidacyjnego: 4.359751\tNajlepsza wartość straty: 0.237459\tDokładność: 94.29%\n",
      "12\tF. straty dla zbioru walidacyjnego: 4.948890\tNajlepsza wartość straty: 0.237459\tDokładność: 95.78%\n",
      "13\tF. straty dla zbioru walidacyjnego: 113.799187\tNajlepsza wartość straty: 0.237459\tDokładność: 94.64%\n",
      "14\tF. straty dla zbioru walidacyjnego: 20.241613\tNajlepsza wartość straty: 0.237459\tDokładność: 94.84%\n",
      "15\tF. straty dla zbioru walidacyjnego: 8.151010\tNajlepsza wartość straty: 0.237459\tDokładność: 97.15%\n",
      "16\tF. straty dla zbioru walidacyjnego: 90.246002\tNajlepsza wartość straty: 0.237459\tDokładność: 94.88%\n",
      "17\tF. straty dla zbioru walidacyjnego: 59.416195\tNajlepsza wartość straty: 0.237459\tDokładność: 94.68%\n",
      "18\tF. straty dla zbioru walidacyjnego: 30.589121\tNajlepsza wartość straty: 0.237459\tDokładność: 97.19%\n",
      "19\tF. straty dla zbioru walidacyjnego: 23.772423\tNajlepsza wartość straty: 0.237459\tDokładność: 97.19%\n",
      "20\tF. straty dla zbioru walidacyjnego: 67.536636\tNajlepsza wartość straty: 0.237459\tDokładność: 96.25%\n",
      "21\tF. straty dla zbioru walidacyjnego: 7.317944\tNajlepsza wartość straty: 0.237459\tDokładność: 97.54%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.01, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  56.7s\n",
      "[CV] n_neurons=90, learning_rate=0.01, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.174441\tNajlepsza wartość straty: 0.174441\tDokładność: 96.09%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.566652\tNajlepsza wartość straty: 0.174441\tDokładność: 95.04%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.298970\tNajlepsza wartość straty: 0.174441\tDokładność: 95.50%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.537602\tNajlepsza wartość straty: 0.174441\tDokładność: 96.99%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.358625\tNajlepsza wartość straty: 0.174441\tDokładność: 96.17%\n",
      "5\tF. straty dla zbioru walidacyjnego: 2.036001\tNajlepsza wartość straty: 0.174441\tDokładność: 92.06%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.548477\tNajlepsza wartość straty: 0.174441\tDokładność: 97.07%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.755096\tNajlepsza wartość straty: 0.174441\tDokładność: 95.82%\n",
      "8\tF. straty dla zbioru walidacyjnego: 3.107771\tNajlepsza wartość straty: 0.174441\tDokładność: 96.17%\n",
      "9\tF. straty dla zbioru walidacyjnego: 3.496936\tNajlepsza wartość straty: 0.174441\tDokładność: 96.29%\n",
      "10\tF. straty dla zbioru walidacyjnego: 8.616745\tNajlepsza wartość straty: 0.174441\tDokładność: 96.36%\n",
      "11\tF. straty dla zbioru walidacyjnego: 8.671240\tNajlepsza wartość straty: 0.174441\tDokładność: 95.47%\n",
      "12\tF. straty dla zbioru walidacyjnego: 3.072583\tNajlepsza wartość straty: 0.174441\tDokładność: 96.29%\n",
      "13\tF. straty dla zbioru walidacyjnego: 32.646366\tNajlepsza wartość straty: 0.174441\tDokładność: 95.78%\n",
      "14\tF. straty dla zbioru walidacyjnego: 9.549938\tNajlepsza wartość straty: 0.174441\tDokładność: 96.29%\n",
      "15\tF. straty dla zbioru walidacyjnego: 12.408001\tNajlepsza wartość straty: 0.174441\tDokładność: 96.91%\n",
      "16\tF. straty dla zbioru walidacyjnego: 7.266411\tNajlepsza wartość straty: 0.174441\tDokładność: 95.43%\n",
      "17\tF. straty dla zbioru walidacyjnego: 59.331833\tNajlepsza wartość straty: 0.174441\tDokładność: 93.55%\n",
      "18\tF. straty dla zbioru walidacyjnego: 8.624018\tNajlepsza wartość straty: 0.174441\tDokładność: 96.99%\n",
      "19\tF. straty dla zbioru walidacyjnego: 107.862076\tNajlepsza wartość straty: 0.174441\tDokładność: 94.92%\n",
      "20\tF. straty dla zbioru walidacyjnego: 48.759937\tNajlepsza wartość straty: 0.174441\tDokładność: 94.57%\n",
      "21\tF. straty dla zbioru walidacyjnego: 73.415398\tNajlepsza wartość straty: 0.174441\tDokładność: 96.09%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.01, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  56.7s\n",
      "[CV] n_neurons=90, learning_rate=0.01, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.281816\tNajlepsza wartość straty: 0.281816\tDokładność: 94.33%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.156449\tNajlepsza wartość straty: 0.156449\tDokładność: 96.21%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.835594\tNajlepsza wartość straty: 0.156449\tDokładność: 93.71%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.546408\tNajlepsza wartość straty: 0.156449\tDokładność: 93.98%\n",
      "4\tF. straty dla zbioru walidacyjnego: 6.144213\tNajlepsza wartość straty: 0.156449\tDokładność: 94.72%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.009636\tNajlepsza wartość straty: 0.156449\tDokładność: 95.23%\n",
      "6\tF. straty dla zbioru walidacyjnego: 2.121828\tNajlepsza wartość straty: 0.156449\tDokładność: 94.76%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.618336\tNajlepsza wartość straty: 0.156449\tDokładność: 92.96%\n",
      "8\tF. straty dla zbioru walidacyjnego: 171.794113\tNajlepsza wartość straty: 0.156449\tDokładność: 84.17%\n",
      "9\tF. straty dla zbioru walidacyjnego: 6.227705\tNajlepsza wartość straty: 0.156449\tDokładność: 94.57%\n",
      "10\tF. straty dla zbioru walidacyjnego: 3.899816\tNajlepsza wartość straty: 0.156449\tDokładność: 95.23%\n",
      "11\tF. straty dla zbioru walidacyjnego: 4.842701\tNajlepsza wartość straty: 0.156449\tDokładność: 96.01%\n",
      "12\tF. straty dla zbioru walidacyjnego: 17.961185\tNajlepsza wartość straty: 0.156449\tDokładność: 94.96%\n",
      "13\tF. straty dla zbioru walidacyjnego: 10.968676\tNajlepsza wartość straty: 0.156449\tDokładność: 96.17%\n",
      "14\tF. straty dla zbioru walidacyjnego: 6.137538\tNajlepsza wartość straty: 0.156449\tDokładność: 96.76%\n",
      "15\tF. straty dla zbioru walidacyjnego: 8.422156\tNajlepsza wartość straty: 0.156449\tDokładność: 95.07%\n",
      "16\tF. straty dla zbioru walidacyjnego: 25.601206\tNajlepsza wartość straty: 0.156449\tDokładność: 96.21%\n",
      "17\tF. straty dla zbioru walidacyjnego: 8.593834\tNajlepsza wartość straty: 0.156449\tDokładność: 97.65%\n",
      "18\tF. straty dla zbioru walidacyjnego: 8.123973\tNajlepsza wartość straty: 0.156449\tDokładność: 97.38%\n",
      "19\tF. straty dla zbioru walidacyjnego: 10.238359\tNajlepsza wartość straty: 0.156449\tDokładność: 95.90%\n",
      "20\tF. straty dla zbioru walidacyjnego: 25.810965\tNajlepsza wartość straty: 0.156449\tDokładność: 88.51%\n",
      "21\tF. straty dla zbioru walidacyjnego: 12.564628\tNajlepsza wartość straty: 0.156449\tDokładność: 96.87%\n",
      "22\tF. straty dla zbioru walidacyjnego: 13.500602\tNajlepsza wartość straty: 0.156449\tDokładność: 97.19%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=90, learning_rate=0.01, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  59.5s\n",
      "[CV] n_neurons=160, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 232.666458\tNajlepsza wartość straty: 232.666458\tDokładność: 92.89%\n",
      "1\tF. straty dla zbioru walidacyjnego: 81.927917\tNajlepsza wartość straty: 81.927917\tDokładność: 94.88%\n",
      "2\tF. straty dla zbioru walidacyjnego: 52.104343\tNajlepsza wartość straty: 52.104343\tDokładność: 95.39%\n",
      "3\tF. straty dla zbioru walidacyjnego: 31.356718\tNajlepsza wartość straty: 31.356718\tDokładność: 95.93%\n",
      "4\tF. straty dla zbioru walidacyjnego: 39.658447\tNajlepsza wartość straty: 31.356718\tDokładność: 95.82%\n",
      "5\tF. straty dla zbioru walidacyjnego: 22.839771\tNajlepsza wartość straty: 22.839771\tDokładność: 96.83%\n",
      "6\tF. straty dla zbioru walidacyjnego: 201.397980\tNajlepsza wartość straty: 22.839771\tDokładność: 93.32%\n",
      "7\tF. straty dla zbioru walidacyjnego: 28.380657\tNajlepsza wartość straty: 22.839771\tDokładność: 95.97%\n",
      "8\tF. straty dla zbioru walidacyjnego: 25.409012\tNajlepsza wartość straty: 22.839771\tDokładność: 96.64%\n",
      "9\tF. straty dla zbioru walidacyjnego: 28.664434\tNajlepsza wartość straty: 22.839771\tDokładność: 95.11%\n",
      "10\tF. straty dla zbioru walidacyjnego: 63.238846\tNajlepsza wartość straty: 22.839771\tDokładność: 92.73%\n",
      "11\tF. straty dla zbioru walidacyjnego: 27.404522\tNajlepsza wartość straty: 22.839771\tDokładność: 96.17%\n",
      "12\tF. straty dla zbioru walidacyjnego: 23.256517\tNajlepsza wartość straty: 22.839771\tDokładność: 96.01%\n",
      "13\tF. straty dla zbioru walidacyjnego: 9.556460\tNajlepsza wartość straty: 9.556460\tDokładność: 97.46%\n",
      "14\tF. straty dla zbioru walidacyjnego: 8.471555\tNajlepsza wartość straty: 8.471555\tDokładność: 97.81%\n",
      "15\tF. straty dla zbioru walidacyjnego: 9.082010\tNajlepsza wartość straty: 8.471555\tDokładność: 97.97%\n",
      "16\tF. straty dla zbioru walidacyjnego: 15.228566\tNajlepsza wartość straty: 8.471555\tDokładność: 97.03%\n",
      "17\tF. straty dla zbioru walidacyjnego: 6.536887\tNajlepsza wartość straty: 6.536887\tDokładność: 97.77%\n",
      "18\tF. straty dla zbioru walidacyjnego: 9.936001\tNajlepsza wartość straty: 6.536887\tDokładność: 97.11%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1179332.875000\tNajlepsza wartość straty: 6.536887\tDokładność: 34.87%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1275983.000000\tNajlepsza wartość straty: 6.536887\tDokładność: 79.87%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1025469.750000\tNajlepsza wartość straty: 6.536887\tDokładność: 79.55%\n",
      "22\tF. straty dla zbioru walidacyjnego: 373320.687500\tNajlepsza wartość straty: 6.536887\tDokładność: 90.62%\n",
      "23\tF. straty dla zbioru walidacyjnego: 246481.843750\tNajlepsza wartość straty: 6.536887\tDokładność: 89.68%\n",
      "24\tF. straty dla zbioru walidacyjnego: 166234.078125\tNajlepsza wartość straty: 6.536887\tDokładność: 92.89%\n",
      "25\tF. straty dla zbioru walidacyjnego: 134893.453125\tNajlepsza wartość straty: 6.536887\tDokładność: 94.21%\n",
      "26\tF. straty dla zbioru walidacyjnego: 332296.562500\tNajlepsza wartość straty: 6.536887\tDokładność: 87.18%\n",
      "27\tF. straty dla zbioru walidacyjnego: 195122.609375\tNajlepsza wartość straty: 6.536887\tDokładność: 93.67%\n",
      "28\tF. straty dla zbioru walidacyjnego: 86838.265625\tNajlepsza wartość straty: 6.536887\tDokładność: 95.31%\n",
      "29\tF. straty dla zbioru walidacyjnego: 143522.484375\tNajlepsza wartość straty: 6.536887\tDokładność: 92.65%\n",
      "30\tF. straty dla zbioru walidacyjnego: 201360.656250\tNajlepsza wartość straty: 6.536887\tDokładność: 90.70%\n",
      "31\tF. straty dla zbioru walidacyjnego: 200102.437500\tNajlepsza wartość straty: 6.536887\tDokładność: 89.56%\n",
      "32\tF. straty dla zbioru walidacyjnego: 210676.187500\tNajlepsza wartość straty: 6.536887\tDokładność: 89.87%\n",
      "33\tF. straty dla zbioru walidacyjnego: 144712.234375\tNajlepsza wartość straty: 6.536887\tDokładność: 95.23%\n",
      "34\tF. straty dla zbioru walidacyjnego: 107812.062500\tNajlepsza wartość straty: 6.536887\tDokładność: 94.68%\n",
      "35\tF. straty dla zbioru walidacyjnego: 65547.109375\tNajlepsza wartość straty: 6.536887\tDokładność: 95.93%\n",
      "36\tF. straty dla zbioru walidacyjnego: 117510.695312\tNajlepsza wartość straty: 6.536887\tDokładność: 95.11%\n",
      "37\tF. straty dla zbioru walidacyjnego: 206900.234375\tNajlepsza wartość straty: 6.536887\tDokładność: 88.62%\n",
      "38\tF. straty dla zbioru walidacyjnego: 63192.292969\tNajlepsza wartość straty: 6.536887\tDokładność: 96.17%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=160, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  38.2s\n",
      "[CV] n_neurons=160, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 57.825352\tNajlepsza wartość straty: 57.825352\tDokładność: 89.25%\n",
      "1\tF. straty dla zbioru walidacyjnego: 11.103949\tNajlepsza wartość straty: 11.103949\tDokładność: 94.53%\n",
      "2\tF. straty dla zbioru walidacyjnego: 3.890102\tNajlepsza wartość straty: 3.890102\tDokładność: 96.05%\n",
      "3\tF. straty dla zbioru walidacyjnego: 8.535509\tNajlepsza wartość straty: 3.890102\tDokładność: 94.21%\n",
      "4\tF. straty dla zbioru walidacyjnego: 11.591078\tNajlepsza wartość straty: 3.890102\tDokładność: 96.79%\n",
      "5\tF. straty dla zbioru walidacyjnego: 4.600556\tNajlepsza wartość straty: 3.890102\tDokładność: 96.29%\n",
      "6\tF. straty dla zbioru walidacyjnego: 3.039146\tNajlepsza wartość straty: 3.039146\tDokładność: 96.99%\n",
      "7\tF. straty dla zbioru walidacyjnego: 2.122723\tNajlepsza wartość straty: 2.122723\tDokładność: 97.22%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.863830\tNajlepsza wartość straty: 1.863830\tDokładność: 97.30%\n",
      "9\tF. straty dla zbioru walidacyjnego: 7.831685\tNajlepsza wartość straty: 1.863830\tDokładność: 93.75%\n",
      "10\tF. straty dla zbioru walidacyjnego: 2.483612\tNajlepsza wartość straty: 1.863830\tDokładność: 97.22%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.870010\tNajlepsza wartość straty: 1.863830\tDokładność: 97.65%\n",
      "12\tF. straty dla zbioru walidacyjnego: 2.285368\tNajlepsza wartość straty: 1.863830\tDokładność: 97.46%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.977166\tNajlepsza wartość straty: 1.863830\tDokładność: 94.68%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1836572.000000\tNajlepsza wartość straty: 1.863830\tDokładność: 87.92%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1577934.500000\tNajlepsza wartość straty: 1.863830\tDokładność: 92.30%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1224964.000000\tNajlepsza wartość straty: 1.863830\tDokładność: 89.05%\n",
      "17\tF. straty dla zbioru walidacyjnego: 619483.250000\tNajlepsza wartość straty: 1.863830\tDokładność: 94.76%\n",
      "18\tF. straty dla zbioru walidacyjnego: 468851.375000\tNajlepsza wartość straty: 1.863830\tDokładność: 94.84%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1157726.875000\tNajlepsza wartość straty: 1.863830\tDokładność: 90.58%\n",
      "20\tF. straty dla zbioru walidacyjnego: 448086.468750\tNajlepsza wartość straty: 1.863830\tDokładność: 95.15%\n",
      "21\tF. straty dla zbioru walidacyjnego: 358717.875000\tNajlepsza wartość straty: 1.863830\tDokładność: 95.35%\n",
      "22\tF. straty dla zbioru walidacyjnego: 564595.375000\tNajlepsza wartość straty: 1.863830\tDokładność: 93.28%\n",
      "23\tF. straty dla zbioru walidacyjnego: 275567.875000\tNajlepsza wartość straty: 1.863830\tDokładność: 95.54%\n",
      "24\tF. straty dla zbioru walidacyjnego: 350355.000000\tNajlepsza wartość straty: 1.863830\tDokładność: 93.98%\n",
      "25\tF. straty dla zbioru walidacyjnego: 223968.531250\tNajlepsza wartość straty: 1.863830\tDokładność: 96.40%\n",
      "26\tF. straty dla zbioru walidacyjnego: 202556.593750\tNajlepsza wartość straty: 1.863830\tDokładność: 95.54%\n",
      "27\tF. straty dla zbioru walidacyjnego: 237126.984375\tNajlepsza wartość straty: 1.863830\tDokładność: 95.19%\n",
      "28\tF. straty dla zbioru walidacyjnego: 340822.375000\tNajlepsza wartość straty: 1.863830\tDokładność: 95.23%\n",
      "29\tF. straty dla zbioru walidacyjnego: 198308.671875\tNajlepsza wartość straty: 1.863830\tDokładność: 96.44%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=160, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  29.2s\n",
      "[CV] n_neurons=160, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 9328.256836\tNajlepsza wartość straty: 9328.256836\tDokładność: 89.99%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1389.353271\tNajlepsza wartość straty: 1389.353271\tDokładność: 92.96%\n",
      "2\tF. straty dla zbioru walidacyjnego: 931.191162\tNajlepsza wartość straty: 931.191162\tDokładność: 94.61%\n",
      "3\tF. straty dla zbioru walidacyjnego: 520.614563\tNajlepsza wartość straty: 520.614563\tDokładność: 94.06%\n",
      "4\tF. straty dla zbioru walidacyjnego: 287.988770\tNajlepsza wartość straty: 287.988770\tDokładność: 95.23%\n",
      "5\tF. straty dla zbioru walidacyjnego: 253.370071\tNajlepsza wartość straty: 253.370071\tDokładność: 95.82%\n",
      "6\tF. straty dla zbioru walidacyjnego: 291.571533\tNajlepsza wartość straty: 253.370071\tDokładność: 94.72%\n",
      "7\tF. straty dla zbioru walidacyjnego: 443.275299\tNajlepsza wartość straty: 253.370071\tDokładność: 93.71%\n",
      "8\tF. straty dla zbioru walidacyjnego: 200.568451\tNajlepsza wartość straty: 200.568451\tDokładność: 95.58%\n",
      "9\tF. straty dla zbioru walidacyjnego: 147.833771\tNajlepsza wartość straty: 147.833771\tDokładność: 96.99%\n",
      "10\tF. straty dla zbioru walidacyjnego: 227.188065\tNajlepsza wartość straty: 147.833771\tDokładność: 95.04%\n",
      "11\tF. straty dla zbioru walidacyjnego: 130.011887\tNajlepsza wartość straty: 130.011887\tDokładność: 97.42%\n",
      "12\tF. straty dla zbioru walidacyjnego: 88.511673\tNajlepsza wartość straty: 88.511673\tDokładność: 97.11%\n",
      "13\tF. straty dla zbioru walidacyjnego: 801.673401\tNajlepsza wartość straty: 88.511673\tDokładność: 94.96%\n",
      "14\tF. straty dla zbioru walidacyjnego: 223.867325\tNajlepsza wartość straty: 88.511673\tDokładność: 97.07%\n",
      "15\tF. straty dla zbioru walidacyjnego: 258.833710\tNajlepsza wartość straty: 88.511673\tDokładność: 97.93%\n",
      "16\tF. straty dla zbioru walidacyjnego: 342.014771\tNajlepsza wartość straty: 88.511673\tDokładność: 96.05%\n",
      "17\tF. straty dla zbioru walidacyjnego: 4885.333008\tNajlepsza wartość straty: 88.511673\tDokładność: 88.08%\n",
      "18\tF. straty dla zbioru walidacyjnego: 5047443.500000\tNajlepsza wartość straty: 88.511673\tDokładność: 74.86%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1028123.312500\tNajlepsza wartość straty: 88.511673\tDokładność: 86.12%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1426451.125000\tNajlepsza wartość straty: 88.511673\tDokładność: 85.38%\n",
      "21\tF. straty dla zbioru walidacyjnego: 841628.125000\tNajlepsza wartość straty: 88.511673\tDokładność: 91.63%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1029126.875000\tNajlepsza wartość straty: 88.511673\tDokładność: 93.08%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1451040.000000\tNajlepsza wartość straty: 88.511673\tDokładność: 89.80%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1576002.375000\tNajlepsza wartość straty: 88.511673\tDokładność: 92.69%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1533420.625000\tNajlepsza wartość straty: 88.511673\tDokładność: 90.46%\n",
      "26\tF. straty dla zbioru walidacyjnego: 699152.875000\tNajlepsza wartość straty: 88.511673\tDokładność: 94.10%\n",
      "27\tF. straty dla zbioru walidacyjnego: 434189.218750\tNajlepsza wartość straty: 88.511673\tDokładność: 94.45%\n",
      "28\tF. straty dla zbioru walidacyjnego: 344010.593750\tNajlepsza wartość straty: 88.511673\tDokładność: 95.47%\n",
      "29\tF. straty dla zbioru walidacyjnego: 640066.750000\tNajlepsza wartość straty: 88.511673\tDokładność: 94.18%\n",
      "30\tF. straty dla zbioru walidacyjnego: 326328.343750\tNajlepsza wartość straty: 88.511673\tDokładność: 96.52%\n",
      "31\tF. straty dla zbioru walidacyjnego: 397646.218750\tNajlepsza wartość straty: 88.511673\tDokładność: 94.68%\n",
      "32\tF. straty dla zbioru walidacyjnego: 261348.531250\tNajlepsza wartość straty: 88.511673\tDokładność: 96.40%\n",
      "33\tF. straty dla zbioru walidacyjnego: 601613.125000\tNajlepsza wartość straty: 88.511673\tDokładność: 93.28%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=160, learning_rate=0.1, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  32.8s\n",
      "[CV] n_neurons=50, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.112955\tNajlepsza wartość straty: 0.112955\tDokładność: 96.60%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.236034\tNajlepsza wartość straty: 0.112955\tDokładność: 95.39%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.070568\tNajlepsza wartość straty: 0.070568\tDokładność: 98.12%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.074937\tNajlepsza wartość straty: 0.070568\tDokładność: 97.89%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.067180\tNajlepsza wartość straty: 0.067180\tDokładność: 98.16%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.080829\tNajlepsza wartość straty: 0.067180\tDokładność: 97.97%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.057505\tNajlepsza wartość straty: 0.057505\tDokładność: 98.51%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.659175\tNajlepsza wartość straty: 0.057505\tDokładność: 77.56%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.164080\tNajlepsza wartość straty: 0.057505\tDokładność: 96.91%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.121735\tNajlepsza wartość straty: 0.057505\tDokładność: 97.50%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.121198\tNajlepsza wartość straty: 0.057505\tDokładność: 97.77%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.097516\tNajlepsza wartość straty: 0.057505\tDokładność: 97.77%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.120414\tNajlepsza wartość straty: 0.057505\tDokładność: 97.81%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.092679\tNajlepsza wartość straty: 0.057505\tDokładność: 97.85%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.098198\tNajlepsza wartość straty: 0.057505\tDokładność: 97.85%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.071263\tNajlepsza wartość straty: 0.057505\tDokładność: 98.16%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.106703\tNajlepsza wartość straty: 0.057505\tDokładność: 97.69%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.163186\tNajlepsza wartość straty: 0.057505\tDokładność: 97.07%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.661205\tNajlepsza wartość straty: 0.057505\tDokładność: 23.69%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.633599\tNajlepsza wartość straty: 0.057505\tDokładność: 20.91%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.645523\tNajlepsza wartość straty: 0.057505\tDokładność: 20.91%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.638539\tNajlepsza wartość straty: 0.057505\tDokładność: 19.27%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.628914\tNajlepsza wartość straty: 0.057505\tDokładność: 19.27%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.616357\tNajlepsza wartość straty: 0.057505\tDokładność: 19.08%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.625740\tNajlepsza wartość straty: 0.057505\tDokładność: 22.01%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.648354\tNajlepsza wartość straty: 0.057505\tDokładność: 19.08%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.613783\tNajlepsza wartość straty: 0.057505\tDokładność: 19.08%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.625776\tNajlepsza wartość straty: 0.057505\tDokładność: 19.27%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=  11.6s\n",
      "[CV] n_neurons=50, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.088141\tNajlepsza wartość straty: 0.088141\tDokładność: 96.68%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.081911\tNajlepsza wartość straty: 0.081911\tDokładność: 97.65%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.094709\tNajlepsza wartość straty: 0.081911\tDokładność: 97.19%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.070731\tNajlepsza wartość straty: 0.070731\tDokładność: 98.28%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.078472\tNajlepsza wartość straty: 0.070731\tDokładność: 97.69%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.071985\tNajlepsza wartość straty: 0.070731\tDokładność: 98.01%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.106582\tNajlepsza wartość straty: 0.070731\tDokładność: 97.62%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.084907\tNajlepsza wartość straty: 0.070731\tDokładność: 98.08%\n",
      "8\tF. straty dla zbioru walidacyjnego: 5.625145\tNajlepsza wartość straty: 0.070731\tDokładność: 86.24%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.292483\tNajlepsza wartość straty: 0.070731\tDokładność: 93.28%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.334779\tNajlepsza wartość straty: 0.070731\tDokładność: 95.43%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.216723\tNajlepsza wartość straty: 0.070731\tDokładność: 96.56%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.348846\tNajlepsza wartość straty: 0.070731\tDokładność: 96.87%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.191149\tNajlepsza wartość straty: 0.070731\tDokładność: 97.11%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.202977\tNajlepsza wartość straty: 0.070731\tDokładność: 96.99%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.162288\tNajlepsza wartość straty: 0.070731\tDokładność: 95.86%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.213275\tNajlepsza wartość straty: 0.070731\tDokładność: 97.26%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.186262\tNajlepsza wartość straty: 0.070731\tDokładność: 97.30%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.155694\tNajlepsza wartość straty: 0.070731\tDokładność: 97.34%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.282855\tNajlepsza wartość straty: 0.070731\tDokładność: 97.03%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.110254\tNajlepsza wartość straty: 0.070731\tDokładność: 97.85%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.154783\tNajlepsza wartość straty: 0.070731\tDokładność: 97.69%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.106644\tNajlepsza wartość straty: 0.070731\tDokładność: 97.30%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.262272\tNajlepsza wartość straty: 0.070731\tDokładność: 97.77%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.090211\tNajlepsza wartość straty: 0.070731\tDokładność: 97.93%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=  10.5s\n",
      "[CV] n_neurons=50, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.088614\tNajlepsza wartość straty: 0.088614\tDokładność: 97.30%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.120830\tNajlepsza wartość straty: 0.088614\tDokładność: 96.87%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.076235\tNajlepsza wartość straty: 0.076235\tDokładność: 97.93%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.062614\tNajlepsza wartość straty: 0.062614\tDokładność: 97.97%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.112637\tNajlepsza wartość straty: 0.062614\tDokładność: 97.62%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.977320\tNajlepsza wartość straty: 0.062614\tDokładność: 70.88%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.180664\tNajlepsza wartość straty: 0.062614\tDokładność: 39.25%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.189767\tNajlepsza wartość straty: 0.062614\tDokładność: 42.03%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.189835\tNajlepsza wartość straty: 0.062614\tDokładność: 41.95%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.198637\tNajlepsza wartość straty: 0.062614\tDokładność: 41.95%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.152138\tNajlepsza wartość straty: 0.062614\tDokładność: 42.57%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.142099\tNajlepsza wartość straty: 0.062614\tDokładność: 39.72%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.156034\tNajlepsza wartość straty: 0.062614\tDokładność: 39.41%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.832723\tNajlepsza wartość straty: 0.062614\tDokładność: 60.99%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.162361\tNajlepsza wartość straty: 0.062614\tDokładność: 96.56%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.128024\tNajlepsza wartość straty: 0.062614\tDokładność: 97.46%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.125085\tNajlepsza wartość straty: 0.062614\tDokładność: 97.42%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.252384\tNajlepsza wartość straty: 0.062614\tDokładność: 97.50%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.498697\tNajlepsza wartość straty: 0.062614\tDokładność: 88.55%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.142114\tNajlepsza wartość straty: 0.062614\tDokładność: 97.73%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.097949\tNajlepsza wartość straty: 0.062614\tDokładność: 98.05%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.107385\tNajlepsza wartość straty: 0.062614\tDokładność: 97.89%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.099797\tNajlepsza wartość straty: 0.062614\tDokładność: 98.24%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.113631\tNajlepsza wartość straty: 0.062614\tDokładność: 97.65%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.098021\tNajlepsza wartość straty: 0.062614\tDokładność: 98.24%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=50, learning_rate=0.02, batch_size=100, activation=<function elu at 0x0000029B128289D8>, total=  10.7s\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.142274\tNajlepsza wartość straty: 0.142274\tDokładność: 96.36%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.133510\tNajlepsza wartość straty: 0.133510\tDokładność: 96.91%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.122996\tNajlepsza wartość straty: 0.122996\tDokładność: 96.48%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.117298\tNajlepsza wartość straty: 0.117298\tDokładność: 97.07%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.116032\tNajlepsza wartość straty: 0.116032\tDokładność: 97.30%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.117270\tNajlepsza wartość straty: 0.116032\tDokładność: 96.79%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.117821\tNajlepsza wartość straty: 0.116032\tDokładność: 96.91%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.115785\tNajlepsza wartość straty: 0.115785\tDokładność: 97.15%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.106973\tNajlepsza wartość straty: 0.106973\tDokładność: 97.34%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.128108\tNajlepsza wartość straty: 0.106973\tDokładność: 96.72%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.134549\tNajlepsza wartość straty: 0.106973\tDokładność: 95.90%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.115317\tNajlepsza wartość straty: 0.106973\tDokładność: 97.34%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.119406\tNajlepsza wartość straty: 0.106973\tDokładność: 97.07%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.130994\tNajlepsza wartość straty: 0.106973\tDokładność: 96.83%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.116347\tNajlepsza wartość straty: 0.106973\tDokładność: 97.22%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.118916\tNajlepsza wartość straty: 0.106973\tDokładność: 97.07%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.133563\tNajlepsza wartość straty: 0.106973\tDokładność: 96.25%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.138334\tNajlepsza wartość straty: 0.106973\tDokładność: 96.99%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.153166\tNajlepsza wartość straty: 0.106973\tDokładność: 96.56%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.127485\tNajlepsza wartość straty: 0.106973\tDokładność: 97.19%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.128048\tNajlepsza wartość straty: 0.106973\tDokładność: 96.99%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.139403\tNajlepsza wartość straty: 0.106973\tDokładność: 96.76%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.131706\tNajlepsza wartość straty: 0.106973\tDokładność: 97.03%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.134932\tNajlepsza wartość straty: 0.106973\tDokładność: 97.42%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.122625\tNajlepsza wartość straty: 0.106973\tDokładność: 97.11%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.141914\tNajlepsza wartość straty: 0.106973\tDokładność: 96.56%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.149653\tNajlepsza wartość straty: 0.106973\tDokładność: 97.26%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.166638\tNajlepsza wartość straty: 0.106973\tDokładność: 97.03%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.124902\tNajlepsza wartość straty: 0.106973\tDokładność: 97.22%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.151435\tNajlepsza wartość straty: 0.106973\tDokładność: 97.11%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=   6.8s\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.283628\tNajlepsza wartość straty: 0.283628\tDokładność: 90.97%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.223862\tNajlepsza wartość straty: 0.223862\tDokładność: 92.06%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.235979\tNajlepsza wartość straty: 0.223862\tDokładność: 91.79%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.225741\tNajlepsza wartość straty: 0.223862\tDokładność: 92.34%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.198069\tNajlepsza wartość straty: 0.198069\tDokładność: 94.29%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.172973\tNajlepsza wartość straty: 0.172973\tDokładność: 94.57%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.137036\tNajlepsza wartość straty: 0.137036\tDokładność: 96.01%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.144611\tNajlepsza wartość straty: 0.137036\tDokładność: 96.01%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.124917\tNajlepsza wartość straty: 0.124917\tDokładność: 96.48%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.119010\tNajlepsza wartość straty: 0.119010\tDokładność: 96.60%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.125340\tNajlepsza wartość straty: 0.119010\tDokładność: 96.33%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.111674\tNajlepsza wartość straty: 0.111674\tDokładność: 96.52%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.120103\tNajlepsza wartość straty: 0.111674\tDokładność: 96.44%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.116540\tNajlepsza wartość straty: 0.111674\tDokładność: 96.68%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.131577\tNajlepsza wartość straty: 0.111674\tDokładność: 96.05%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.114561\tNajlepsza wartość straty: 0.111674\tDokładność: 96.44%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.122875\tNajlepsza wartość straty: 0.111674\tDokładność: 96.68%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.126301\tNajlepsza wartość straty: 0.111674\tDokładność: 95.90%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.121655\tNajlepsza wartość straty: 0.111674\tDokładność: 96.40%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.126490\tNajlepsza wartość straty: 0.111674\tDokładność: 96.68%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.123501\tNajlepsza wartość straty: 0.111674\tDokładność: 96.44%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.142561\tNajlepsza wartość straty: 0.111674\tDokładność: 96.25%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.121936\tNajlepsza wartość straty: 0.111674\tDokładność: 96.56%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.124475\tNajlepsza wartość straty: 0.111674\tDokładność: 96.48%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.136242\tNajlepsza wartość straty: 0.111674\tDokładność: 96.25%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.113483\tNajlepsza wartość straty: 0.111674\tDokładność: 96.52%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.126685\tNajlepsza wartość straty: 0.111674\tDokładność: 96.29%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.135723\tNajlepsza wartość straty: 0.111674\tDokładność: 96.17%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.126566\tNajlepsza wartość straty: 0.111674\tDokładność: 96.25%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.133099\tNajlepsza wartość straty: 0.111674\tDokładność: 96.44%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.127501\tNajlepsza wartość straty: 0.111674\tDokładność: 96.40%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.128417\tNajlepsza wartość straty: 0.111674\tDokładność: 96.17%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.140294\tNajlepsza wartość straty: 0.111674\tDokładność: 96.99%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=   7.6s\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.142010\tNajlepsza wartość straty: 0.142010\tDokładność: 95.70%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.134442\tNajlepsza wartość straty: 0.134442\tDokładność: 95.97%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.151768\tNajlepsza wartość straty: 0.134442\tDokładność: 95.50%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.124456\tNajlepsza wartość straty: 0.124456\tDokładność: 96.33%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.119641\tNajlepsza wartość straty: 0.119641\tDokładność: 96.83%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.130711\tNajlepsza wartość straty: 0.119641\tDokładność: 96.40%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.112593\tNajlepsza wartość straty: 0.112593\tDokładność: 96.72%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.129857\tNajlepsza wartość straty: 0.112593\tDokładność: 96.79%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.141163\tNajlepsza wartość straty: 0.112593\tDokładność: 96.36%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.112534\tNajlepsza wartość straty: 0.112534\tDokładność: 96.60%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.113710\tNajlepsza wartość straty: 0.112534\tDokładność: 96.40%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.125712\tNajlepsza wartość straty: 0.112534\tDokładność: 96.52%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.110109\tNajlepsza wartość straty: 0.110109\tDokładność: 96.87%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.124391\tNajlepsza wartość straty: 0.110109\tDokładność: 96.68%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.121947\tNajlepsza wartość straty: 0.110109\tDokładność: 96.83%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.124316\tNajlepsza wartość straty: 0.110109\tDokładność: 96.83%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.129616\tNajlepsza wartość straty: 0.110109\tDokładność: 96.36%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.108232\tNajlepsza wartość straty: 0.108232\tDokładność: 97.07%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.108273\tNajlepsza wartość straty: 0.108232\tDokładność: 97.03%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.126465\tNajlepsza wartość straty: 0.108232\tDokładność: 97.26%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.117089\tNajlepsza wartość straty: 0.108232\tDokładność: 97.30%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.126979\tNajlepsza wartość straty: 0.108232\tDokładność: 96.68%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.125558\tNajlepsza wartość straty: 0.108232\tDokładność: 97.03%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.124630\tNajlepsza wartość straty: 0.108232\tDokładność: 96.95%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.115338\tNajlepsza wartość straty: 0.108232\tDokładność: 97.69%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.140903\tNajlepsza wartość straty: 0.108232\tDokładność: 96.83%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.148140\tNajlepsza wartość straty: 0.108232\tDokładność: 96.52%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.114315\tNajlepsza wartość straty: 0.108232\tDokładność: 97.03%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.114067\tNajlepsza wartość straty: 0.108232\tDokładność: 97.22%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.138574\tNajlepsza wartość straty: 0.108232\tDokładność: 97.19%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.118866\tNajlepsza wartość straty: 0.108232\tDokładność: 96.99%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.131335\tNajlepsza wartość straty: 0.108232\tDokładność: 96.91%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.141866\tNajlepsza wartość straty: 0.108232\tDokładność: 96.64%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.135479\tNajlepsza wartość straty: 0.108232\tDokładność: 96.76%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.125793\tNajlepsza wartość straty: 0.108232\tDokładność: 96.87%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.142817\tNajlepsza wartość straty: 0.108232\tDokładność: 96.79%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.113437\tNajlepsza wartość straty: 0.108232\tDokładność: 97.26%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.142921\tNajlepsza wartość straty: 0.108232\tDokładność: 96.87%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.114737\tNajlepsza wartość straty: 0.108232\tDokładność: 97.19%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=   8.5s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=10, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.226811\tNajlepsza wartość straty: 0.226811\tDokładność: 93.47%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.173675\tNajlepsza wartość straty: 0.173675\tDokładność: 94.37%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.141937\tNajlepsza wartość straty: 0.141937\tDokładność: 96.17%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.260986\tNajlepsza wartość straty: 0.141937\tDokładność: 93.47%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.401998\tNajlepsza wartość straty: 0.141937\tDokładność: 88.43%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.302731\tNajlepsza wartość straty: 0.141937\tDokładność: 93.47%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.203495\tNajlepsza wartość straty: 0.141937\tDokładność: 95.43%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.282680\tNajlepsza wartość straty: 0.141937\tDokładność: 93.04%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.254946\tNajlepsza wartość straty: 0.141937\tDokładność: 95.39%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.337815\tNajlepsza wartość straty: 0.141937\tDokładność: 94.88%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.596439\tNajlepsza wartość straty: 0.141937\tDokładność: 87.41%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.610731\tNajlepsza wartość straty: 0.141937\tDokładność: 82.96%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.554833\tNajlepsza wartość straty: 0.141937\tDokładność: 81.51%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.552374\tNajlepsza wartość straty: 0.141937\tDokładność: 84.64%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.512260\tNajlepsza wartość straty: 0.141937\tDokładność: 84.87%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.516988\tNajlepsza wartość straty: 0.141937\tDokładność: 85.57%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.459229\tNajlepsza wartość straty: 0.141937\tDokładność: 87.18%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.476922\tNajlepsza wartość straty: 0.141937\tDokładność: 87.65%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.624044\tNajlepsza wartość straty: 0.141937\tDokładność: 82.92%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.431548\tNajlepsza wartość straty: 0.141937\tDokładność: 91.24%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.406700\tNajlepsza wartość straty: 0.141937\tDokładność: 92.06%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.392749\tNajlepsza wartość straty: 0.141937\tDokładność: 92.73%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.630513\tNajlepsza wartość straty: 0.141937\tDokładność: 89.52%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.533368\tNajlepsza wartość straty: 0.141937\tDokładność: 86.12%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=10, activation=<function relu at 0x0000029B12839B70>, total=  31.9s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=10, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.216327\tNajlepsza wartość straty: 0.216327\tDokładność: 94.33%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.144497\tNajlepsza wartość straty: 0.144497\tDokładność: 96.40%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.481415\tNajlepsza wartość straty: 0.144497\tDokładność: 86.98%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.210280\tNajlepsza wartość straty: 0.144497\tDokładność: 94.61%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.262645\tNajlepsza wartość straty: 0.144497\tDokładność: 92.73%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.745544\tNajlepsza wartość straty: 0.144497\tDokładność: 88.74%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.629815\tNajlepsza wartość straty: 0.144497\tDokładność: 76.66%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.427794\tNajlepsza wartość straty: 0.144497\tDokładność: 83.35%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.435731\tNajlepsza wartość straty: 0.144497\tDokładność: 86.28%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.648155\tNajlepsza wartość straty: 0.144497\tDokładność: 71.89%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.929194\tNajlepsza wartość straty: 0.144497\tDokładność: 60.87%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.837225\tNajlepsza wartość straty: 0.144497\tDokładność: 59.46%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.861165\tNajlepsza wartość straty: 0.144497\tDokładność: 55.32%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.894374\tNajlepsza wartość straty: 0.144497\tDokładność: 59.34%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.029724\tNajlepsza wartość straty: 0.144497\tDokładność: 53.95%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.935296\tNajlepsza wartość straty: 0.144497\tDokładność: 56.88%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.122424\tNajlepsza wartość straty: 0.144497\tDokładność: 48.40%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.049631\tNajlepsza wartość straty: 0.144497\tDokładność: 48.79%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.029071\tNajlepsza wartość straty: 0.144497\tDokładność: 54.22%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.954643\tNajlepsza wartość straty: 0.144497\tDokładność: 56.76%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.960431\tNajlepsza wartość straty: 0.144497\tDokładność: 53.48%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.961963\tNajlepsza wartość straty: 0.144497\tDokładność: 56.76%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.966618\tNajlepsza wartość straty: 0.144497\tDokładność: 53.48%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=10, activation=<function relu at 0x0000029B12839B70>, total=  30.9s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=10, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.292272\tNajlepsza wartość straty: 0.292272\tDokładność: 96.52%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.183075\tNajlepsza wartość straty: 0.183075\tDokładność: 95.70%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.280473\tNajlepsza wartość straty: 0.183075\tDokładność: 93.94%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.204894\tNajlepsza wartość straty: 0.183075\tDokładność: 94.92%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.638365\tNajlepsza wartość straty: 0.183075\tDokładność: 78.66%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.463306\tNajlepsza wartość straty: 0.183075\tDokładność: 77.44%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.739119\tNajlepsza wartość straty: 0.183075\tDokładność: 67.47%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.836412\tNajlepsza wartość straty: 0.183075\tDokładność: 60.24%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.971388\tNajlepsza wartość straty: 0.183075\tDokładność: 52.42%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.706860\tNajlepsza wartość straty: 0.183075\tDokładność: 60.79%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.815490\tNajlepsza wartość straty: 0.183075\tDokładność: 59.23%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.792384\tNajlepsza wartość straty: 0.183075\tDokładność: 59.93%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.931413\tNajlepsza wartość straty: 0.183075\tDokładność: 53.71%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.600673\tNajlepsza wartość straty: 0.183075\tDokładność: 20.41%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.312802\tNajlepsza wartość straty: 0.183075\tDokładność: 36.98%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.314196\tNajlepsza wartość straty: 0.183075\tDokładność: 36.94%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.159288\tNajlepsza wartość straty: 0.183075\tDokładność: 42.34%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.207461\tNajlepsza wartość straty: 0.183075\tDokładność: 37.92%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.202302\tNajlepsza wartość straty: 0.183075\tDokładność: 40.89%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.203584\tNajlepsza wartość straty: 0.183075\tDokładność: 37.92%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.203974\tNajlepsza wartość straty: 0.183075\tDokładność: 38.04%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.206129\tNajlepsza wartość straty: 0.183075\tDokładność: 40.89%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.202040\tNajlepsza wartość straty: 0.183075\tDokładność: 40.89%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=10, activation=<function relu at 0x0000029B12839B70>, total=  31.1s\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.100560\tNajlepsza wartość straty: 0.100560\tDokładność: 97.42%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.064571\tNajlepsza wartość straty: 0.064571\tDokładność: 98.24%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.059896\tNajlepsza wartość straty: 0.059896\tDokładność: 98.28%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.061158\tNajlepsza wartość straty: 0.059896\tDokładność: 98.44%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.062338\tNajlepsza wartość straty: 0.059896\tDokładność: 98.28%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.082460\tNajlepsza wartość straty: 0.059896\tDokładność: 98.01%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.066188\tNajlepsza wartość straty: 0.059896\tDokładność: 98.32%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.104567\tNajlepsza wartość straty: 0.059896\tDokładność: 97.19%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.067717\tNajlepsza wartość straty: 0.059896\tDokładność: 98.36%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.062248\tNajlepsza wartość straty: 0.059896\tDokładność: 98.48%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.282088\tNajlepsza wartość straty: 0.059896\tDokładność: 96.64%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.293159\tNajlepsza wartość straty: 0.059896\tDokładność: 96.79%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.076235\tNajlepsza wartość straty: 0.059896\tDokładność: 98.12%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.132692\tNajlepsza wartość straty: 0.059896\tDokładność: 98.51%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.085585\tNajlepsza wartość straty: 0.059896\tDokładność: 98.24%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.082006\tNajlepsza wartość straty: 0.059896\tDokładność: 98.48%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.113223\tNajlepsza wartość straty: 0.059896\tDokładność: 98.83%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.086171\tNajlepsza wartość straty: 0.059896\tDokładność: 98.44%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.076994\tNajlepsza wartość straty: 0.059896\tDokładność: 98.75%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.094069\tNajlepsza wartość straty: 0.059896\tDokładność: 98.67%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.077440\tNajlepsza wartość straty: 0.059896\tDokładność: 98.79%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.108786\tNajlepsza wartość straty: 0.059896\tDokładność: 98.48%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.054789\tNajlepsza wartość straty: 0.054789\tDokładność: 98.71%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.067582\tNajlepsza wartość straty: 0.054789\tDokładność: 98.94%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.082732\tNajlepsza wartość straty: 0.054789\tDokładność: 98.67%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.075371\tNajlepsza wartość straty: 0.054789\tDokładność: 98.75%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.102223\tNajlepsza wartość straty: 0.054789\tDokładność: 98.51%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.063994\tNajlepsza wartość straty: 0.054789\tDokładność: 98.91%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.091288\tNajlepsza wartość straty: 0.054789\tDokładność: 98.59%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.076489\tNajlepsza wartość straty: 0.054789\tDokładność: 98.79%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.197025\tNajlepsza wartość straty: 0.054789\tDokładność: 98.63%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.243037\tNajlepsza wartość straty: 0.054789\tDokładność: 98.59%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.131051\tNajlepsza wartość straty: 0.054789\tDokładność: 96.83%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.281952\tNajlepsza wartość straty: 0.054789\tDokładność: 96.95%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.233171\tNajlepsza wartość straty: 0.054789\tDokładność: 95.62%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.194303\tNajlepsza wartość straty: 0.054789\tDokładność: 96.72%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.133571\tNajlepsza wartość straty: 0.054789\tDokładność: 97.34%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.180068\tNajlepsza wartość straty: 0.054789\tDokładność: 96.76%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.162878\tNajlepsza wartość straty: 0.054789\tDokładność: 97.73%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.272459\tNajlepsza wartość straty: 0.054789\tDokładność: 97.42%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.218594\tNajlepsza wartość straty: 0.054789\tDokładność: 98.28%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.155324\tNajlepsza wartość straty: 0.054789\tDokładność: 98.55%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.128950\tNajlepsza wartość straty: 0.054789\tDokładność: 97.93%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.212647\tNajlepsza wartość straty: 0.054789\tDokładność: 98.28%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=  30.7s\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.101060\tNajlepsza wartość straty: 0.101060\tDokładność: 97.11%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.076030\tNajlepsza wartość straty: 0.076030\tDokładność: 97.93%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.059172\tNajlepsza wartość straty: 0.059172\tDokładność: 98.32%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.071612\tNajlepsza wartość straty: 0.059172\tDokładność: 98.24%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.067769\tNajlepsza wartość straty: 0.059172\tDokładność: 98.36%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.079202\tNajlepsza wartość straty: 0.059172\tDokładność: 98.12%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.083621\tNajlepsza wartość straty: 0.059172\tDokładność: 98.44%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.102612\tNajlepsza wartość straty: 0.059172\tDokładność: 97.30%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.056823\tNajlepsza wartość straty: 0.056823\tDokładność: 98.71%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.075331\tNajlepsza wartość straty: 0.056823\tDokładność: 98.24%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.077186\tNajlepsza wartość straty: 0.056823\tDokładność: 97.93%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.075564\tNajlepsza wartość straty: 0.056823\tDokładność: 98.12%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.057039\tNajlepsza wartość straty: 0.056823\tDokładność: 98.71%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.087574\tNajlepsza wartość straty: 0.056823\tDokładność: 98.51%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.089095\tNajlepsza wartość straty: 0.056823\tDokładność: 98.36%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.089334\tNajlepsza wartość straty: 0.056823\tDokładność: 98.24%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.080439\tNajlepsza wartość straty: 0.056823\tDokładność: 98.83%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.114533\tNajlepsza wartość straty: 0.056823\tDokładność: 97.93%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.167646\tNajlepsza wartość straty: 0.056823\tDokładność: 97.62%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.209120\tNajlepsza wartość straty: 0.056823\tDokładność: 98.24%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.083873\tNajlepsza wartość straty: 0.056823\tDokładność: 98.55%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.079726\tNajlepsza wartość straty: 0.056823\tDokładność: 98.59%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.082089\tNajlepsza wartość straty: 0.056823\tDokładność: 98.98%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.104310\tNajlepsza wartość straty: 0.056823\tDokładność: 98.59%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.078105\tNajlepsza wartość straty: 0.056823\tDokładność: 98.59%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.095846\tNajlepsza wartość straty: 0.056823\tDokładność: 98.67%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.090554\tNajlepsza wartość straty: 0.056823\tDokładność: 98.51%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.130618\tNajlepsza wartość straty: 0.056823\tDokładność: 98.75%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.167692\tNajlepsza wartość straty: 0.056823\tDokładność: 98.59%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.308898\tNajlepsza wartość straty: 0.056823\tDokładność: 98.71%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=  21.0s\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.119909\tNajlepsza wartość straty: 0.119909\tDokładność: 97.07%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.073641\tNajlepsza wartość straty: 0.073641\tDokładność: 98.01%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.073299\tNajlepsza wartość straty: 0.073299\tDokładność: 98.20%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.085810\tNajlepsza wartość straty: 0.073299\tDokładność: 98.05%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.063358\tNajlepsza wartość straty: 0.063358\tDokładność: 98.24%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.048215\tNajlepsza wartość straty: 0.048215\tDokładność: 98.59%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.077630\tNajlepsza wartość straty: 0.048215\tDokładność: 98.36%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.085183\tNajlepsza wartość straty: 0.048215\tDokładność: 98.12%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.095770\tNajlepsza wartość straty: 0.048215\tDokładność: 98.51%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.092411\tNajlepsza wartość straty: 0.048215\tDokładność: 98.20%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.058113\tNajlepsza wartość straty: 0.048215\tDokładność: 98.87%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.061329\tNajlepsza wartość straty: 0.048215\tDokładność: 98.67%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.131267\tNajlepsza wartość straty: 0.048215\tDokładność: 97.46%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.119723\tNajlepsza wartość straty: 0.048215\tDokładność: 97.77%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.133003\tNajlepsza wartość straty: 0.048215\tDokładność: 98.24%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.137388\tNajlepsza wartość straty: 0.048215\tDokładność: 98.32%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.075406\tNajlepsza wartość straty: 0.048215\tDokładność: 98.71%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.085748\tNajlepsza wartość straty: 0.048215\tDokładność: 98.75%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.114046\tNajlepsza wartość straty: 0.048215\tDokładność: 98.91%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.168248\tNajlepsza wartość straty: 0.048215\tDokładność: 98.51%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.173646\tNajlepsza wartość straty: 0.048215\tDokładność: 97.77%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.103537\tNajlepsza wartość straty: 0.048215\tDokładność: 97.69%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.120076\tNajlepsza wartość straty: 0.048215\tDokładność: 98.55%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.466013\tNajlepsza wartość straty: 0.048215\tDokładność: 98.55%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.139663\tNajlepsza wartość straty: 0.048215\tDokładność: 97.15%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.212927\tNajlepsza wartość straty: 0.048215\tDokładność: 97.93%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.159866\tNajlepsza wartość straty: 0.048215\tDokładność: 98.20%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=100, activation=<function relu at 0x0000029B12839B70>, total=  18.6s\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=10, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.177730\tNajlepsza wartość straty: 0.177730\tDokładność: 94.53%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.155086\tNajlepsza wartość straty: 0.155086\tDokładność: 95.58%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.289644\tNajlepsza wartość straty: 0.155086\tDokładność: 90.62%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.230656\tNajlepsza wartość straty: 0.155086\tDokładność: 95.00%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.167033\tNajlepsza wartość straty: 0.155086\tDokładność: 97.07%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.148125\tNajlepsza wartość straty: 0.148125\tDokładność: 97.22%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.124431\tNajlepsza wartość straty: 0.124431\tDokładność: 96.91%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.563381\tNajlepsza wartość straty: 0.124431\tDokładność: 94.18%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.522956\tNajlepsza wartość straty: 0.124431\tDokładność: 74.82%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.458252\tNajlepsza wartość straty: 0.124431\tDokładność: 78.54%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.986265\tNajlepsza wartość straty: 0.124431\tDokładność: 58.25%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.822748\tNajlepsza wartość straty: 0.124431\tDokładność: 58.37%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.785720\tNajlepsza wartość straty: 0.124431\tDokładność: 59.89%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.532159\tNajlepsza wartość straty: 0.124431\tDokładność: 73.61%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.469878\tNajlepsza wartość straty: 0.124431\tDokładność: 77.13%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.482317\tNajlepsza wartość straty: 0.124431\tDokładność: 75.49%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.586140\tNajlepsza wartość straty: 0.124431\tDokładność: 89.17%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.564406\tNajlepsza wartość straty: 0.124431\tDokładność: 94.84%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.442381\tNajlepsza wartość straty: 0.124431\tDokładność: 89.87%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.324564\tNajlepsza wartość straty: 0.124431\tDokładność: 93.78%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.709384\tNajlepsza wartość straty: 0.124431\tDokładność: 76.70%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.760272\tNajlepsza wartość straty: 0.124431\tDokładność: 95.39%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.587781\tNajlepsza wartość straty: 0.124431\tDokładność: 96.25%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.838749\tNajlepsza wartość straty: 0.124431\tDokładność: 96.33%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.672867\tNajlepsza wartość straty: 0.124431\tDokładność: 95.47%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.135988\tNajlepsza wartość straty: 0.124431\tDokładność: 96.21%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.000608\tNajlepsza wartość straty: 0.124431\tDokładność: 96.21%\n",
      "27\tF. straty dla zbioru walidacyjnego: 2.559272\tNajlepsza wartość straty: 0.124431\tDokładność: 41.59%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=10, activation=<function relu at 0x0000029B12839B70>, total= 1.3min\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=10, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.368036\tNajlepsza wartość straty: 0.368036\tDokładność: 94.45%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.291047\tNajlepsza wartość straty: 0.291047\tDokładność: 90.73%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.213540\tNajlepsza wartość straty: 0.213540\tDokładność: 95.00%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.517551\tNajlepsza wartość straty: 0.213540\tDokładność: 95.35%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.474658\tNajlepsza wartość straty: 0.213540\tDokładność: 95.04%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.166551\tNajlepsza wartość straty: 0.166551\tDokładność: 97.11%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.147559\tNajlepsza wartość straty: 0.147559\tDokładność: 96.91%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.222711\tNajlepsza wartość straty: 0.147559\tDokładność: 95.50%\n",
      "8\tF. straty dla zbioru walidacyjnego: 2.528255\tNajlepsza wartość straty: 0.147559\tDokładność: 90.70%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.242839\tNajlepsza wartość straty: 0.147559\tDokładność: 95.74%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.601278\tNajlepsza wartość straty: 0.147559\tDokładność: 74.75%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.772968\tNajlepsza wartość straty: 0.147559\tDokładność: 53.09%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.063805\tNajlepsza wartość straty: 0.147559\tDokładność: 46.76%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.875379\tNajlepsza wartość straty: 0.147559\tDokładność: 55.71%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.074155\tNajlepsza wartość straty: 0.147559\tDokładność: 46.33%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.625923\tNajlepsza wartość straty: 0.147559\tDokładność: 22.20%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.623887\tNajlepsza wartość straty: 0.147559\tDokładność: 22.20%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.624331\tNajlepsza wartość straty: 0.147559\tDokładność: 18.96%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.622670\tNajlepsza wartość straty: 0.147559\tDokładność: 22.20%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.621648\tNajlepsza wartość straty: 0.147559\tDokładność: 22.20%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.623025\tNajlepsza wartość straty: 0.147559\tDokładność: 22.20%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.626566\tNajlepsza wartość straty: 0.147559\tDokładność: 22.20%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.630303\tNajlepsza wartość straty: 0.147559\tDokładność: 19.27%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.626819\tNajlepsza wartość straty: 0.147559\tDokładność: 22.20%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.624639\tNajlepsza wartość straty: 0.147559\tDokładność: 22.20%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.623239\tNajlepsza wartość straty: 0.147559\tDokładność: 22.20%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.622122\tNajlepsza wartość straty: 0.147559\tDokładność: 22.20%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.627398\tNajlepsza wartość straty: 0.147559\tDokładność: 22.20%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=10, activation=<function relu at 0x0000029B12839B70>, total= 1.3min\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=10, activation=<function relu at 0x0000029B12839B70> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.180149\tNajlepsza wartość straty: 0.180149\tDokładność: 94.68%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.157084\tNajlepsza wartość straty: 0.157084\tDokładność: 95.74%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.130313\tNajlepsza wartość straty: 0.130313\tDokładność: 96.68%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.231468\tNajlepsza wartość straty: 0.130313\tDokładność: 95.19%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.203979\tNajlepsza wartość straty: 0.130313\tDokładność: 96.09%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.185015\tNajlepsza wartość straty: 0.130313\tDokładność: 96.72%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.349428\tNajlepsza wartość straty: 0.130313\tDokładność: 95.19%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.207343\tNajlepsza wartość straty: 0.130313\tDokładność: 96.17%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.184760\tNajlepsza wartość straty: 0.130313\tDokładność: 96.72%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.192384\tNajlepsza wartość straty: 0.130313\tDokładność: 96.44%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.456134\tNajlepsza wartość straty: 0.130313\tDokładność: 92.65%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.388055\tNajlepsza wartość straty: 0.130313\tDokładność: 95.54%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.270736\tNajlepsza wartość straty: 0.130313\tDokładność: 94.72%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.537817\tNajlepsza wartość straty: 0.130313\tDokładność: 71.23%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.211486\tNajlepsza wartość straty: 0.130313\tDokładność: 96.36%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.748074\tNajlepsza wartość straty: 0.130313\tDokładność: 81.94%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.447970\tNajlepsza wartość straty: 0.130313\tDokładność: 95.19%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.532634\tNajlepsza wartość straty: 0.130313\tDokładność: 77.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.472701\tNajlepsza wartość straty: 0.130313\tDokładność: 78.54%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.466343\tNajlepsza wartość straty: 0.130313\tDokładność: 77.25%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.358487\tNajlepsza wartość straty: 0.130313\tDokładność: 90.50%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.766432\tNajlepsza wartość straty: 0.130313\tDokładność: 58.41%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.844737\tNajlepsza wartość straty: 0.130313\tDokładność: 57.19%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.987269\tNajlepsza wartość straty: 0.130313\tDokładność: 57.66%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=10, activation=<function relu at 0x0000029B12839B70>, total= 1.1min\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 937.351318\tNajlepsza wartość straty: 937.351318\tDokładność: 58.87%\n",
      "1\tF. straty dla zbioru walidacyjnego: 23.323162\tNajlepsza wartość straty: 23.323162\tDokładność: 76.00%\n",
      "2\tF. straty dla zbioru walidacyjnego: 3.319624\tNajlepsza wartość straty: 3.319624\tDokładność: 91.24%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.833943\tNajlepsza wartość straty: 1.833943\tDokładność: 93.82%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.109632\tNajlepsza wartość straty: 1.109632\tDokładność: 94.25%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.145719\tNajlepsza wartość straty: 1.109632\tDokładność: 93.86%\n",
      "6\tF. straty dla zbioru walidacyjnego: 7.042073\tNajlepsza wartość straty: 1.109632\tDokładność: 77.60%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.894228\tNajlepsza wartość straty: 1.109632\tDokładność: 94.21%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.950218\tNajlepsza wartość straty: 0.950218\tDokładność: 95.07%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.606301\tNajlepsza wartość straty: 0.606301\tDokładność: 95.90%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.511399\tNajlepsza wartość straty: 0.511399\tDokładność: 95.97%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.541621\tNajlepsza wartość straty: 0.511399\tDokładność: 94.64%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.783225\tNajlepsza wartość straty: 0.511399\tDokładność: 91.40%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.013482\tNajlepsza wartość straty: 0.511399\tDokładność: 94.88%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.637737\tNajlepsza wartość straty: 0.511399\tDokładność: 95.90%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.457778\tNajlepsza wartość straty: 0.457778\tDokładność: 96.01%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.359711\tNajlepsza wartość straty: 0.359711\tDokładność: 95.70%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.322349\tNajlepsza wartość straty: 0.322349\tDokładność: 96.40%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.319511\tNajlepsza wartość straty: 0.319511\tDokładność: 96.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.300492\tNajlepsza wartość straty: 0.300492\tDokładność: 95.97%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.331264\tNajlepsza wartość straty: 0.300492\tDokładność: 95.74%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.368421\tNajlepsza wartość straty: 0.300492\tDokładność: 95.43%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.370960\tNajlepsza wartość straty: 0.300492\tDokładność: 96.21%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.251154\tNajlepsza wartość straty: 0.251154\tDokładność: 96.79%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.357798\tNajlepsza wartość straty: 0.251154\tDokładność: 95.97%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.296905\tNajlepsza wartość straty: 0.251154\tDokładność: 96.79%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.343756\tNajlepsza wartość straty: 0.251154\tDokładność: 95.70%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.273392\tNajlepsza wartość straty: 0.251154\tDokładność: 95.54%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.251059\tNajlepsza wartość straty: 0.251059\tDokładność: 96.95%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.499325\tNajlepsza wartość straty: 0.251059\tDokładność: 95.90%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.384826\tNajlepsza wartość straty: 0.251059\tDokładność: 95.27%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.303310\tNajlepsza wartość straty: 0.251059\tDokładność: 96.99%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.288117\tNajlepsza wartość straty: 0.251059\tDokładność: 96.64%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.308343\tNajlepsza wartość straty: 0.251059\tDokładność: 97.03%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.370919\tNajlepsza wartość straty: 0.251059\tDokładność: 96.21%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.296865\tNajlepsza wartość straty: 0.251059\tDokładność: 96.56%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.290316\tNajlepsza wartość straty: 0.251059\tDokładność: 96.79%\n",
      "37\tF. straty dla zbioru walidacyjnego: 0.357526\tNajlepsza wartość straty: 0.251059\tDokładność: 95.86%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.445961\tNajlepsza wartość straty: 0.251059\tDokładność: 95.66%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.283479\tNajlepsza wartość straty: 0.251059\tDokładność: 97.22%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.262465\tNajlepsza wartość straty: 0.251059\tDokładność: 97.15%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.292834\tNajlepsza wartość straty: 0.251059\tDokładność: 96.79%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.286109\tNajlepsza wartość straty: 0.251059\tDokładność: 97.19%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.248679\tNajlepsza wartość straty: 0.248679\tDokładność: 97.30%\n",
      "44\tF. straty dla zbioru walidacyjnego: 0.267327\tNajlepsza wartość straty: 0.248679\tDokładność: 97.07%\n",
      "45\tF. straty dla zbioru walidacyjnego: 0.299408\tNajlepsza wartość straty: 0.248679\tDokładność: 96.99%\n",
      "46\tF. straty dla zbioru walidacyjnego: 0.292746\tNajlepsza wartość straty: 0.248679\tDokładność: 97.38%\n",
      "47\tF. straty dla zbioru walidacyjnego: 0.356117\tNajlepsza wartość straty: 0.248679\tDokładność: 96.05%\n",
      "48\tF. straty dla zbioru walidacyjnego: 0.254641\tNajlepsza wartość straty: 0.248679\tDokładność: 97.26%\n",
      "49\tF. straty dla zbioru walidacyjnego: 0.356572\tNajlepsza wartość straty: 0.248679\tDokładność: 97.03%\n",
      "50\tF. straty dla zbioru walidacyjnego: 0.270575\tNajlepsza wartość straty: 0.248679\tDokładność: 96.29%\n",
      "51\tF. straty dla zbioru walidacyjnego: 0.302076\tNajlepsza wartość straty: 0.248679\tDokładność: 96.72%\n",
      "52\tF. straty dla zbioru walidacyjnego: 0.244429\tNajlepsza wartość straty: 0.244429\tDokładność: 97.26%\n",
      "53\tF. straty dla zbioru walidacyjnego: 0.367095\tNajlepsza wartość straty: 0.244429\tDokładność: 97.19%\n",
      "54\tF. straty dla zbioru walidacyjnego: 0.271841\tNajlepsza wartość straty: 0.244429\tDokładność: 97.15%\n",
      "55\tF. straty dla zbioru walidacyjnego: 0.217622\tNajlepsza wartość straty: 0.217622\tDokładność: 97.30%\n",
      "56\tF. straty dla zbioru walidacyjnego: 0.240967\tNajlepsza wartość straty: 0.217622\tDokładność: 97.50%\n",
      "57\tF. straty dla zbioru walidacyjnego: 0.385473\tNajlepsza wartość straty: 0.217622\tDokładność: 97.03%\n",
      "58\tF. straty dla zbioru walidacyjnego: 0.429246\tNajlepsza wartość straty: 0.217622\tDokładność: 96.64%\n",
      "59\tF. straty dla zbioru walidacyjnego: 0.200828\tNajlepsza wartość straty: 0.200828\tDokładność: 97.77%\n",
      "60\tF. straty dla zbioru walidacyjnego: 0.220383\tNajlepsza wartość straty: 0.200828\tDokładność: 97.07%\n",
      "61\tF. straty dla zbioru walidacyjnego: 0.330541\tNajlepsza wartość straty: 0.200828\tDokładność: 95.50%\n",
      "62\tF. straty dla zbioru walidacyjnego: 0.255946\tNajlepsza wartość straty: 0.200828\tDokładność: 96.68%\n",
      "63\tF. straty dla zbioru walidacyjnego: 0.301481\tNajlepsza wartość straty: 0.200828\tDokładność: 97.54%\n",
      "64\tF. straty dla zbioru walidacyjnego: 0.256653\tNajlepsza wartość straty: 0.200828\tDokładność: 97.62%\n",
      "65\tF. straty dla zbioru walidacyjnego: 0.361173\tNajlepsza wartość straty: 0.200828\tDokładność: 96.68%\n",
      "66\tF. straty dla zbioru walidacyjnego: 0.265041\tNajlepsza wartość straty: 0.200828\tDokładność: 97.85%\n",
      "67\tF. straty dla zbioru walidacyjnego: 0.238821\tNajlepsza wartość straty: 0.200828\tDokładność: 97.50%\n",
      "68\tF. straty dla zbioru walidacyjnego: 0.285580\tNajlepsza wartość straty: 0.200828\tDokładność: 97.42%\n",
      "69\tF. straty dla zbioru walidacyjnego: 0.293243\tNajlepsza wartość straty: 0.200828\tDokładność: 97.38%\n",
      "70\tF. straty dla zbioru walidacyjnego: 0.292487\tNajlepsza wartość straty: 0.200828\tDokładność: 97.42%\n",
      "71\tF. straty dla zbioru walidacyjnego: 0.271195\tNajlepsza wartość straty: 0.200828\tDokładność: 97.65%\n",
      "72\tF. straty dla zbioru walidacyjnego: 0.351114\tNajlepsza wartość straty: 0.200828\tDokładność: 97.65%\n",
      "73\tF. straty dla zbioru walidacyjnego: 0.229523\tNajlepsza wartość straty: 0.200828\tDokładność: 97.69%\n",
      "74\tF. straty dla zbioru walidacyjnego: 0.263113\tNajlepsza wartość straty: 0.200828\tDokładność: 97.54%\n",
      "75\tF. straty dla zbioru walidacyjnego: 0.256046\tNajlepsza wartość straty: 0.200828\tDokładność: 97.77%\n",
      "76\tF. straty dla zbioru walidacyjnego: 0.380495\tNajlepsza wartość straty: 0.200828\tDokładność: 97.77%\n",
      "77\tF. straty dla zbioru walidacyjnego: 0.243622\tNajlepsza wartość straty: 0.200828\tDokładność: 97.46%\n",
      "78\tF. straty dla zbioru walidacyjnego: 0.222517\tNajlepsza wartość straty: 0.200828\tDokładność: 96.87%\n",
      "79\tF. straty dla zbioru walidacyjnego: 0.141869\tNajlepsza wartość straty: 0.141869\tDokładność: 97.81%\n",
      "80\tF. straty dla zbioru walidacyjnego: 0.171836\tNajlepsza wartość straty: 0.141869\tDokładność: 97.38%\n",
      "81\tF. straty dla zbioru walidacyjnego: 0.244034\tNajlepsza wartość straty: 0.141869\tDokładność: 95.97%\n",
      "82\tF. straty dla zbioru walidacyjnego: 0.235036\tNajlepsza wartość straty: 0.141869\tDokładność: 97.07%\n",
      "83\tF. straty dla zbioru walidacyjnego: 0.151595\tNajlepsza wartość straty: 0.141869\tDokładność: 97.58%\n",
      "84\tF. straty dla zbioru walidacyjnego: 0.264189\tNajlepsza wartość straty: 0.141869\tDokładność: 97.22%\n",
      "85\tF. straty dla zbioru walidacyjnego: 0.235006\tNajlepsza wartość straty: 0.141869\tDokładność: 97.97%\n",
      "86\tF. straty dla zbioru walidacyjnego: 0.212410\tNajlepsza wartość straty: 0.141869\tDokładność: 97.26%\n",
      "87\tF. straty dla zbioru walidacyjnego: 0.191127\tNajlepsza wartość straty: 0.141869\tDokładność: 97.38%\n",
      "88\tF. straty dla zbioru walidacyjnego: 0.320956\tNajlepsza wartość straty: 0.141869\tDokładność: 97.77%\n",
      "89\tF. straty dla zbioru walidacyjnego: 0.274070\tNajlepsza wartość straty: 0.141869\tDokładność: 97.54%\n",
      "90\tF. straty dla zbioru walidacyjnego: 0.377264\tNajlepsza wartość straty: 0.141869\tDokładność: 96.87%\n",
      "91\tF. straty dla zbioru walidacyjnego: 0.243358\tNajlepsza wartość straty: 0.141869\tDokładność: 97.19%\n",
      "92\tF. straty dla zbioru walidacyjnego: 0.196108\tNajlepsza wartość straty: 0.141869\tDokładność: 97.46%\n",
      "93\tF. straty dla zbioru walidacyjnego: 0.202162\tNajlepsza wartość straty: 0.141869\tDokładność: 97.42%\n",
      "94\tF. straty dla zbioru walidacyjnego: 0.203542\tNajlepsza wartość straty: 0.141869\tDokładność: 97.22%\n",
      "95\tF. straty dla zbioru walidacyjnego: 0.216120\tNajlepsza wartość straty: 0.141869\tDokładność: 97.15%\n",
      "96\tF. straty dla zbioru walidacyjnego: 0.312784\tNajlepsza wartość straty: 0.141869\tDokładność: 96.33%\n",
      "97\tF. straty dla zbioru walidacyjnego: 0.240179\tNajlepsza wartość straty: 0.141869\tDokładność: 97.54%\n",
      "98\tF. straty dla zbioru walidacyjnego: 0.235622\tNajlepsza wartość straty: 0.141869\tDokładność: 97.38%\n",
      "99\tF. straty dla zbioru walidacyjnego: 0.251198\tNajlepsza wartość straty: 0.141869\tDokładność: 96.68%\n",
      "100\tF. straty dla zbioru walidacyjnego: 0.277830\tNajlepsza wartość straty: 0.141869\tDokładność: 96.79%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  48.1s\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 932.208069\tNajlepsza wartość straty: 932.208069\tDokładność: 24.86%\n",
      "1\tF. straty dla zbioru walidacyjnego: 62.815895\tNajlepsza wartość straty: 62.815895\tDokładność: 72.63%\n",
      "2\tF. straty dla zbioru walidacyjnego: 21.440659\tNajlepsza wartość straty: 21.440659\tDokładność: 77.76%\n",
      "3\tF. straty dla zbioru walidacyjnego: 8.069399\tNajlepsza wartość straty: 8.069399\tDokładność: 91.59%\n",
      "4\tF. straty dla zbioru walidacyjnego: 4.445180\tNajlepsza wartość straty: 4.445180\tDokładność: 93.82%\n",
      "5\tF. straty dla zbioru walidacyjnego: 4.113484\tNajlepsza wartość straty: 4.113484\tDokładność: 93.55%\n",
      "6\tF. straty dla zbioru walidacyjnego: 2.797242\tNajlepsza wartość straty: 2.797242\tDokładność: 94.41%\n",
      "7\tF. straty dla zbioru walidacyjnego: 2.754288\tNajlepsza wartość straty: 2.754288\tDokładność: 93.98%\n",
      "8\tF. straty dla zbioru walidacyjnego: 2.887421\tNajlepsza wartość straty: 2.754288\tDokładność: 94.02%\n",
      "9\tF. straty dla zbioru walidacyjnego: 2.276258\tNajlepsza wartość straty: 2.276258\tDokładność: 94.53%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.945069\tNajlepsza wartość straty: 1.945069\tDokładność: 95.47%\n",
      "11\tF. straty dla zbioru walidacyjnego: 2.045178\tNajlepsza wartość straty: 1.945069\tDokładność: 94.41%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.519734\tNajlepsza wartość straty: 1.519734\tDokładność: 96.01%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.783953\tNajlepsza wartość straty: 1.519734\tDokładność: 95.15%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.358704\tNajlepsza wartość straty: 1.358704\tDokładność: 96.36%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.396054\tNajlepsza wartość straty: 1.358704\tDokładność: 95.66%\n",
      "16\tF. straty dla zbioru walidacyjnego: 3.832327\tNajlepsza wartość straty: 1.358704\tDokładność: 88.78%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.705525\tNajlepsza wartość straty: 1.358704\tDokładność: 95.70%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.198238\tNajlepsza wartość straty: 1.198238\tDokładność: 95.90%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.849677\tNajlepsza wartość straty: 1.198238\tDokładność: 93.71%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.266339\tNajlepsza wartość straty: 1.198238\tDokładność: 96.17%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.028803\tNajlepsza wartość straty: 1.028803\tDokładność: 96.29%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.331074\tNajlepsza wartość straty: 1.028803\tDokładność: 94.76%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.982436\tNajlepsza wartość straty: 0.982436\tDokładność: 96.87%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.066955\tNajlepsza wartość straty: 0.982436\tDokładność: 96.01%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.369812\tNajlepsza wartość straty: 0.982436\tDokładność: 95.39%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.037647\tNajlepsza wartość straty: 0.982436\tDokładność: 96.99%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.225570\tNajlepsza wartość straty: 0.982436\tDokładność: 96.05%\n",
      "28\tF. straty dla zbioru walidacyjnego: 1.189608\tNajlepsza wartość straty: 0.982436\tDokładność: 95.70%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.884369\tNajlepsza wartość straty: 0.884369\tDokładność: 96.60%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.898277\tNajlepsza wartość straty: 0.884369\tDokładność: 96.25%\n",
      "31\tF. straty dla zbioru walidacyjnego: 1.970573\tNajlepsza wartość straty: 0.884369\tDokładność: 92.42%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.841065\tNajlepsza wartość straty: 0.841065\tDokładność: 96.44%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.906153\tNajlepsza wartość straty: 0.841065\tDokładność: 96.68%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.850315\tNajlepsza wartość straty: 0.841065\tDokładność: 96.91%\n",
      "35\tF. straty dla zbioru walidacyjnego: 0.732348\tNajlepsza wartość straty: 0.732348\tDokładność: 97.11%\n",
      "36\tF. straty dla zbioru walidacyjnego: 0.728538\tNajlepsza wartość straty: 0.728538\tDokładność: 96.95%\n",
      "37\tF. straty dla zbioru walidacyjnego: 1.075766\tNajlepsza wartość straty: 0.728538\tDokładność: 95.58%\n",
      "38\tF. straty dla zbioru walidacyjnego: 0.760252\tNajlepsza wartość straty: 0.728538\tDokładność: 97.03%\n",
      "39\tF. straty dla zbioru walidacyjnego: 0.815813\tNajlepsza wartość straty: 0.728538\tDokładność: 96.79%\n",
      "40\tF. straty dla zbioru walidacyjnego: 0.893132\tNajlepsza wartość straty: 0.728538\tDokładność: 96.25%\n",
      "41\tF. straty dla zbioru walidacyjnego: 0.842061\tNajlepsza wartość straty: 0.728538\tDokładność: 96.68%\n",
      "42\tF. straty dla zbioru walidacyjnego: 0.925552\tNajlepsza wartość straty: 0.728538\tDokładność: 96.25%\n",
      "43\tF. straty dla zbioru walidacyjnego: 0.890564\tNajlepsza wartość straty: 0.728538\tDokładność: 96.76%\n",
      "44\tF. straty dla zbioru walidacyjnego: 0.853202\tNajlepsza wartość straty: 0.728538\tDokładność: 96.01%\n",
      "45\tF. straty dla zbioru walidacyjnego: 0.928082\tNajlepsza wartość straty: 0.728538\tDokładność: 96.68%\n",
      "46\tF. straty dla zbioru walidacyjnego: 0.839204\tNajlepsza wartość straty: 0.728538\tDokładność: 96.48%\n",
      "47\tF. straty dla zbioru walidacyjnego: 1.013823\tNajlepsza wartość straty: 0.728538\tDokładność: 95.23%\n",
      "48\tF. straty dla zbioru walidacyjnego: 0.787675\tNajlepsza wartość straty: 0.728538\tDokładność: 96.64%\n",
      "49\tF. straty dla zbioru walidacyjnego: 0.802847\tNajlepsza wartość straty: 0.728538\tDokładność: 95.93%\n",
      "50\tF. straty dla zbioru walidacyjnego: 0.824373\tNajlepsza wartość straty: 0.728538\tDokładność: 97.03%\n",
      "51\tF. straty dla zbioru walidacyjnego: 1.106146\tNajlepsza wartość straty: 0.728538\tDokładność: 96.25%\n",
      "52\tF. straty dla zbioru walidacyjnego: 0.677037\tNajlepsza wartość straty: 0.677037\tDokładność: 97.15%\n",
      "53\tF. straty dla zbioru walidacyjnego: 0.854502\tNajlepsza wartość straty: 0.677037\tDokładność: 96.68%\n",
      "54\tF. straty dla zbioru walidacyjnego: 1.007826\tNajlepsza wartość straty: 0.677037\tDokładność: 96.99%\n",
      "55\tF. straty dla zbioru walidacyjnego: 0.911254\tNajlepsza wartość straty: 0.677037\tDokładność: 96.91%\n",
      "56\tF. straty dla zbioru walidacyjnego: 0.823436\tNajlepsza wartość straty: 0.677037\tDokładność: 97.26%\n",
      "57\tF. straty dla zbioru walidacyjnego: 0.723436\tNajlepsza wartość straty: 0.677037\tDokładność: 96.79%\n",
      "58\tF. straty dla zbioru walidacyjnego: 0.721934\tNajlepsza wartość straty: 0.677037\tDokładność: 96.83%\n",
      "59\tF. straty dla zbioru walidacyjnego: 0.677597\tNajlepsza wartość straty: 0.677037\tDokładność: 97.54%\n",
      "60\tF. straty dla zbioru walidacyjnego: 0.813347\tNajlepsza wartość straty: 0.677037\tDokładność: 96.44%\n",
      "61\tF. straty dla zbioru walidacyjnego: 0.853633\tNajlepsza wartość straty: 0.677037\tDokładność: 97.38%\n",
      "62\tF. straty dla zbioru walidacyjnego: 0.823132\tNajlepsza wartość straty: 0.677037\tDokładność: 97.15%\n",
      "63\tF. straty dla zbioru walidacyjnego: 0.933727\tNajlepsza wartość straty: 0.677037\tDokładność: 96.40%\n",
      "64\tF. straty dla zbioru walidacyjnego: 1.186187\tNajlepsza wartość straty: 0.677037\tDokładność: 95.74%\n",
      "65\tF. straty dla zbioru walidacyjnego: 0.885813\tNajlepsza wartość straty: 0.677037\tDokładność: 96.83%\n",
      "66\tF. straty dla zbioru walidacyjnego: 0.752798\tNajlepsza wartość straty: 0.677037\tDokładność: 97.11%\n",
      "67\tF. straty dla zbioru walidacyjnego: 0.784632\tNajlepsza wartość straty: 0.677037\tDokładność: 97.07%\n",
      "68\tF. straty dla zbioru walidacyjnego: 0.740041\tNajlepsza wartość straty: 0.677037\tDokładność: 97.22%\n",
      "69\tF. straty dla zbioru walidacyjnego: 1.004805\tNajlepsza wartość straty: 0.677037\tDokładność: 97.34%\n",
      "70\tF. straty dla zbioru walidacyjnego: 0.719259\tNajlepsza wartość straty: 0.677037\tDokładność: 96.79%\n",
      "71\tF. straty dla zbioru walidacyjnego: 0.691672\tNajlepsza wartość straty: 0.677037\tDokładność: 96.68%\n",
      "72\tF. straty dla zbioru walidacyjnego: 234460960.000000\tNajlepsza wartość straty: 0.677037\tDokładność: 18.73%\n",
      "73\tF. straty dla zbioru walidacyjnego: 10594529.000000\tNajlepsza wartość straty: 0.677037\tDokładność: 32.29%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  35.9s\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 374.003845\tNajlepsza wartość straty: 374.003845\tDokładność: 51.33%\n",
      "1\tF. straty dla zbioru walidacyjnego: 169430.734375\tNajlepsza wartość straty: 374.003845\tDokładność: 19.27%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1914.309204\tNajlepsza wartość straty: 374.003845\tDokładność: 81.24%\n",
      "3\tF. straty dla zbioru walidacyjnego: 246.338638\tNajlepsza wartość straty: 246.338638\tDokładność: 93.20%\n",
      "4\tF. straty dla zbioru walidacyjnego: 137.660080\tNajlepsza wartość straty: 137.660080\tDokładność: 93.24%\n",
      "5\tF. straty dla zbioru walidacyjnego: 98.162331\tNajlepsza wartość straty: 98.162331\tDokładność: 94.18%\n",
      "6\tF. straty dla zbioru walidacyjnego: 83.259789\tNajlepsza wartość straty: 83.259789\tDokładność: 93.98%\n",
      "7\tF. straty dla zbioru walidacyjnego: 62.992706\tNajlepsza wartość straty: 62.992706\tDokładność: 94.53%\n",
      "8\tF. straty dla zbioru walidacyjnego: 48.408894\tNajlepsza wartość straty: 48.408894\tDokładność: 95.35%\n",
      "9\tF. straty dla zbioru walidacyjnego: 44.125984\tNajlepsza wartość straty: 44.125984\tDokładność: 95.58%\n",
      "10\tF. straty dla zbioru walidacyjnego: 49.359730\tNajlepsza wartość straty: 44.125984\tDokładność: 95.23%\n",
      "11\tF. straty dla zbioru walidacyjnego: 70.868073\tNajlepsza wartość straty: 44.125984\tDokładność: 94.41%\n",
      "12\tF. straty dla zbioru walidacyjnego: 42.021652\tNajlepsza wartość straty: 42.021652\tDokładność: 95.70%\n",
      "13\tF. straty dla zbioru walidacyjnego: 38.325909\tNajlepsza wartość straty: 38.325909\tDokładność: 95.15%\n",
      "14\tF. straty dla zbioru walidacyjnego: 35.446056\tNajlepsza wartość straty: 35.446056\tDokładność: 95.31%\n",
      "15\tF. straty dla zbioru walidacyjnego: 46.161556\tNajlepsza wartość straty: 35.446056\tDokładność: 94.14%\n",
      "16\tF. straty dla zbioru walidacyjnego: 43.533810\tNajlepsza wartość straty: 35.446056\tDokładność: 95.00%\n",
      "17\tF. straty dla zbioru walidacyjnego: 32.516541\tNajlepsza wartość straty: 32.516541\tDokładność: 95.43%\n",
      "18\tF. straty dla zbioru walidacyjnego: 25.486969\tNajlepsza wartość straty: 25.486969\tDokładność: 95.31%\n",
      "19\tF. straty dla zbioru walidacyjnego: 33.883423\tNajlepsza wartość straty: 25.486969\tDokładność: 94.41%\n",
      "20\tF. straty dla zbioru walidacyjnego: 32.266609\tNajlepsza wartość straty: 25.486969\tDokładność: 96.29%\n",
      "21\tF. straty dla zbioru walidacyjnego: 37.301052\tNajlepsza wartość straty: 25.486969\tDokładność: 94.14%\n",
      "22\tF. straty dla zbioru walidacyjnego: 26.593481\tNajlepsza wartość straty: 25.486969\tDokładność: 96.33%\n",
      "23\tF. straty dla zbioru walidacyjnego: 27.516214\tNajlepsza wartość straty: 25.486969\tDokładność: 95.62%\n",
      "24\tF. straty dla zbioru walidacyjnego: 26.928465\tNajlepsza wartość straty: 25.486969\tDokładność: 95.15%\n",
      "25\tF. straty dla zbioru walidacyjnego: 31.737062\tNajlepsza wartość straty: 25.486969\tDokładność: 94.92%\n",
      "26\tF. straty dla zbioru walidacyjnego: 39.603886\tNajlepsza wartość straty: 25.486969\tDokładność: 95.39%\n",
      "27\tF. straty dla zbioru walidacyjnego: 27.403923\tNajlepsza wartość straty: 25.486969\tDokładność: 95.97%\n",
      "28\tF. straty dla zbioru walidacyjnego: 31.061657\tNajlepsza wartość straty: 25.486969\tDokładność: 96.01%\n",
      "29\tF. straty dla zbioru walidacyjnego: 37.611771\tNajlepsza wartość straty: 25.486969\tDokładność: 95.74%\n",
      "30\tF. straty dla zbioru walidacyjnego: 40.684856\tNajlepsza wartość straty: 25.486969\tDokładność: 95.23%\n",
      "31\tF. straty dla zbioru walidacyjnego: 36.854374\tNajlepsza wartość straty: 25.486969\tDokładność: 96.36%\n",
      "32\tF. straty dla zbioru walidacyjnego: 19.092474\tNajlepsza wartość straty: 19.092474\tDokładność: 95.74%\n",
      "33\tF. straty dla zbioru walidacyjnego: 21.234945\tNajlepsza wartość straty: 19.092474\tDokładność: 95.50%\n",
      "34\tF. straty dla zbioru walidacyjnego: 26.890766\tNajlepsza wartość straty: 19.092474\tDokładność: 95.35%\n",
      "35\tF. straty dla zbioru walidacyjnego: 19.593563\tNajlepsza wartość straty: 19.092474\tDokładność: 96.76%\n",
      "36\tF. straty dla zbioru walidacyjnego: 19.657900\tNajlepsza wartość straty: 19.092474\tDokładność: 96.83%\n",
      "37\tF. straty dla zbioru walidacyjnego: 35.599224\tNajlepsza wartość straty: 19.092474\tDokładność: 93.75%\n",
      "38\tF. straty dla zbioru walidacyjnego: 19.682587\tNajlepsza wartość straty: 19.092474\tDokładność: 96.40%\n",
      "39\tF. straty dla zbioru walidacyjnego: 15.422065\tNajlepsza wartość straty: 15.422065\tDokładność: 96.56%\n",
      "40\tF. straty dla zbioru walidacyjnego: 12.882860\tNajlepsza wartość straty: 12.882860\tDokładność: 96.83%\n",
      "41\tF. straty dla zbioru walidacyjnego: 19.119328\tNajlepsza wartość straty: 12.882860\tDokładność: 95.66%\n",
      "42\tF. straty dla zbioru walidacyjnego: 13.169023\tNajlepsza wartość straty: 12.882860\tDokładność: 97.30%\n",
      "43\tF. straty dla zbioru walidacyjnego: 17.423220\tNajlepsza wartość straty: 12.882860\tDokładność: 95.82%\n",
      "44\tF. straty dla zbioru walidacyjnego: 15.510068\tNajlepsza wartość straty: 12.882860\tDokładność: 96.56%\n",
      "45\tF. straty dla zbioru walidacyjnego: 12.258205\tNajlepsza wartość straty: 12.258205\tDokładność: 96.79%\n",
      "46\tF. straty dla zbioru walidacyjnego: 19.238981\tNajlepsza wartość straty: 12.258205\tDokładność: 96.44%\n",
      "47\tF. straty dla zbioru walidacyjnego: 17.314310\tNajlepsza wartość straty: 12.258205\tDokładność: 96.05%\n",
      "48\tF. straty dla zbioru walidacyjnego: 14.067799\tNajlepsza wartość straty: 12.258205\tDokładność: 97.07%\n",
      "49\tF. straty dla zbioru walidacyjnego: 17.640030\tNajlepsza wartość straty: 12.258205\tDokładność: 96.60%\n",
      "50\tF. straty dla zbioru walidacyjnego: 10.285506\tNajlepsza wartość straty: 10.285506\tDokładność: 97.30%\n",
      "51\tF. straty dla zbioru walidacyjnego: 16.519432\tNajlepsza wartość straty: 10.285506\tDokładność: 96.56%\n",
      "52\tF. straty dla zbioru walidacyjnego: 12.952973\tNajlepsza wartość straty: 10.285506\tDokładność: 95.70%\n",
      "53\tF. straty dla zbioru walidacyjnego: 18.872400\tNajlepsza wartość straty: 10.285506\tDokładność: 96.60%\n",
      "54\tF. straty dla zbioru walidacyjnego: 11.786853\tNajlepsza wartość straty: 10.285506\tDokładność: 96.40%\n",
      "55\tF. straty dla zbioru walidacyjnego: 26.622238\tNajlepsza wartość straty: 10.285506\tDokładność: 95.74%\n",
      "56\tF. straty dla zbioru walidacyjnego: 19.355867\tNajlepsza wartość straty: 10.285506\tDokładność: 96.29%\n",
      "57\tF. straty dla zbioru walidacyjnego: 21.433214\tNajlepsza wartość straty: 10.285506\tDokładność: 96.72%\n",
      "58\tF. straty dla zbioru walidacyjnego: 22.462477\tNajlepsza wartość straty: 10.285506\tDokładność: 96.40%\n",
      "59\tF. straty dla zbioru walidacyjnego: 13.860013\tNajlepsza wartość straty: 10.285506\tDokładność: 97.50%\n",
      "60\tF. straty dla zbioru walidacyjnego: 50.476124\tNajlepsza wartość straty: 10.285506\tDokładność: 90.66%\n",
      "61\tF. straty dla zbioru walidacyjnego: 15.571870\tNajlepsza wartość straty: 10.285506\tDokładność: 97.38%\n",
      "62\tF. straty dla zbioru walidacyjnego: 18.840605\tNajlepsza wartość straty: 10.285506\tDokładność: 95.54%\n",
      "63\tF. straty dla zbioru walidacyjnego: 111.327774\tNajlepsza wartość straty: 10.285506\tDokładność: 90.85%\n",
      "64\tF. straty dla zbioru walidacyjnego: 37.702106\tNajlepsza wartość straty: 10.285506\tDokładność: 95.11%\n",
      "65\tF. straty dla zbioru walidacyjnego: 22.563660\tNajlepsza wartość straty: 10.285506\tDokładność: 96.36%\n",
      "66\tF. straty dla zbioru walidacyjnego: 18.967199\tNajlepsza wartość straty: 10.285506\tDokładność: 96.48%\n",
      "67\tF. straty dla zbioru walidacyjnego: 13.645690\tNajlepsza wartość straty: 10.285506\tDokładność: 97.15%\n",
      "68\tF. straty dla zbioru walidacyjnego: 11.168672\tNajlepsza wartość straty: 10.285506\tDokładność: 97.42%\n",
      "69\tF. straty dla zbioru walidacyjnego: 10.943726\tNajlepsza wartość straty: 10.285506\tDokładność: 97.34%\n",
      "70\tF. straty dla zbioru walidacyjnego: 10.370327\tNajlepsza wartość straty: 10.285506\tDokładność: 97.77%\n",
      "71\tF. straty dla zbioru walidacyjnego: 13.504732\tNajlepsza wartość straty: 10.285506\tDokładność: 95.93%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  34.8s\n",
      "[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.112523\tNajlepsza wartość straty: 0.112523\tDokładność: 96.44%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.086763\tNajlepsza wartość straty: 0.086763\tDokładność: 97.46%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.072743\tNajlepsza wartość straty: 0.072743\tDokładność: 97.85%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.065047\tNajlepsza wartość straty: 0.065047\tDokładność: 98.24%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.061005\tNajlepsza wartość straty: 0.061005\tDokładność: 98.32%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.062099\tNajlepsza wartość straty: 0.061005\tDokładność: 98.44%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.074967\tNajlepsza wartość straty: 0.061005\tDokładność: 98.20%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.084532\tNajlepsza wartość straty: 0.061005\tDokładność: 98.28%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.056337\tNajlepsza wartość straty: 0.056337\tDokładność: 98.51%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.050394\tNajlepsza wartość straty: 0.050394\tDokładność: 98.83%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.071967\tNajlepsza wartość straty: 0.050394\tDokładność: 98.44%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.071422\tNajlepsza wartość straty: 0.050394\tDokładność: 98.79%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.057855\tNajlepsza wartość straty: 0.050394\tDokładność: 98.59%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.049854\tNajlepsza wartość straty: 0.049854\tDokładność: 98.83%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.058492\tNajlepsza wartość straty: 0.049854\tDokładność: 98.87%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.061254\tNajlepsza wartość straty: 0.049854\tDokładność: 98.59%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.065606\tNajlepsza wartość straty: 0.049854\tDokładność: 98.63%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.062952\tNajlepsza wartość straty: 0.049854\tDokładność: 98.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.066961\tNajlepsza wartość straty: 0.049854\tDokładność: 98.71%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.069544\tNajlepsza wartość straty: 0.049854\tDokładność: 98.12%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.097761\tNajlepsza wartość straty: 0.049854\tDokładność: 98.48%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.071329\tNajlepsza wartość straty: 0.049854\tDokładność: 98.67%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.077133\tNajlepsza wartość straty: 0.049854\tDokładność: 98.59%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.089784\tNajlepsza wartość straty: 0.049854\tDokładność: 98.63%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.072917\tNajlepsza wartość straty: 0.049854\tDokładność: 98.79%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.075092\tNajlepsza wartość straty: 0.049854\tDokładność: 98.87%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.071347\tNajlepsza wartość straty: 0.049854\tDokładność: 98.83%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.083931\tNajlepsza wartość straty: 0.049854\tDokładność: 98.75%\n",
      "28\tF. straty dla zbioru walidacyjnego: 0.069430\tNajlepsza wartość straty: 0.049854\tDokładność: 98.71%\n",
      "29\tF. straty dla zbioru walidacyjnego: 0.081895\tNajlepsza wartość straty: 0.049854\tDokładność: 98.51%\n",
      "30\tF. straty dla zbioru walidacyjnego: 0.069283\tNajlepsza wartość straty: 0.049854\tDokładność: 98.75%\n",
      "31\tF. straty dla zbioru walidacyjnego: 0.078438\tNajlepsza wartość straty: 0.049854\tDokładność: 98.83%\n",
      "32\tF. straty dla zbioru walidacyjnego: 0.081701\tNajlepsza wartość straty: 0.049854\tDokładność: 98.79%\n",
      "33\tF. straty dla zbioru walidacyjnego: 0.073133\tNajlepsza wartość straty: 0.049854\tDokładność: 98.87%\n",
      "34\tF. straty dla zbioru walidacyjnego: 0.057064\tNajlepsza wartość straty: 0.049854\tDokładność: 99.10%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  24.1s\n",
      "[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.115785\tNajlepsza wartość straty: 0.115785\tDokładność: 96.21%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.073724\tNajlepsza wartość straty: 0.073724\tDokładność: 97.93%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.062511\tNajlepsza wartość straty: 0.062511\tDokładność: 97.97%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.058065\tNajlepsza wartość straty: 0.058065\tDokładność: 98.16%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.058361\tNajlepsza wartość straty: 0.058065\tDokładność: 98.05%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.046347\tNajlepsza wartość straty: 0.046347\tDokładność: 98.67%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.038877\tNajlepsza wartość straty: 0.038877\tDokładność: 98.67%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.062876\tNajlepsza wartość straty: 0.038877\tDokładność: 98.12%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.063844\tNajlepsza wartość straty: 0.038877\tDokładność: 98.40%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.060789\tNajlepsza wartość straty: 0.038877\tDokładność: 98.67%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.061298\tNajlepsza wartość straty: 0.038877\tDokładność: 98.71%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.067748\tNajlepsza wartość straty: 0.038877\tDokładność: 98.67%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.057786\tNajlepsza wartość straty: 0.038877\tDokładność: 98.59%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.049524\tNajlepsza wartość straty: 0.038877\tDokładność: 98.83%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.061521\tNajlepsza wartość straty: 0.038877\tDokładność: 98.75%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.046064\tNajlepsza wartość straty: 0.038877\tDokładność: 98.75%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.047777\tNajlepsza wartość straty: 0.038877\tDokładność: 98.87%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.052571\tNajlepsza wartość straty: 0.038877\tDokładność: 98.55%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.041696\tNajlepsza wartość straty: 0.038877\tDokładność: 99.14%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.052528\tNajlepsza wartość straty: 0.038877\tDokładność: 98.79%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.066619\tNajlepsza wartość straty: 0.038877\tDokładność: 98.79%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.073735\tNajlepsza wartość straty: 0.038877\tDokładność: 98.63%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.050141\tNajlepsza wartość straty: 0.038877\tDokładność: 98.87%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.063104\tNajlepsza wartość straty: 0.038877\tDokładność: 98.91%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.055084\tNajlepsza wartość straty: 0.038877\tDokładność: 98.83%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.065965\tNajlepsza wartość straty: 0.038877\tDokładność: 98.91%\n",
      "26\tF. straty dla zbioru walidacyjnego: 0.067330\tNajlepsza wartość straty: 0.038877\tDokładność: 98.75%\n",
      "27\tF. straty dla zbioru walidacyjnego: 0.077081\tNajlepsza wartość straty: 0.038877\tDokładność: 98.75%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  18.7s\n",
      "[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.118380\tNajlepsza wartość straty: 0.118380\tDokładność: 96.21%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.069706\tNajlepsza wartość straty: 0.069706\tDokładność: 97.93%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.063211\tNajlepsza wartość straty: 0.063211\tDokładność: 97.81%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.060138\tNajlepsza wartość straty: 0.060138\tDokładność: 98.32%\n",
      "4\tF. straty dla zbioru walidacyjnego: 0.040684\tNajlepsza wartość straty: 0.040684\tDokładność: 98.67%\n",
      "5\tF. straty dla zbioru walidacyjnego: 0.053982\tNajlepsza wartość straty: 0.040684\tDokładność: 98.59%\n",
      "6\tF. straty dla zbioru walidacyjnego: 0.049696\tNajlepsza wartość straty: 0.040684\tDokładność: 98.44%\n",
      "7\tF. straty dla zbioru walidacyjnego: 0.046630\tNajlepsza wartość straty: 0.040684\tDokładność: 98.44%\n",
      "8\tF. straty dla zbioru walidacyjnego: 0.075689\tNajlepsza wartość straty: 0.040684\tDokładność: 98.16%\n",
      "9\tF. straty dla zbioru walidacyjnego: 0.053539\tNajlepsza wartość straty: 0.040684\tDokładność: 98.44%\n",
      "10\tF. straty dla zbioru walidacyjnego: 0.053265\tNajlepsza wartość straty: 0.040684\tDokładność: 98.48%\n",
      "11\tF. straty dla zbioru walidacyjnego: 0.051157\tNajlepsza wartość straty: 0.040684\tDokładność: 98.51%\n",
      "12\tF. straty dla zbioru walidacyjnego: 0.056588\tNajlepsza wartość straty: 0.040684\tDokładność: 98.83%\n",
      "13\tF. straty dla zbioru walidacyjnego: 0.058253\tNajlepsza wartość straty: 0.040684\tDokładność: 98.79%\n",
      "14\tF. straty dla zbioru walidacyjnego: 0.058207\tNajlepsza wartość straty: 0.040684\tDokładność: 98.79%\n",
      "15\tF. straty dla zbioru walidacyjnego: 0.058027\tNajlepsza wartość straty: 0.040684\tDokładność: 98.75%\n",
      "16\tF. straty dla zbioru walidacyjnego: 0.077479\tNajlepsza wartość straty: 0.040684\tDokładność: 99.02%\n",
      "17\tF. straty dla zbioru walidacyjnego: 0.080033\tNajlepsza wartość straty: 0.040684\tDokładność: 98.24%\n",
      "18\tF. straty dla zbioru walidacyjnego: 0.065103\tNajlepsza wartość straty: 0.040684\tDokładność: 98.36%\n",
      "19\tF. straty dla zbioru walidacyjnego: 0.096554\tNajlepsza wartość straty: 0.040684\tDokładność: 98.48%\n",
      "20\tF. straty dla zbioru walidacyjnego: 0.050517\tNajlepsza wartość straty: 0.040684\tDokładność: 98.87%\n",
      "21\tF. straty dla zbioru walidacyjnego: 0.067264\tNajlepsza wartość straty: 0.040684\tDokładność: 98.91%\n",
      "22\tF. straty dla zbioru walidacyjnego: 0.062312\tNajlepsza wartość straty: 0.040684\tDokładność: 98.91%\n",
      "23\tF. straty dla zbioru walidacyjnego: 0.084345\tNajlepsza wartość straty: 0.040684\tDokładność: 98.20%\n",
      "24\tF. straty dla zbioru walidacyjnego: 0.062215\tNajlepsza wartość straty: 0.040684\tDokładność: 98.91%\n",
      "25\tF. straty dla zbioru walidacyjnego: 0.075490\tNajlepsza wartość straty: 0.040684\tDokładność: 98.75%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B2035FA60>, total=  17.6s\n",
      "[CV] n_neurons=30, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.800856\tNajlepsza wartość straty: 1.800856\tDokładność: 18.73%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.635771\tNajlepsza wartość straty: 1.635771\tDokładność: 20.91%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.738793\tNajlepsza wartość straty: 1.635771\tDokładność: 18.73%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.666945\tNajlepsza wartość straty: 1.635771\tDokładność: 19.27%\n",
      "4\tF. straty dla zbioru walidacyjnego: 1.640410\tNajlepsza wartość straty: 1.635771\tDokładność: 22.01%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.907916\tNajlepsza wartość straty: 1.635771\tDokładność: 19.08%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.861140\tNajlepsza wartość straty: 1.635771\tDokładność: 22.01%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.646459\tNajlepsza wartość straty: 1.635771\tDokładność: 19.27%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.682895\tNajlepsza wartość straty: 1.635771\tDokładność: 22.01%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.800018\tNajlepsza wartość straty: 1.635771\tDokładność: 18.73%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.787576\tNajlepsza wartość straty: 1.635771\tDokładność: 22.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.982576\tNajlepsza wartość straty: 1.635771\tDokładność: 22.01%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.943088\tNajlepsza wartość straty: 1.635771\tDokładność: 20.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.777600\tNajlepsza wartość straty: 1.635771\tDokładność: 19.27%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.791733\tNajlepsza wartość straty: 1.635771\tDokładność: 19.08%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.913679\tNajlepsza wartość straty: 1.635771\tDokładność: 22.01%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.850441\tNajlepsza wartość straty: 1.635771\tDokładność: 19.08%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.676430\tNajlepsza wartość straty: 1.635771\tDokładność: 20.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 1.803301\tNajlepsza wartość straty: 1.635771\tDokładność: 22.01%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.727329\tNajlepsza wartość straty: 1.635771\tDokładność: 22.01%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.709788\tNajlepsza wartość straty: 1.635771\tDokładność: 18.73%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.744463\tNajlepsza wartość straty: 1.635771\tDokładność: 18.73%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.651262\tNajlepsza wartość straty: 1.635771\tDokładność: 20.91%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total=  30.8s\n",
      "[CV] n_neurons=30, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.315558\tNajlepsza wartość straty: 1.315558\tDokładność: 42.06%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.616300\tNajlepsza wartość straty: 1.315558\tDokładność: 19.08%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.993598\tNajlepsza wartość straty: 1.315558\tDokładność: 19.08%\n",
      "3\tF. straty dla zbioru walidacyjnego: 2.080343\tNajlepsza wartość straty: 1.315558\tDokładność: 19.27%\n",
      "4\tF. straty dla zbioru walidacyjnego: 2.255693\tNajlepsza wartość straty: 1.315558\tDokładność: 19.08%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.676018\tNajlepsza wartość straty: 1.315558\tDokładność: 22.01%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.621794\tNajlepsza wartość straty: 1.315558\tDokładność: 19.27%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.678816\tNajlepsza wartość straty: 1.315558\tDokładność: 18.73%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.846662\tNajlepsza wartość straty: 1.315558\tDokładność: 20.91%\n",
      "9\tF. straty dla zbioru walidacyjnego: 1.905608\tNajlepsza wartość straty: 1.315558\tDokładność: 22.01%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.684334\tNajlepsza wartość straty: 1.315558\tDokładność: 22.01%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.647433\tNajlepsza wartość straty: 1.315558\tDokładność: 20.91%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.853975\tNajlepsza wartość straty: 1.315558\tDokładność: 20.91%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.669575\tNajlepsza wartość straty: 1.315558\tDokładność: 22.01%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.703588\tNajlepsza wartość straty: 1.315558\tDokładność: 19.27%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.998330\tNajlepsza wartość straty: 1.315558\tDokładność: 19.08%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.656304\tNajlepsza wartość straty: 1.315558\tDokładność: 20.91%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.947683\tNajlepsza wartość straty: 1.315558\tDokładność: 20.91%\n",
      "18\tF. straty dla zbioru walidacyjnego: 2.143590\tNajlepsza wartość straty: 1.315558\tDokładność: 20.91%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.900221\tNajlepsza wartość straty: 1.315558\tDokładność: 20.91%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.930351\tNajlepsza wartość straty: 1.315558\tDokładność: 18.73%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.900989\tNajlepsza wartość straty: 1.315558\tDokładność: 22.01%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total=  29.9s\n",
      "[CV] n_neurons=30, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8> \n",
      "0\tF. straty dla zbioru walidacyjnego: 1.644132\tNajlepsza wartość straty: 1.644132\tDokładność: 19.27%\n",
      "1\tF. straty dla zbioru walidacyjnego: 1.879338\tNajlepsza wartość straty: 1.644132\tDokładność: 19.08%\n",
      "2\tF. straty dla zbioru walidacyjnego: 1.736940\tNajlepsza wartość straty: 1.644132\tDokładność: 18.73%\n",
      "3\tF. straty dla zbioru walidacyjnego: 1.995339\tNajlepsza wartość straty: 1.644132\tDokładność: 19.08%\n",
      "4\tF. straty dla zbioru walidacyjnego: 2.141521\tNajlepsza wartość straty: 1.644132\tDokładność: 19.08%\n",
      "5\tF. straty dla zbioru walidacyjnego: 1.850739\tNajlepsza wartość straty: 1.644132\tDokładność: 19.27%\n",
      "6\tF. straty dla zbioru walidacyjnego: 1.746686\tNajlepsza wartość straty: 1.644132\tDokładność: 19.08%\n",
      "7\tF. straty dla zbioru walidacyjnego: 1.830477\tNajlepsza wartość straty: 1.644132\tDokładność: 18.73%\n",
      "8\tF. straty dla zbioru walidacyjnego: 1.649748\tNajlepsza wartość straty: 1.644132\tDokładność: 22.01%\n",
      "9\tF. straty dla zbioru walidacyjnego: 2.048921\tNajlepsza wartość straty: 1.644132\tDokładność: 22.01%\n",
      "10\tF. straty dla zbioru walidacyjnego: 1.832471\tNajlepsza wartość straty: 1.644132\tDokładność: 18.73%\n",
      "11\tF. straty dla zbioru walidacyjnego: 1.816108\tNajlepsza wartość straty: 1.644132\tDokładność: 19.27%\n",
      "12\tF. straty dla zbioru walidacyjnego: 1.836795\tNajlepsza wartość straty: 1.644132\tDokładność: 18.73%\n",
      "13\tF. straty dla zbioru walidacyjnego: 1.796181\tNajlepsza wartość straty: 1.644132\tDokładność: 19.08%\n",
      "14\tF. straty dla zbioru walidacyjnego: 1.792563\tNajlepsza wartość straty: 1.644132\tDokładność: 22.01%\n",
      "15\tF. straty dla zbioru walidacyjnego: 1.879795\tNajlepsza wartość straty: 1.644132\tDokładność: 20.91%\n",
      "16\tF. straty dla zbioru walidacyjnego: 1.704591\tNajlepsza wartość straty: 1.644132\tDokładność: 18.73%\n",
      "17\tF. straty dla zbioru walidacyjnego: 1.803577\tNajlepsza wartość straty: 1.644132\tDokładność: 19.08%\n",
      "18\tF. straty dla zbioru walidacyjnego: 2.113103\tNajlepsza wartość straty: 1.644132\tDokładność: 19.08%\n",
      "19\tF. straty dla zbioru walidacyjnego: 1.628989\tNajlepsza wartość straty: 1.628989\tDokładność: 19.27%\n",
      "20\tF. straty dla zbioru walidacyjnego: 1.801372\tNajlepsza wartość straty: 1.628989\tDokładność: 19.27%\n",
      "21\tF. straty dla zbioru walidacyjnego: 1.647705\tNajlepsza wartość straty: 1.628989\tDokładność: 22.01%\n",
      "22\tF. straty dla zbioru walidacyjnego: 1.653069\tNajlepsza wartość straty: 1.628989\tDokładność: 20.91%\n",
      "23\tF. straty dla zbioru walidacyjnego: 1.739611\tNajlepsza wartość straty: 1.628989\tDokładność: 22.01%\n",
      "24\tF. straty dla zbioru walidacyjnego: 1.861976\tNajlepsza wartość straty: 1.628989\tDokładność: 22.01%\n",
      "25\tF. straty dla zbioru walidacyjnego: 1.744551\tNajlepsza wartość straty: 1.628989\tDokładność: 19.27%\n",
      "26\tF. straty dla zbioru walidacyjnego: 1.719069\tNajlepsza wartość straty: 1.628989\tDokładność: 19.08%\n",
      "27\tF. straty dla zbioru walidacyjnego: 1.685107\tNajlepsza wartość straty: 1.628989\tDokładność: 22.01%\n",
      "28\tF. straty dla zbioru walidacyjnego: 2.323926\tNajlepsza wartość straty: 1.628989\tDokładność: 19.08%\n",
      "29\tF. straty dla zbioru walidacyjnego: 2.050607\tNajlepsza wartość straty: 1.628989\tDokładność: 18.73%\n",
      "30\tF. straty dla zbioru walidacyjnego: 1.854667\tNajlepsza wartość straty: 1.628989\tDokładność: 19.27%\n",
      "31\tF. straty dla zbioru walidacyjnego: 1.956275\tNajlepsza wartość straty: 1.628989\tDokładność: 18.73%\n",
      "32\tF. straty dla zbioru walidacyjnego: 1.666343\tNajlepsza wartość straty: 1.628989\tDokładność: 19.08%\n",
      "33\tF. straty dla zbioru walidacyjnego: 1.684592\tNajlepsza wartość straty: 1.628989\tDokładność: 20.91%\n",
      "34\tF. straty dla zbioru walidacyjnego: 1.747721\tNajlepsza wartość straty: 1.628989\tDokładność: 19.08%\n",
      "35\tF. straty dla zbioru walidacyjnego: 1.761388\tNajlepsza wartość straty: 1.628989\tDokładność: 22.01%\n",
      "36\tF. straty dla zbioru walidacyjnego: 1.971677\tNajlepsza wartość straty: 1.628989\tDokładność: 22.01%\n",
      "37\tF. straty dla zbioru walidacyjnego: 1.921869\tNajlepsza wartość straty: 1.628989\tDokładność: 22.01%\n",
      "38\tF. straty dla zbioru walidacyjnego: 1.989990\tNajlepsza wartość straty: 1.628989\tDokładność: 18.73%\n",
      "39\tF. straty dla zbioru walidacyjnego: 1.904663\tNajlepsza wartość straty: 1.628989\tDokładność: 19.08%\n",
      "40\tF. straty dla zbioru walidacyjnego: 1.730792\tNajlepsza wartość straty: 1.628989\tDokładność: 19.08%\n",
      "Wczesne zatrzymywanie!\n",
      "[CV]  n_neurons=30, learning_rate=0.05, batch_size=10, activation=<function elu at 0x0000029B128289D8>, total=  54.3s\n",
      "[CV] n_neurons=100, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x0000029B18054EA0> \n",
      "0\tF. straty dla zbioru walidacyjnego: 0.303190\tNajlepsza wartość straty: 0.303190\tDokładność: 91.52%\n",
      "1\tF. straty dla zbioru walidacyjnego: 0.158832\tNajlepsza wartość straty: 0.158832\tDokładność: 95.35%\n",
      "2\tF. straty dla zbioru walidacyjnego: 0.156829\tNajlepsza wartość straty: 0.156829\tDokładność: 96.60%\n",
      "3\tF. straty dla zbioru walidacyjnego: 0.119779\tNajlepsza wartość straty: 0.119779\tDokładność: 96.72%\n",
      "4\tF. straty dla zbioru walidacyjnego: 17041.654297\tNajlepsza wartość straty: 0.119779\tDokładność: 22.13%\n",
      "5\tF. straty dla zbioru walidacyjnego: 360.686768\tNajlepsza wartość straty: 0.119779\tDokładność: 53.21%\n",
      "6\tF. straty dla zbioru walidacyjnego: 74.655258\tNajlepsza wartość straty: 0.119779\tDokładność: 67.87%\n",
      "7\tF. straty dla zbioru walidacyjnego: 57.351776\tNajlepsza wartość straty: 0.119779\tDokładność: 71.97%\n",
      "8\tF. straty dla zbioru walidacyjnego: 41.824642\tNajlepsza wartość straty: 0.119779\tDokładność: 81.55%\n",
      "9\tF. straty dla zbioru walidacyjnego: 84.618233\tNajlepsza wartość straty: 0.119779\tDokładność: 69.74%\n",
      "10\tF. straty dla zbioru walidacyjnego: 38.044308\tNajlepsza wartość straty: 0.119779\tDokładność: 80.34%\n",
      "11\tF. straty dla zbioru walidacyjnego: 32.051949\tNajlepsza wartość straty: 0.119779\tDokładność: 85.18%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # moglibyśmy również sprawdzić inne wartości warstw ukrytych, różne optymalizatory itd.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n",
    "                                random_state=42, verbose=2)\n",
    "rnd_search.fit(X_train1, y_train1)\n",
    "\n",
    "# argument fit_params jako część konstruktora został uznany za przestarzały w wersji 0.19 modułu Scikit-Learn i\n",
    "# będzie usunięty w wersji 0.21. Należy wtedy przekazać parametry dopasowania metodzie fit():\n",
    "# rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "#                                 random_state=42, verbose=2)\n",
    "# fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000}\n",
    "# rnd_search.fit(X_train1, y_train1, **fit_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': <function __main__.leaky_relu.<locals>.parametrized_leaky_relu>,\n",
       " 'batch_size': 500,\n",
       " 'learning_rate': 0.01,\n",
       " 'n_neurons': 140}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99318933644677954"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rnd_search.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cudownie! Dostrojenie hiperparametrów pozwoliło nam osiągnąć dokładność rzędu 99,32%! Być może nie wygląda to na dużą różnicę, pamiętaj jednak o stopie błędu: zmalała z 2% do 0,7%. Jest redukcja liczby błędów generowanych przez ten model o 65%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobrym pomysłem jest również zachowanie tego modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnd_search.best_estimator_.save(\"./moj_najlepszy_model_mnist_0_do_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ćwiczenie: Spróbuj teraz dodać normalizację wsadową i porównać uzyskane krzywe uczenia: czy model staje się teraz zbieżny szybciej? Czy dokładność modelu jest lepsza?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeszcze razy wyuczmy najlepszy znaleziony model, aby sprawdzić, jak szybko uzyska zbieżność (ewentualnie możesz zmodyfikowac powyższy kod, aby generował podsumowania narzędzia TensorBoard, dzięki czemu moglibyśmy zwizualizować krzywą uczenia): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.090732\tBest loss: 0.090732\tAccuracy: 97.22%\n",
      "1\tValidation loss: 0.052198\tBest loss: 0.052198\tAccuracy: 98.40%\n",
      "2\tValidation loss: 0.040040\tBest loss: 0.040040\tAccuracy: 98.94%\n",
      "3\tValidation loss: 0.057495\tBest loss: 0.040040\tAccuracy: 98.55%\n",
      "4\tValidation loss: 0.045600\tBest loss: 0.040040\tAccuracy: 98.75%\n",
      "5\tValidation loss: 0.062344\tBest loss: 0.040040\tAccuracy: 98.48%\n",
      "6\tValidation loss: 0.048719\tBest loss: 0.040040\tAccuracy: 98.67%\n",
      "7\tValidation loss: 0.050346\tBest loss: 0.040040\tAccuracy: 98.79%\n",
      "8\tValidation loss: 0.051224\tBest loss: 0.040040\tAccuracy: 98.79%\n",
      "9\tValidation loss: 0.036505\tBest loss: 0.036505\tAccuracy: 98.98%\n",
      "10\tValidation loss: 0.052532\tBest loss: 0.036505\tAccuracy: 98.71%\n",
      "11\tValidation loss: 0.057086\tBest loss: 0.036505\tAccuracy: 99.10%\n",
      "12\tValidation loss: 0.036754\tBest loss: 0.036505\tAccuracy: 99.06%\n",
      "13\tValidation loss: 0.046782\tBest loss: 0.036505\tAccuracy: 98.87%\n",
      "14\tValidation loss: 0.048929\tBest loss: 0.036505\tAccuracy: 98.91%\n",
      "15\tValidation loss: 0.052919\tBest loss: 0.036505\tAccuracy: 98.75%\n",
      "16\tValidation loss: 0.054287\tBest loss: 0.036505\tAccuracy: 98.67%\n",
      "17\tValidation loss: 0.047722\tBest loss: 0.036505\tAccuracy: 98.79%\n",
      "18\tValidation loss: 0.040474\tBest loss: 0.036505\tAccuracy: 99.14%\n",
      "19\tValidation loss: 0.033867\tBest loss: 0.033867\tAccuracy: 99.14%\n",
      "20\tValidation loss: 0.046808\tBest loss: 0.033867\tAccuracy: 98.83%\n",
      "21\tValidation loss: 0.052966\tBest loss: 0.033867\tAccuracy: 98.91%\n",
      "22\tValidation loss: 0.095892\tBest loss: 0.033867\tAccuracy: 98.08%\n",
      "23\tValidation loss: 0.054250\tBest loss: 0.033867\tAccuracy: 98.87%\n",
      "24\tValidation loss: 0.061026\tBest loss: 0.033867\tAccuracy: 98.87%\n",
      "25\tValidation loss: 0.081977\tBest loss: 0.033867\tAccuracy: 98.67%\n",
      "26\tValidation loss: 0.079819\tBest loss: 0.033867\tAccuracy: 98.71%\n",
      "27\tValidation loss: 0.059824\tBest loss: 0.033867\tAccuracy: 98.75%\n",
      "28\tValidation loss: 0.057758\tBest loss: 0.033867\tAccuracy: 98.94%\n",
      "29\tValidation loss: 0.087165\tBest loss: 0.033867\tAccuracy: 98.91%\n",
      "30\tValidation loss: 0.052274\tBest loss: 0.033867\tAccuracy: 99.10%\n",
      "31\tValidation loss: 0.059831\tBest loss: 0.033867\tAccuracy: 98.79%\n",
      "32\tValidation loss: 0.054240\tBest loss: 0.033867\tAccuracy: 98.91%\n",
      "33\tValidation loss: 0.048165\tBest loss: 0.033867\tAccuracy: 98.94%\n",
      "34\tValidation loss: 0.040565\tBest loss: 0.033867\tAccuracy: 99.18%\n",
      "35\tValidation loss: 0.103207\tBest loss: 0.033867\tAccuracy: 98.28%\n",
      "36\tValidation loss: 400.716797\tBest loss: 0.033867\tAccuracy: 71.46%\n",
      "37\tValidation loss: 11.996887\tBest loss: 0.033867\tAccuracy: 96.09%\n",
      "38\tValidation loss: 2.623182\tBest loss: 0.033867\tAccuracy: 96.56%\n",
      "39\tValidation loss: 1.344962\tBest loss: 0.033867\tAccuracy: 97.69%\n",
      "40\tValidation loss: 1.125381\tBest loss: 0.033867\tAccuracy: 97.42%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7fd9d19e37b8>,\n",
       "       batch_norm_momentum=None, batch_size=500, dropout_rate=None,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7fd9d5e628c8>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=140,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                        n_neurons=140, random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepsza wartość funkcji straty wystąpiła w epoce 19., ale już w epoce 9. uzyskaliśmy wynik gorszy o zaledwie 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy, czy rzeczywiście uzyskujemy 99,32% dokładności wobec zbioru testowego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99318933644677954"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobrze, użyjmy teraz dokładnie tego samego modelu, tym razem jednak wraz z normalizacją wsadową:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.046053\tBest loss: 0.046053\tAccuracy: 98.67%\n",
      "1\tValidation loss: 0.032228\tBest loss: 0.032228\tAccuracy: 98.83%\n",
      "2\tValidation loss: 0.032974\tBest loss: 0.032228\tAccuracy: 98.83%\n",
      "3\tValidation loss: 0.035961\tBest loss: 0.032228\tAccuracy: 98.94%\n",
      "4\tValidation loss: 0.040250\tBest loss: 0.032228\tAccuracy: 98.94%\n",
      "5\tValidation loss: 0.033051\tBest loss: 0.032228\tAccuracy: 99.06%\n",
      "6\tValidation loss: 0.056053\tBest loss: 0.032228\tAccuracy: 98.32%\n",
      "7\tValidation loss: 0.031729\tBest loss: 0.031729\tAccuracy: 99.18%\n",
      "8\tValidation loss: 0.027662\tBest loss: 0.027662\tAccuracy: 99.26%\n",
      "9\tValidation loss: 0.034074\tBest loss: 0.027662\tAccuracy: 98.94%\n",
      "10\tValidation loss: 0.032173\tBest loss: 0.027662\tAccuracy: 99.06%\n",
      "11\tValidation loss: 0.030538\tBest loss: 0.027662\tAccuracy: 99.10%\n",
      "12\tValidation loss: 0.030337\tBest loss: 0.027662\tAccuracy: 99.10%\n",
      "13\tValidation loss: 0.022219\tBest loss: 0.022219\tAccuracy: 99.45%\n",
      "14\tValidation loss: 0.036824\tBest loss: 0.022219\tAccuracy: 99.14%\n",
      "15\tValidation loss: 0.033945\tBest loss: 0.022219\tAccuracy: 99.18%\n",
      "16\tValidation loss: 0.032533\tBest loss: 0.022219\tAccuracy: 98.98%\n",
      "17\tValidation loss: 0.037204\tBest loss: 0.022219\tAccuracy: 99.02%\n",
      "18\tValidation loss: 0.026982\tBest loss: 0.022219\tAccuracy: 99.34%\n",
      "19\tValidation loss: 0.022094\tBest loss: 0.022094\tAccuracy: 99.53%\n",
      "20\tValidation loss: 0.026196\tBest loss: 0.022094\tAccuracy: 99.26%\n",
      "21\tValidation loss: 0.022107\tBest loss: 0.022094\tAccuracy: 99.49%\n",
      "22\tValidation loss: 0.021436\tBest loss: 0.021436\tAccuracy: 99.53%\n",
      "23\tValidation loss: 0.025607\tBest loss: 0.021436\tAccuracy: 99.37%\n",
      "24\tValidation loss: 0.038882\tBest loss: 0.021436\tAccuracy: 99.22%\n",
      "25\tValidation loss: 0.032011\tBest loss: 0.021436\tAccuracy: 99.26%\n",
      "26\tValidation loss: 0.027673\tBest loss: 0.021436\tAccuracy: 99.22%\n",
      "27\tValidation loss: 0.026874\tBest loss: 0.021436\tAccuracy: 99.30%\n",
      "28\tValidation loss: 0.021123\tBest loss: 0.021123\tAccuracy: 99.41%\n",
      "29\tValidation loss: 0.024784\tBest loss: 0.021123\tAccuracy: 99.45%\n",
      "30\tValidation loss: 0.024108\tBest loss: 0.021123\tAccuracy: 99.49%\n",
      "31\tValidation loss: 0.028439\tBest loss: 0.021123\tAccuracy: 99.37%\n",
      "32\tValidation loss: 0.032366\tBest loss: 0.021123\tAccuracy: 99.22%\n",
      "33\tValidation loss: 0.037057\tBest loss: 0.021123\tAccuracy: 99.18%\n",
      "34\tValidation loss: 0.042305\tBest loss: 0.021123\tAccuracy: 98.98%\n",
      "35\tValidation loss: 0.039662\tBest loss: 0.021123\tAccuracy: 99.14%\n",
      "36\tValidation loss: 0.036299\tBest loss: 0.021123\tAccuracy: 99.14%\n",
      "37\tValidation loss: 0.026997\tBest loss: 0.021123\tAccuracy: 99.53%\n",
      "38\tValidation loss: 0.034407\tBest loss: 0.021123\tAccuracy: 99.22%\n",
      "39\tValidation loss: 0.027668\tBest loss: 0.021123\tAccuracy: 99.41%\n",
      "40\tValidation loss: 0.029128\tBest loss: 0.021123\tAccuracy: 99.30%\n",
      "41\tValidation loss: 0.033564\tBest loss: 0.021123\tAccuracy: 99.14%\n",
      "42\tValidation loss: 0.033810\tBest loss: 0.021123\tAccuracy: 99.30%\n",
      "43\tValidation loss: 0.044953\tBest loss: 0.021123\tAccuracy: 98.98%\n",
      "44\tValidation loss: 0.026280\tBest loss: 0.021123\tAccuracy: 99.26%\n",
      "45\tValidation loss: 0.020275\tBest loss: 0.020275\tAccuracy: 99.61%\n",
      "46\tValidation loss: 0.018810\tBest loss: 0.018810\tAccuracy: 99.45%\n",
      "47\tValidation loss: 0.027529\tBest loss: 0.018810\tAccuracy: 99.18%\n",
      "48\tValidation loss: 0.018120\tBest loss: 0.018120\tAccuracy: 99.53%\n",
      "49\tValidation loss: 0.019378\tBest loss: 0.018120\tAccuracy: 99.45%\n",
      "50\tValidation loss: 0.029760\tBest loss: 0.018120\tAccuracy: 99.34%\n",
      "51\tValidation loss: 0.035702\tBest loss: 0.018120\tAccuracy: 99.26%\n",
      "52\tValidation loss: 0.032662\tBest loss: 0.018120\tAccuracy: 99.02%\n",
      "53\tValidation loss: 0.026943\tBest loss: 0.018120\tAccuracy: 99.37%\n",
      "54\tValidation loss: 0.029007\tBest loss: 0.018120\tAccuracy: 99.53%\n",
      "55\tValidation loss: 0.021956\tBest loss: 0.018120\tAccuracy: 99.49%\n",
      "56\tValidation loss: 0.018983\tBest loss: 0.018120\tAccuracy: 99.61%\n",
      "57\tValidation loss: 0.022788\tBest loss: 0.018120\tAccuracy: 99.49%\n",
      "58\tValidation loss: 0.019578\tBest loss: 0.018120\tAccuracy: 99.61%\n",
      "59\tValidation loss: 0.021676\tBest loss: 0.018120\tAccuracy: 99.61%\n",
      "60\tValidation loss: 0.021580\tBest loss: 0.018120\tAccuracy: 99.65%\n",
      "61\tValidation loss: 0.021467\tBest loss: 0.018120\tAccuracy: 99.65%\n",
      "62\tValidation loss: 0.020513\tBest loss: 0.018120\tAccuracy: 99.65%\n",
      "63\tValidation loss: 0.020252\tBest loss: 0.018120\tAccuracy: 99.65%\n",
      "64\tValidation loss: 0.021724\tBest loss: 0.018120\tAccuracy: 99.65%\n",
      "65\tValidation loss: 0.021499\tBest loss: 0.018120\tAccuracy: 99.69%\n",
      "66\tValidation loss: 0.021627\tBest loss: 0.018120\tAccuracy: 99.69%\n",
      "67\tValidation loss: 0.021569\tBest loss: 0.018120\tAccuracy: 99.69%\n",
      "68\tValidation loss: 0.021727\tBest loss: 0.018120\tAccuracy: 99.69%\n",
      "69\tValidation loss: 0.021104\tBest loss: 0.018120\tAccuracy: 99.69%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7fd9d19e3c80>,\n",
       "       batch_norm_momentum=0.95, batch_size=500, dropout_rate=None,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7fd9d5e628c8>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                           n_neurons=90, random_state=42,\n",
    "                           batch_norm_momentum=0.95)\n",
    "dnn_clf_bn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepsze parametry są znalezione w epoce 48.; uzyskujemy tu zbieżność wolniej niż poprzednio. Sprawdźmy dokładność:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99241097489784003"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cóż, normalizacja wsadowa nie poprawiła dokładności. Sprawdźmy, czy możemy znaleźć dobry zestaw hiperparametrów współpracujących z techniką normalizacji wsadowej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV] activation=<function relu at 0x7fd9e8a660d0>, n_neurons=70, learning_rate=0.01, batch_norm_momentum=0.99, batch_size=50 \n",
      "0\tValidation loss: 0.113224\tBest loss: 0.113224\tAccuracy: 97.30%\n",
      "1\tValidation loss: 0.064190\tBest loss: 0.064190\tAccuracy: 98.24%\n",
      "2\tValidation loss: 0.080173\tBest loss: 0.064190\tAccuracy: 98.28%\n",
      "3\tValidation loss: 0.059603\tBest loss: 0.059603\tAccuracy: 98.28%\n",
      "4\tValidation loss: 0.043533\tBest loss: 0.043533\tAccuracy: 98.48%\n",
      "5\tValidation loss: 0.040107\tBest loss: 0.040107\tAccuracy: 98.87%\n",
      "6\tValidation loss: 0.051212\tBest loss: 0.040107\tAccuracy: 98.24%\n",
      "7\tValidation loss: 0.046029\tBest loss: 0.040107\tAccuracy: 98.71%\n",
      "8\tValidation loss: 0.053079\tBest loss: 0.040107\tAccuracy: 98.59%\n",
      "9\tValidation loss: 0.066891\tBest loss: 0.040107\tAccuracy: 98.28%\n",
      "10\tValidation loss: 0.037712\tBest loss: 0.037712\tAccuracy: 98.83%\n",
      "11\tValidation loss: 0.055569\tBest loss: 0.037712\tAccuracy: 98.55%\n",
      "12\tValidation loss: 0.040949\tBest loss: 0.037712\tAccuracy: 98.98%\n",
      "13\tValidation loss: 0.077433\tBest loss: 0.037712\tAccuracy: 98.36%\n",
      "14\tValidation loss: 0.065955\tBest loss: 0.037712\tAccuracy: 98.63%\n",
      "15\tValidation loss: 0.038968\tBest loss: 0.037712\tAccuracy: 99.02%\n",
      "16\tValidation loss: 0.039190\tBest loss: 0.037712\tAccuracy: 99.06%\n",
      "17\tValidation loss: 0.050690\tBest loss: 0.037712\tAccuracy: 98.71%\n",
      "18\tValidation loss: 0.043054\tBest loss: 0.037712\tAccuracy: 99.02%\n",
      "19\tValidation loss: 0.063156\tBest loss: 0.037712\tAccuracy: 98.71%\n",
      "20\tValidation loss: 0.043066\tBest loss: 0.037712\tAccuracy: 99.14%\n",
      "21\tValidation loss: 0.058145\tBest loss: 0.037712\tAccuracy: 98.79%\n",
      "22\tValidation loss: 0.039590\tBest loss: 0.037712\tAccuracy: 99.06%\n",
      "23\tValidation loss: 0.049981\tBest loss: 0.037712\tAccuracy: 98.75%\n",
      "24\tValidation loss: 0.047458\tBest loss: 0.037712\tAccuracy: 99.10%\n",
      "25\tValidation loss: 0.040638\tBest loss: 0.037712\tAccuracy: 99.06%\n",
      "26\tValidation loss: 0.041426\tBest loss: 0.037712\tAccuracy: 98.98%\n",
      "27\tValidation loss: 0.041325\tBest loss: 0.037712\tAccuracy: 98.98%\n",
      "28\tValidation loss: 0.054609\tBest loss: 0.037712\tAccuracy: 98.91%\n",
      "29\tValidation loss: 0.067671\tBest loss: 0.037712\tAccuracy: 98.75%\n",
      "30\tValidation loss: 0.037608\tBest loss: 0.037608\tAccuracy: 98.79%\n",
      "31\tValidation loss: 0.047441\tBest loss: 0.037608\tAccuracy: 98.98%\n",
      "32\tValidation loss: 0.053716\tBest loss: 0.037608\tAccuracy: 99.02%\n",
      "33\tValidation loss: 0.045445\tBest loss: 0.037608\tAccuracy: 98.83%\n",
      "34\tValidation loss: 0.046023\tBest loss: 0.037608\tAccuracy: 98.94%\n",
      "35\tValidation loss: 0.050073\tBest loss: 0.037608\tAccuracy: 98.91%\n",
      "36\tValidation loss: 0.051887\tBest loss: 0.037608\tAccuracy: 98.87%\n",
      "37\tValidation loss: 0.050272\tBest loss: 0.037608\tAccuracy: 99.02%\n",
      "38\tValidation loss: 0.043531\tBest loss: 0.037608\tAccuracy: 99.10%\n",
      "39\tValidation loss: 0.054661\tBest loss: 0.037608\tAccuracy: 98.87%\n",
      "40\tValidation loss: 0.047607\tBest loss: 0.037608\tAccuracy: 98.87%\n",
      "41\tValidation loss: 0.051862\tBest loss: 0.037608\tAccuracy: 99.14%\n",
      "42\tValidation loss: 0.044218\tBest loss: 0.037608\tAccuracy: 99.14%\n",
      "43\tValidation loss: 0.043707\tBest loss: 0.037608\tAccuracy: 99.06%\n",
      "44\tValidation loss: 0.039602\tBest loss: 0.037608\tAccuracy: 99.06%\n",
      "45\tValidation loss: 0.048998\tBest loss: 0.037608\tAccuracy: 99.02%\n",
      "46\tValidation loss: 0.045562\tBest loss: 0.037608\tAccuracy: 99.14%\n",
      "47\tValidation loss: 0.042198\tBest loss: 0.037608\tAccuracy: 99.10%\n",
      "48\tValidation loss: 0.027679\tBest loss: 0.027679\tAccuracy: 99.10%\n",
      "49\tValidation loss: 0.033783\tBest loss: 0.027679\tAccuracy: 98.94%\n",
      "50\tValidation loss: 0.032935\tBest loss: 0.027679\tAccuracy: 99.41%\n",
      "51\tValidation loss: 0.042930\tBest loss: 0.027679\tAccuracy: 98.98%\n",
      "52\tValidation loss: 0.045454\tBest loss: 0.027679\tAccuracy: 99.06%\n",
      "53\tValidation loss: 0.047336\tBest loss: 0.027679\tAccuracy: 98.91%\n",
      "54\tValidation loss: 0.036523\tBest loss: 0.027679\tAccuracy: 99.14%\n",
      "55\tValidation loss: 0.064401\tBest loss: 0.027679\tAccuracy: 98.94%\n",
      "56\tValidation loss: 0.047686\tBest loss: 0.027679\tAccuracy: 98.83%\n",
      "57\tValidation loss: 0.049083\tBest loss: 0.027679\tAccuracy: 98.98%\n",
      "58\tValidation loss: 0.057310\tBest loss: 0.027679\tAccuracy: 99.10%\n",
      "59\tValidation loss: 0.043757\tBest loss: 0.027679\tAccuracy: 99.14%\n",
      "60\tValidation loss: 0.058742\tBest loss: 0.027679\tAccuracy: 99.02%\n",
      "61\tValidation loss: 0.055049\tBest loss: 0.027679\tAccuracy: 99.06%\n",
      "62\tValidation loss: 0.039837\tBest loss: 0.027679\tAccuracy: 99.18%\n",
      "63\tValidation loss: 0.057108\tBest loss: 0.027679\tAccuracy: 99.06%\n",
      "64\tValidation loss: 0.043212\tBest loss: 0.027679\tAccuracy: 98.98%\n",
      "65\tValidation loss: 0.046874\tBest loss: 0.027679\tAccuracy: 99.18%\n",
      "66\tValidation loss: 0.052819\tBest loss: 0.027679\tAccuracy: 99.10%\n",
      "67\tValidation loss: 0.045977\tBest loss: 0.027679\tAccuracy: 99.14%\n",
      "68\tValidation loss: 0.053290\tBest loss: 0.027679\tAccuracy: 99.10%\n",
      "69\tValidation loss: 0.052941\tBest loss: 0.027679\tAccuracy: 99.06%\n",
      "Early stopping!\n",
      "[CV]  activation=<function relu at 0x7fd9e8a660d0>, n_neurons=70, learning_rate=0.01, batch_norm_momentum=0.99, batch_size=50, total= 2.7min\n",
      "[CV] activation=<function relu at 0x7fd9e8a660d0>, n_neurons=70, learning_rate=0.01, batch_norm_momentum=0.99, batch_size=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.144984\tBest loss: 0.144984\tAccuracy: 96.40%\n",
      "1\tValidation loss: 0.067873\tBest loss: 0.067873\tAccuracy: 98.44%\n",
      "2\tValidation loss: 0.091854\tBest loss: 0.067873\tAccuracy: 97.30%\n",
      "3\tValidation loss: 0.074647\tBest loss: 0.067873\tAccuracy: 98.05%\n",
      "4\tValidation loss: 0.053722\tBest loss: 0.053722\tAccuracy: 98.48%\n",
      "5\tValidation loss: 0.049216\tBest loss: 0.049216\tAccuracy: 98.44%\n",
      "6\tValidation loss: 0.057619\tBest loss: 0.049216\tAccuracy: 98.48%\n",
      "7\tValidation loss: 0.045842\tBest loss: 0.045842\tAccuracy: 98.75%\n",
      "8\tValidation loss: 0.042398\tBest loss: 0.042398\tAccuracy: 98.63%\n",
      "9\tValidation loss: 0.052629\tBest loss: 0.042398\tAccuracy: 98.63%\n",
      "10\tValidation loss: 0.056892\tBest loss: 0.042398\tAccuracy: 98.63%\n",
      "11\tValidation loss: 0.051838\tBest loss: 0.042398\tAccuracy: 98.75%\n",
      "12\tValidation loss: 0.042647\tBest loss: 0.042398\tAccuracy: 98.67%\n",
      "13\tValidation loss: 0.061297\tBest loss: 0.042398\tAccuracy: 98.59%\n",
      "14\tValidation loss: 0.049706\tBest loss: 0.042398\tAccuracy: 98.87%\n",
      "15\tValidation loss: 0.061934\tBest loss: 0.042398\tAccuracy: 98.79%\n",
      "16\tValidation loss: 0.049027\tBest loss: 0.042398\tAccuracy: 98.87%\n",
      "17\tValidation loss: 0.052187\tBest loss: 0.042398\tAccuracy: 98.79%\n",
      "18\tValidation loss: 0.052031\tBest loss: 0.042398\tAccuracy: 98.94%\n",
      "[...and much later...]\n",
      "13\tValidation loss: 0.043686\tBest loss: 0.040332\tAccuracy: 99.02%\n",
      "14\tValidation loss: 0.046940\tBest loss: 0.040332\tAccuracy: 99.18%\n",
      "15\tValidation loss: 0.045355\tBest loss: 0.040332\tAccuracy: 99.14%\n",
      "16\tValidation loss: 0.084697\tBest loss: 0.040332\tAccuracy: 98.87%\n",
      "17\tValidation loss: 0.123538\tBest loss: 0.040332\tAccuracy: 97.81%\n",
      "18\tValidation loss: 0.296928\tBest loss: 0.040332\tAccuracy: 97.50%\n",
      "19\tValidation loss: 0.053660\tBest loss: 0.040332\tAccuracy: 98.91%\n",
      "20\tValidation loss: 0.045684\tBest loss: 0.040332\tAccuracy: 98.94%\n",
      "21\tValidation loss: 0.051971\tBest loss: 0.040332\tAccuracy: 99.14%\n",
      "22\tValidation loss: 0.071830\tBest loss: 0.040332\tAccuracy: 99.06%\n",
      "23\tValidation loss: 0.069619\tBest loss: 0.040332\tAccuracy: 98.79%\n",
      "24\tValidation loss: 0.086642\tBest loss: 0.040332\tAccuracy: 98.71%\n",
      "25\tValidation loss: 0.072563\tBest loss: 0.040332\tAccuracy: 98.83%\n",
      "26\tValidation loss: 0.058974\tBest loss: 0.040332\tAccuracy: 99.06%\n",
      "27\tValidation loss: 0.048388\tBest loss: 0.040332\tAccuracy: 98.98%\n",
      "28\tValidation loss: 0.054847\tBest loss: 0.040332\tAccuracy: 99.06%\n",
      "29\tValidation loss: 0.077242\tBest loss: 0.040332\tAccuracy: 98.91%\n",
      "30\tValidation loss: 0.556978\tBest loss: 0.040332\tAccuracy: 95.54%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x7fd9e8a620d0>, n_neurons=140, learning_rate=0.05, batch_norm_momentum=0.99, batch_size=50, total= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 355.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.076371\tBest loss: 0.076371\tAccuracy: 97.85%\n",
      "1\tValidation loss: 0.049312\tBest loss: 0.049312\tAccuracy: 98.63%\n",
      "2\tValidation loss: 0.033071\tBest loss: 0.033071\tAccuracy: 98.94%\n",
      "3\tValidation loss: 0.027357\tBest loss: 0.027357\tAccuracy: 99.10%\n",
      "4\tValidation loss: 0.028748\tBest loss: 0.027357\tAccuracy: 99.26%\n",
      "5\tValidation loss: 0.036602\tBest loss: 0.027357\tAccuracy: 98.94%\n",
      "6\tValidation loss: 0.048089\tBest loss: 0.027357\tAccuracy: 98.94%\n",
      "7\tValidation loss: 0.030332\tBest loss: 0.027357\tAccuracy: 99.30%\n",
      "8\tValidation loss: 0.029336\tBest loss: 0.027357\tAccuracy: 99.22%\n",
      "9\tValidation loss: 0.033328\tBest loss: 0.027357\tAccuracy: 99.26%\n",
      "10\tValidation loss: 0.041745\tBest loss: 0.027357\tAccuracy: 98.98%\n",
      "11\tValidation loss: 0.048739\tBest loss: 0.027357\tAccuracy: 98.75%\n",
      "12\tValidation loss: 0.049520\tBest loss: 0.027357\tAccuracy: 98.94%\n",
      "13\tValidation loss: 0.034222\tBest loss: 0.027357\tAccuracy: 99.18%\n",
      "14\tValidation loss: 0.040270\tBest loss: 0.027357\tAccuracy: 99.34%\n",
      "15\tValidation loss: 0.033074\tBest loss: 0.027357\tAccuracy: 99.37%\n",
      "16\tValidation loss: 0.035130\tBest loss: 0.027357\tAccuracy: 99.06%\n",
      "17\tValidation loss: 0.031875\tBest loss: 0.027357\tAccuracy: 99.18%\n",
      "18\tValidation loss: 0.034898\tBest loss: 0.027357\tAccuracy: 99.37%\n",
      "19\tValidation loss: 0.019222\tBest loss: 0.019222\tAccuracy: 99.53%\n",
      "20\tValidation loss: 0.043814\tBest loss: 0.019222\tAccuracy: 99.37%\n",
      "21\tValidation loss: 0.028773\tBest loss: 0.019222\tAccuracy: 99.34%\n",
      "22\tValidation loss: 0.024850\tBest loss: 0.019222\tAccuracy: 99.45%\n",
      "23\tValidation loss: 0.021789\tBest loss: 0.019222\tAccuracy: 99.45%\n",
      "24\tValidation loss: 0.028846\tBest loss: 0.019222\tAccuracy: 99.37%\n",
      "25\tValidation loss: 0.064211\tBest loss: 0.019222\tAccuracy: 98.98%\n",
      "26\tValidation loss: 0.024425\tBest loss: 0.019222\tAccuracy: 99.49%\n",
      "27\tValidation loss: 0.035453\tBest loss: 0.019222\tAccuracy: 99.22%\n",
      "28\tValidation loss: 0.023940\tBest loss: 0.019222\tAccuracy: 99.37%\n",
      "29\tValidation loss: 0.041495\tBest loss: 0.019222\tAccuracy: 99.18%\n",
      "30\tValidation loss: 0.028030\tBest loss: 0.019222\tAccuracy: 99.37%\n",
      "31\tValidation loss: 0.028003\tBest loss: 0.019222\tAccuracy: 99.49%\n",
      "32\tValidation loss: 0.026579\tBest loss: 0.019222\tAccuracy: 99.45%\n",
      "33\tValidation loss: 0.037838\tBest loss: 0.019222\tAccuracy: 98.91%\n",
      "34\tValidation loss: 0.026082\tBest loss: 0.019222\tAccuracy: 99.49%\n",
      "35\tValidation loss: 0.031529\tBest loss: 0.019222\tAccuracy: 99.34%\n",
      "36\tValidation loss: 0.028220\tBest loss: 0.019222\tAccuracy: 99.18%\n",
      "37\tValidation loss: 0.038546\tBest loss: 0.019222\tAccuracy: 99.10%\n",
      "38\tValidation loss: 0.041586\tBest loss: 0.019222\tAccuracy: 98.75%\n",
      "39\tValidation loss: 0.038835\tBest loss: 0.019222\tAccuracy: 99.41%\n",
      "40\tValidation loss: 0.042555\tBest loss: 0.019222\tAccuracy: 99.14%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=DNNClassifier(activation=<function elu at 0x7fd9e8a620d0>,\n",
       "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7fd9d5e628c8>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42),\n",
       "          fit_params={'y_valid': array([0, 4, ..., 1, 2], dtype=uint8), 'X_valid': array([[ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.]], dtype=float32), 'n_epochs': 1000},\n",
       "          iid=True, n_iter=50, n_jobs=1,\n",
       "          param_distributions={'batch_norm_momentum': [0.9, 0.95, 0.98, 0.99, 0.999], 'n_neurons': [10, 30, 50, 70, 90, 100, 120, 140, 160], 'learning_rate': [0.01, 0.02, 0.05, 0.1], 'activation': [<function relu at 0x7fd9e8a660d0>, <function elu at 0x7fd9e8a620d0>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x7fd9d19e3bf8>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x7fd9d19e3a60>], 'batch_size': [10, 50, 100, 500]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # możemy również sprawdzić inne wartości warstw ukrytych, różne optymalizatory itd.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "    \"batch_norm_momentum\": [0.9, 0.95, 0.98, 0.99, 0.999],\n",
    "}\n",
    "\n",
    "rnd_search_bn = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                   fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n",
    "                                   random_state=42, verbose=2)\n",
    "rnd_search_bn.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': <function tensorflow.python.ops.gen_nn_ops.relu>,\n",
       " 'batch_norm_momentum': 0.98,\n",
       " 'batch_size': 100,\n",
       " 'learning_rate': 0.01,\n",
       " 'n_neurons': 160}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_bn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99396769799571905"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rnd_search_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nieco lepiej, niż poprzednio: 99.4% w porównaniu do 99.3%. Sprawdźmy, czy metoda porzucania sprawdzi się lepiej. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ćwiczenie: e.\tCzy model ulega przetrenowaniu względem danych uczących? Spróbuj dołączyć metodę porzucania do każdej warstwy i sprawdź rezultaty. Czy to rozwiązanie okazało się pomocne?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wróćmy do najlepszego modelu, którego wytrenowaliśmy wcześniej i sprawdźmy, jak sobie radzi ze zbiorem uczącym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99914401883158566"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf.predict(X_train1)\n",
    "accuracy_score(y_train1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model radzi sobie znacznie lepiej z zestawem uczącym niż testowym (99.91% w porównaniu co 99.32%), co oznacza, że ulega przetrenowaniu. Sprawdźmy, czy pomoże odrobina regularyzacji. Dodajmy porzucanie ze współczynnikiem porzucania rzędu 50%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.162759\tBest loss: 0.162759\tAccuracy: 95.15%\n",
      "1\tValidation loss: 0.120510\tBest loss: 0.120510\tAccuracy: 96.64%\n",
      "2\tValidation loss: 0.110715\tBest loss: 0.110715\tAccuracy: 96.91%\n",
      "3\tValidation loss: 0.104193\tBest loss: 0.104193\tAccuracy: 97.22%\n",
      "4\tValidation loss: 0.103560\tBest loss: 0.103560\tAccuracy: 97.81%\n",
      "5\tValidation loss: 0.087045\tBest loss: 0.087045\tAccuracy: 97.89%\n",
      "6\tValidation loss: 0.087227\tBest loss: 0.087045\tAccuracy: 97.65%\n",
      "7\tValidation loss: 0.079840\tBest loss: 0.079840\tAccuracy: 98.16%\n",
      "8\tValidation loss: 0.083102\tBest loss: 0.079840\tAccuracy: 97.50%\n",
      "9\tValidation loss: 0.076794\tBest loss: 0.076794\tAccuracy: 98.01%\n",
      "10\tValidation loss: 0.074914\tBest loss: 0.074914\tAccuracy: 97.93%\n",
      "11\tValidation loss: 0.073794\tBest loss: 0.073794\tAccuracy: 98.12%\n",
      "12\tValidation loss: 0.079777\tBest loss: 0.073794\tAccuracy: 97.89%\n",
      "13\tValidation loss: 0.080277\tBest loss: 0.073794\tAccuracy: 97.54%\n",
      "14\tValidation loss: 0.072409\tBest loss: 0.072409\tAccuracy: 98.08%\n",
      "15\tValidation loss: 0.071988\tBest loss: 0.071988\tAccuracy: 98.12%\n",
      "16\tValidation loss: 0.074609\tBest loss: 0.071988\tAccuracy: 97.93%\n",
      "17\tValidation loss: 0.069488\tBest loss: 0.069488\tAccuracy: 98.28%\n",
      "18\tValidation loss: 0.080863\tBest loss: 0.069488\tAccuracy: 98.40%\n",
      "19\tValidation loss: 0.074966\tBest loss: 0.069488\tAccuracy: 98.20%\n",
      "20\tValidation loss: 0.071082\tBest loss: 0.069488\tAccuracy: 98.12%\n",
      "21\tValidation loss: 0.070138\tBest loss: 0.069488\tAccuracy: 98.20%\n",
      "22\tValidation loss: 0.066032\tBest loss: 0.066032\tAccuracy: 98.28%\n",
      "23\tValidation loss: 0.061130\tBest loss: 0.061130\tAccuracy: 98.36%\n",
      "24\tValidation loss: 0.067107\tBest loss: 0.061130\tAccuracy: 98.16%\n",
      "25\tValidation loss: 0.071372\tBest loss: 0.061130\tAccuracy: 98.16%\n",
      "26\tValidation loss: 0.068535\tBest loss: 0.061130\tAccuracy: 98.36%\n",
      "27\tValidation loss: 0.065336\tBest loss: 0.061130\tAccuracy: 98.48%\n",
      "28\tValidation loss: 0.066783\tBest loss: 0.061130\tAccuracy: 98.40%\n",
      "29\tValidation loss: 0.092769\tBest loss: 0.061130\tAccuracy: 97.77%\n",
      "30\tValidation loss: 0.075746\tBest loss: 0.061130\tAccuracy: 98.01%\n",
      "31\tValidation loss: 0.084024\tBest loss: 0.061130\tAccuracy: 97.81%\n",
      "32\tValidation loss: 0.116428\tBest loss: 0.061130\tAccuracy: 98.44%\n",
      "33\tValidation loss: 0.079498\tBest loss: 0.061130\tAccuracy: 97.89%\n",
      "34\tValidation loss: 0.078189\tBest loss: 0.061130\tAccuracy: 97.97%\n",
      "35\tValidation loss: 0.083723\tBest loss: 0.061130\tAccuracy: 97.81%\n",
      "36\tValidation loss: 0.088210\tBest loss: 0.061130\tAccuracy: 97.19%\n",
      "37\tValidation loss: 0.080040\tBest loss: 0.061130\tAccuracy: 97.93%\n",
      "38\tValidation loss: 0.086932\tBest loss: 0.061130\tAccuracy: 97.89%\n",
      "39\tValidation loss: 0.240580\tBest loss: 0.061130\tAccuracy: 91.67%\n",
      "40\tValidation loss: 0.166662\tBest loss: 0.061130\tAccuracy: 94.29%\n",
      "41\tValidation loss: 0.125562\tBest loss: 0.061130\tAccuracy: 97.15%\n",
      "42\tValidation loss: 0.124890\tBest loss: 0.061130\tAccuracy: 95.82%\n",
      "43\tValidation loss: 0.127020\tBest loss: 0.061130\tAccuracy: 96.76%\n",
      "44\tValidation loss: 0.121540\tBest loss: 0.061130\tAccuracy: 96.05%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7fd9b2368d08>,\n",
       "       batch_norm_momentum=None, batch_size=500, dropout_rate=0.5,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7fd9d5e628c8>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_dropout = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                                n_neurons=90, random_state=42,\n",
    "                                dropout_rate=0.5)\n",
    "dnn_clf_dropout.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepsze parametry zostają znalezione w epoce 23. Metoda porzucania niejako spowolniła konwergencję."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy dokładność:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98657326328079398"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy pecha; wygląda na to, że metoda porzucania również nam tu nie pomoże. Spróbujmy dostroić hiperparametry, być może uda nam się jeszcze wycisnąć dodatkową wydajność z tego modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV] dropout_rate=0.5, n_neurons=70, learning_rate=0.01, activation=<function relu at 0x7fd9e8a660d0>, batch_size=100 \n",
      "0\tValidation loss: 0.355079\tBest loss: 0.355079\tAccuracy: 91.44%\n",
      "1\tValidation loss: 0.280624\tBest loss: 0.280624\tAccuracy: 94.10%\n",
      "2\tValidation loss: 0.279819\tBest loss: 0.279819\tAccuracy: 92.77%\n",
      "3\tValidation loss: 0.223614\tBest loss: 0.223614\tAccuracy: 94.10%\n",
      "4\tValidation loss: 0.199802\tBest loss: 0.199802\tAccuracy: 95.11%\n",
      "5\tValidation loss: 0.214481\tBest loss: 0.199802\tAccuracy: 95.47%\n",
      "6\tValidation loss: 0.216195\tBest loss: 0.199802\tAccuracy: 95.78%\n",
      "7\tValidation loss: 0.209172\tBest loss: 0.199802\tAccuracy: 94.80%\n",
      "8\tValidation loss: 0.182841\tBest loss: 0.182841\tAccuracy: 95.70%\n",
      "9\tValidation loss: 0.214252\tBest loss: 0.182841\tAccuracy: 95.82%\n",
      "10\tValidation loss: 0.198762\tBest loss: 0.182841\tAccuracy: 95.62%\n",
      "11\tValidation loss: 0.186415\tBest loss: 0.182841\tAccuracy: 95.82%\n",
      "12\tValidation loss: 0.222924\tBest loss: 0.182841\tAccuracy: 96.05%\n",
      "13\tValidation loss: 0.199636\tBest loss: 0.182841\tAccuracy: 95.97%\n",
      "14\tValidation loss: 0.214436\tBest loss: 0.182841\tAccuracy: 95.97%\n",
      "15\tValidation loss: 0.213507\tBest loss: 0.182841\tAccuracy: 95.47%\n",
      "16\tValidation loss: 0.191497\tBest loss: 0.182841\tAccuracy: 95.78%\n",
      "17\tValidation loss: 0.179503\tBest loss: 0.179503\tAccuracy: 95.93%\n",
      "18\tValidation loss: 0.210343\tBest loss: 0.179503\tAccuracy: 95.74%\n",
      "19\tValidation loss: 0.212626\tBest loss: 0.179503\tAccuracy: 95.27%\n",
      "20\tValidation loss: 0.187110\tBest loss: 0.179503\tAccuracy: 96.09%\n",
      "21\tValidation loss: 0.175171\tBest loss: 0.175171\tAccuracy: 95.78%\n",
      "22\tValidation loss: 0.217172\tBest loss: 0.175171\tAccuracy: 95.66%\n",
      "23\tValidation loss: 0.181060\tBest loss: 0.175171\tAccuracy: 96.44%\n",
      "24\tValidation loss: 0.163630\tBest loss: 0.163630\tAccuracy: 95.93%\n",
      "25\tValidation loss: 0.225873\tBest loss: 0.163630\tAccuracy: 95.58%\n",
      "26\tValidation loss: 0.204975\tBest loss: 0.163630\tAccuracy: 95.66%\n",
      "27\tValidation loss: 0.183588\tBest loss: 0.163630\tAccuracy: 95.97%\n",
      "28\tValidation loss: 0.231080\tBest loss: 0.163630\tAccuracy: 95.11%\n",
      "29\tValidation loss: 0.204342\tBest loss: 0.163630\tAccuracy: 95.74%\n",
      "30\tValidation loss: 0.183963\tBest loss: 0.163630\tAccuracy: 95.93%\n",
      "31\tValidation loss: 0.200975\tBest loss: 0.163630\tAccuracy: 95.23%\n",
      "32\tValidation loss: 0.211165\tBest loss: 0.163630\tAccuracy: 95.23%\n",
      "33\tValidation loss: 0.217777\tBest loss: 0.163630\tAccuracy: 95.07%\n",
      "34\tValidation loss: 0.193184\tBest loss: 0.163630\tAccuracy: 95.39%\n",
      "35\tValidation loss: 0.203809\tBest loss: 0.163630\tAccuracy: 95.58%\n",
      "36\tValidation loss: 0.221673\tBest loss: 0.163630\tAccuracy: 94.57%\n",
      "37\tValidation loss: 0.215750\tBest loss: 0.163630\tAccuracy: 95.39%\n",
      "38\tValidation loss: 0.189653\tBest loss: 0.163630\tAccuracy: 96.09%\n",
      "39\tValidation loss: 0.191333\tBest loss: 0.163630\tAccuracy: 95.19%\n",
      "40\tValidation loss: 0.207714\tBest loss: 0.163630\tAccuracy: 96.01%\n",
      "41\tValidation loss: 0.174490\tBest loss: 0.163630\tAccuracy: 95.39%\n",
      "42\tValidation loss: 0.177445\tBest loss: 0.163630\tAccuracy: 95.82%\n",
      "43\tValidation loss: 0.166708\tBest loss: 0.163630\tAccuracy: 96.09%\n",
      "44\tValidation loss: 0.190829\tBest loss: 0.163630\tAccuracy: 95.70%\n",
      "45\tValidation loss: 0.225985\tBest loss: 0.163630\tAccuracy: 96.25%\n",
      "Early stopping!\n",
      "[CV]  dropout_rate=0.5, n_neurons=70, learning_rate=0.01, activation=<function relu at 0x7fd9e8a660d0>, batch_size=100, total=  39.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] dropout_rate=0.5, n_neurons=70, learning_rate=0.01, activation=<function relu at 0x7fd9e8a660d0>, batch_size=100 \n",
      "0\tValidation loss: 0.748480\tBest loss: 0.748480\tAccuracy: 57.70%\n",
      "1\tValidation loss: 0.516088\tBest loss: 0.516088\tAccuracy: 78.50%\n",
      "2\tValidation loss: 0.448866\tBest loss: 0.448866\tAccuracy: 78.89%\n",
      "3\tValidation loss: 0.435606\tBest loss: 0.435606\tAccuracy: 78.54%\n",
      "4\tValidation loss: 0.435243\tBest loss: 0.435243\tAccuracy: 79.40%\n",
      "5\tValidation loss: 0.450605\tBest loss: 0.435243\tAccuracy: 78.42%\n",
      "6\tValidation loss: 0.430706\tBest loss: 0.430706\tAccuracy: 78.62%\n",
      "7\tValidation loss: 0.449289\tBest loss: 0.430706\tAccuracy: 78.30%\n",
      "8\tValidation loss: 0.413226\tBest loss: 0.413226\tAccuracy: 79.05%\n",
      "9\tValidation loss: 0.436053\tBest loss: 0.413226\tAccuracy: 78.46%\n",
      "10\tValidation loss: 0.459932\tBest loss: 0.413226\tAccuracy: 79.24%\n",
      "11\tValidation loss: 0.424138\tBest loss: 0.413226\tAccuracy: 79.24%\n",
      "12\tValidation loss: 0.409538\tBest loss: 0.409538\tAccuracy: 79.55%\n",
      "13\tValidation loss: 0.416324\tBest loss: 0.409538\tAccuracy: 75.41%\n",
      "14\tValidation loss: 0.440273\tBest loss: 0.409538\tAccuracy: 78.46%\n",
      "15\tValidation loss: 0.435736\tBest loss: 0.409538\tAccuracy: 79.05%\n",
      "16\tValidation loss: 0.428412\tBest loss: 0.409538\tAccuracy: 79.20%\n",
      "17\tValidation loss: 0.450156\tBest loss: 0.409538\tAccuracy: 80.02%\n",
      "18\tValidation loss: 0.421057\tBest loss: 0.409538\tAccuracy: 79.24%\n",
      "19\tValidation loss: 0.442284\tBest loss: 0.409538\tAccuracy: 79.01%\n",
      "20\tValidation loss: 0.426907\tBest loss: 0.409538\tAccuracy: 79.16%\n",
      "21\tValidation loss: 0.439567\tBest loss: 0.409538\tAccuracy: 79.05%\n",
      "22\tValidation loss: 0.452601\tBest loss: 0.409538\tAccuracy: 79.67%\n",
      "23\tValidation loss: 0.424887\tBest loss: 0.409538\tAccuracy: 79.09%\n",
      "24\tValidation loss: 0.441096\tBest loss: 0.409538\tAccuracy: 78.97%\n",
      "25\tValidation loss: 0.417390\tBest loss: 0.409538\tAccuracy: 78.89%\n",
      "26\tValidation loss: 0.418550\tBest loss: 0.409538\tAccuracy: 79.05%\n",
      "27\tValidation loss: 0.426065\tBest loss: 0.409538\tAccuracy: 78.66%\n",
      "28\tValidation loss: 0.413968\tBest loss: 0.409538\tAccuracy: 79.36%\n",
      "29\tValidation loss: 0.425434\tBest loss: 0.409538\tAccuracy: 79.24%\n",
      "30\tValidation loss: 0.455391\tBest loss: 0.409538\tAccuracy: 74.71%\n",
      "31\tValidation loss: 0.429498\tBest loss: 0.409538\tAccuracy: 79.20%\n",
      "32\tValidation loss: 0.427383\tBest loss: 0.409538\tAccuracy: 79.52%\n",
      "33\tValidation loss: 0.422621\tBest loss: 0.409538\tAccuracy: 78.62%\n",
      "Early stopping!\n",
      "[CV]  dropout_rate=0.5, n_neurons=70, learning_rate=0.01, activation=<function relu at 0x7fd9e8a660d0>, batch_size=100, total=  27.4s\n",
      "[CV] dropout_rate=0.5, n_neurons=70, learning_rate=0.01, activation=<function relu at 0x7fd9e8a660d0>, batch_size=100 \n",
      "0\tValidation loss: 0.497714\tBest loss: 0.497714\tAccuracy: 86.71%\n",
      "1\tValidation loss: 0.248258\tBest loss: 0.248258\tAccuracy: 93.51%\n",
      "2\tValidation loss: 0.279785\tBest loss: 0.248258\tAccuracy: 93.71%\n",
      "3\tValidation loss: 0.248663\tBest loss: 0.248258\tAccuracy: 94.61%\n",
      "4\tValidation loss: 0.269139\tBest loss: 0.248258\tAccuracy: 94.76%\n",
      "5\tValidation loss: 0.188808\tBest loss: 0.188808\tAccuracy: 95.39%\n",
      "6\tValidation loss: 0.196049\tBest loss: 0.188808\tAccuracy: 95.58%\n",
      "7\tValidation loss: 0.204966\tBest loss: 0.188808\tAccuracy: 95.15%\n",
      "8\tValidation loss: 0.238414\tBest loss: 0.188808\tAccuracy: 94.61%\n",
      "9\tValidation loss: 0.192095\tBest loss: 0.188808\tAccuracy: 95.97%\n",
      "[...and much later...]\n",
      "19\tValidation loss: 1.939112\tBest loss: 1.619874\tAccuracy: 22.01%\n",
      "20\tValidation loss: 1.825761\tBest loss: 1.619874\tAccuracy: 19.27%\n",
      "21\tValidation loss: 1.732937\tBest loss: 1.619874\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.832995\tBest loss: 1.619874\tAccuracy: 20.91%\n",
      "23\tValidation loss: 1.659557\tBest loss: 1.619874\tAccuracy: 20.91%\n",
      "24\tValidation loss: 1.828380\tBest loss: 1.619874\tAccuracy: 18.73%\n",
      "25\tValidation loss: 1.719589\tBest loss: 1.619874\tAccuracy: 22.01%\n",
      "26\tValidation loss: 1.842429\tBest loss: 1.619874\tAccuracy: 18.73%\n",
      "27\tValidation loss: 1.717596\tBest loss: 1.619874\tAccuracy: 19.27%\n",
      "28\tValidation loss: 1.863441\tBest loss: 1.619874\tAccuracy: 19.08%\n",
      "29\tValidation loss: 1.952335\tBest loss: 1.619874\tAccuracy: 19.08%\n",
      "30\tValidation loss: 1.853776\tBest loss: 1.619874\tAccuracy: 20.91%\n",
      "31\tValidation loss: 1.894134\tBest loss: 1.619874\tAccuracy: 22.01%\n",
      "32\tValidation loss: 1.711688\tBest loss: 1.619874\tAccuracy: 19.08%\n",
      "33\tValidation loss: 1.651240\tBest loss: 1.619874\tAccuracy: 18.73%\n",
      "34\tValidation loss: 1.760639\tBest loss: 1.619874\tAccuracy: 20.91%\n",
      "35\tValidation loss: 1.667938\tBest loss: 1.619874\tAccuracy: 22.01%\n",
      "36\tValidation loss: 1.641116\tBest loss: 1.619874\tAccuracy: 20.91%\n",
      "37\tValidation loss: 1.694960\tBest loss: 1.619874\tAccuracy: 19.08%\n",
      "38\tValidation loss: 1.816517\tBest loss: 1.619874\tAccuracy: 18.73%\n",
      "39\tValidation loss: 1.647246\tBest loss: 1.619874\tAccuracy: 18.73%\n",
      "Early stopping!\n",
      "[CV]  dropout_rate=0.5, n_neurons=140, learning_rate=0.05, activation=<function elu at 0x7fd9e8a620d0>, batch_size=100, total= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 130.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.165751\tBest loss: 0.165751\tAccuracy: 95.47%\n",
      "1\tValidation loss: 0.111834\tBest loss: 0.111834\tAccuracy: 96.99%\n",
      "2\tValidation loss: 0.102867\tBest loss: 0.102867\tAccuracy: 96.83%\n",
      "3\tValidation loss: 0.089197\tBest loss: 0.089197\tAccuracy: 97.85%\n",
      "4\tValidation loss: 0.093953\tBest loss: 0.089197\tAccuracy: 97.77%\n",
      "5\tValidation loss: 0.079498\tBest loss: 0.079498\tAccuracy: 98.08%\n",
      "6\tValidation loss: 0.081214\tBest loss: 0.079498\tAccuracy: 98.01%\n",
      "7\tValidation loss: 0.086096\tBest loss: 0.079498\tAccuracy: 97.77%\n",
      "8\tValidation loss: 0.074422\tBest loss: 0.074422\tAccuracy: 97.73%\n",
      "9\tValidation loss: 0.079650\tBest loss: 0.074422\tAccuracy: 97.89%\n",
      "10\tValidation loss: 0.077278\tBest loss: 0.074422\tAccuracy: 97.77%\n",
      "11\tValidation loss: 0.077608\tBest loss: 0.074422\tAccuracy: 98.24%\n",
      "12\tValidation loss: 0.074337\tBest loss: 0.074337\tAccuracy: 98.05%\n",
      "13\tValidation loss: 0.066028\tBest loss: 0.066028\tAccuracy: 98.28%\n",
      "14\tValidation loss: 0.072845\tBest loss: 0.066028\tAccuracy: 98.16%\n",
      "15\tValidation loss: 0.066652\tBest loss: 0.066028\tAccuracy: 98.05%\n",
      "16\tValidation loss: 0.065729\tBest loss: 0.065729\tAccuracy: 98.16%\n",
      "17\tValidation loss: 0.061191\tBest loss: 0.061191\tAccuracy: 98.51%\n",
      "18\tValidation loss: 0.062528\tBest loss: 0.061191\tAccuracy: 98.44%\n",
      "19\tValidation loss: 0.065407\tBest loss: 0.061191\tAccuracy: 98.36%\n",
      "20\tValidation loss: 0.065273\tBest loss: 0.061191\tAccuracy: 98.44%\n",
      "21\tValidation loss: 0.061035\tBest loss: 0.061035\tAccuracy: 98.40%\n",
      "22\tValidation loss: 0.056312\tBest loss: 0.056312\tAccuracy: 98.59%\n",
      "23\tValidation loss: 0.069074\tBest loss: 0.056312\tAccuracy: 98.40%\n",
      "24\tValidation loss: 0.057482\tBest loss: 0.056312\tAccuracy: 98.51%\n",
      "25\tValidation loss: 0.068342\tBest loss: 0.056312\tAccuracy: 98.44%\n",
      "26\tValidation loss: 0.063494\tBest loss: 0.056312\tAccuracy: 98.48%\n",
      "27\tValidation loss: 0.057257\tBest loss: 0.056312\tAccuracy: 98.51%\n",
      "28\tValidation loss: 0.058659\tBest loss: 0.056312\tAccuracy: 98.59%\n",
      "29\tValidation loss: 0.059009\tBest loss: 0.056312\tAccuracy: 98.48%\n",
      "30\tValidation loss: 0.058227\tBest loss: 0.056312\tAccuracy: 98.55%\n",
      "31\tValidation loss: 0.062198\tBest loss: 0.056312\tAccuracy: 98.44%\n",
      "32\tValidation loss: 0.058043\tBest loss: 0.056312\tAccuracy: 98.40%\n",
      "33\tValidation loss: 0.055970\tBest loss: 0.055970\tAccuracy: 98.51%\n",
      "34\tValidation loss: 0.060111\tBest loss: 0.055970\tAccuracy: 98.67%\n",
      "35\tValidation loss: 0.058786\tBest loss: 0.055970\tAccuracy: 98.44%\n",
      "36\tValidation loss: 0.059944\tBest loss: 0.055970\tAccuracy: 98.32%\n",
      "37\tValidation loss: 0.058087\tBest loss: 0.055970\tAccuracy: 98.63%\n",
      "38\tValidation loss: 0.063003\tBest loss: 0.055970\tAccuracy: 98.36%\n",
      "39\tValidation loss: 0.052073\tBest loss: 0.052073\tAccuracy: 98.67%\n",
      "40\tValidation loss: 0.058115\tBest loss: 0.052073\tAccuracy: 98.40%\n",
      "41\tValidation loss: 0.059997\tBest loss: 0.052073\tAccuracy: 98.63%\n",
      "42\tValidation loss: 0.052416\tBest loss: 0.052073\tAccuracy: 98.75%\n",
      "43\tValidation loss: 0.053840\tBest loss: 0.052073\tAccuracy: 98.59%\n",
      "44\tValidation loss: 0.054563\tBest loss: 0.052073\tAccuracy: 98.67%\n",
      "45\tValidation loss: 0.049410\tBest loss: 0.049410\tAccuracy: 98.55%\n",
      "46\tValidation loss: 0.057060\tBest loss: 0.049410\tAccuracy: 98.24%\n",
      "47\tValidation loss: 0.062434\tBest loss: 0.049410\tAccuracy: 98.48%\n",
      "48\tValidation loss: 0.054523\tBest loss: 0.049410\tAccuracy: 98.59%\n",
      "49\tValidation loss: 0.052774\tBest loss: 0.049410\tAccuracy: 98.36%\n",
      "50\tValidation loss: 0.056562\tBest loss: 0.049410\tAccuracy: 98.32%\n",
      "51\tValidation loss: 0.060280\tBest loss: 0.049410\tAccuracy: 98.51%\n",
      "52\tValidation loss: 0.055685\tBest loss: 0.049410\tAccuracy: 98.55%\n",
      "53\tValidation loss: 0.056077\tBest loss: 0.049410\tAccuracy: 98.44%\n",
      "54\tValidation loss: 0.057951\tBest loss: 0.049410\tAccuracy: 98.44%\n",
      "55\tValidation loss: 0.056315\tBest loss: 0.049410\tAccuracy: 98.75%\n",
      "56\tValidation loss: 0.055744\tBest loss: 0.049410\tAccuracy: 98.55%\n",
      "57\tValidation loss: 0.054228\tBest loss: 0.049410\tAccuracy: 98.48%\n",
      "58\tValidation loss: 0.057836\tBest loss: 0.049410\tAccuracy: 98.71%\n",
      "59\tValidation loss: 0.053361\tBest loss: 0.049410\tAccuracy: 98.71%\n",
      "60\tValidation loss: 0.056389\tBest loss: 0.049410\tAccuracy: 98.48%\n",
      "61\tValidation loss: 0.061350\tBest loss: 0.049410\tAccuracy: 98.48%\n",
      "62\tValidation loss: 0.052135\tBest loss: 0.049410\tAccuracy: 98.67%\n",
      "63\tValidation loss: 0.053853\tBest loss: 0.049410\tAccuracy: 98.48%\n",
      "64\tValidation loss: 0.056641\tBest loss: 0.049410\tAccuracy: 98.71%\n",
      "65\tValidation loss: 0.052790\tBest loss: 0.049410\tAccuracy: 98.63%\n",
      "66\tValidation loss: 0.053514\tBest loss: 0.049410\tAccuracy: 98.44%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=DNNClassifier(activation=<function elu at 0x7fd9e8a620d0>,\n",
       "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7fd9d5e628c8>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42),\n",
       "          fit_params={'y_valid': array([0, 4, ..., 1, 2], dtype=uint8), 'X_valid': array([[ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.]], dtype=float32), 'n_epochs': 1000},\n",
       "          iid=True, n_iter=50, n_jobs=1,\n",
       "          param_distributions={'dropout_rate': [0.2, 0.3, 0.4, 0.5, 0.6], 'n_neurons': [10, 30, 50, 70, 90, 100, 120, 140, 160], 'learning_rate': [0.01, 0.02, 0.05, 0.1], 'activation': [<function relu at 0x7fd9e8a660d0>, <function elu at 0x7fd9e8a620d0>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x7fd9b2368950>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x7fd9b23687b8>], 'batch_size': [10, 50, 100, 500]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # moglibyśmy jeżeli spróbować sprawdzić różne wartości warstw ukrytych, różne optymalizatory itd.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "    \"dropout_rate\": [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "}\n",
    "\n",
    "rnd_search_dropout = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                        fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n",
    "                                        random_state=42, verbose=2)\n",
    "rnd_search_dropout.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': <function __main__.leaky_relu.<locals>.parametrized_leaky_relu>,\n",
       " 'batch_size': 500,\n",
       " 'dropout_rate': 0.4,\n",
       " 'learning_rate': 0.01,\n",
       " 'n_neurons': 50}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_dropout.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98812998637867289"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rnd_search_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No cóż, metoda porzucania nie poprawia modelu. Może następnym razem będzie lepiej! :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nic nie szkodzi, udało nam się uzyskać ładną sieć GSN osiągającą 99,4% dokładności wobec zestawu testowego przy użyciu normalizacji wsadowej lub 99,32 bez jej użycia. Sprawdźmy, czy możemy przenieść część tych osiągnięć (klasyfikacja cyfr od 0 do 4) do zadania klasyfikowania cyfr od 5 do 9. W celu zachowania przejrzystości wykorzystamy ponownie sieć neuronową bez bez normalizacji wsadowej, gdyż i tak sprawuje się niemal równie dobrze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Uczenie transferowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ćwiczenie: Stwórz nowy model sieci GSN wykorzystujący wszystkie wyuczone warstwy ukryte z poprzedniego ćwiczenia — zamroź je i wstaw nową warstwę wyjściową z funkcją softmax._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytajmy graf najlepszego modelu i zajmijmy się wsystkimi potrzebnymi operacjami. Zwróć uwagę, że zamiast tworzenia nowej warstwy wyjściowej zawierającej funkcję softmax, wykorzystamy już istniejącą (gdyż ma ona taką samą liczbę wyjść jak już istniejąca warstwa). Przed rozpoczęciem uczenia ponownie zainicjujemy jej parametry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"./moj_najlepszy_model_mnist_0_do_4.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"strata:0\")\n",
    "Y_proba = tf.get_default_graph().get_tensor_by_name(\"Y_prawd:0\")\n",
    "logits = Y_proba.op.inputs[0]\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"dokladnosc:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W celu zamrożenia niższych warstw wykluczymy ich zmienne z listy modyfikowalnych zmiennych przechowywanej przez optymalizator i pozostawimy jedynie modyfikowalne zmienne warstwy wyjściowej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logity\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ćwiczenie: Wyucz ten model wobec cyfr 5 – 9 zestawu MNIST; wyznacz dla każdej cyfry tylko 100 obrazów i zmierz czas poświęcony na jego wytrenowanie. Czy, mimo niewielkiej liczby próbek uczących, możesz osiągnąć dużą wartość precyzji?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy podzbiory uczący, walidacyjny i testowy. Musimy odjąć wartość 5 od etykiet, ponieważ moduł TensorFlow oczekuje wartości stałoprzecinkowych w zakresie od 0 do `n_classes-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rónież w ramach tego ćwiczenia chcemy zachować jedynie po 100 próbek na klasę w zestawie uczącym (a także zachować zaledwie po 30 próbek na klasę w zbiorze walidacyjnym). Posłuży nam do tego niewielka funkcja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas wytrenować model. Skorzystamy z kodu wykorzystującego wczesne zatrzymywanie, ale zmodyfikujemy trochę inicjację: najpierw zainicjujemy wszystkie zmienne, następnie odtworzymy najlepszy, wcześniej wytrenowany model (klasyfikujący cyfry od 0 do 4), a na końcu ponownie zainicjujemy zmienne warstwy wyjściowej: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4\n",
      "0\tValidation loss: 0.967851\tBest loss: 0.967851\tAccuracy: 67.33%\n",
      "1\tValidation loss: 0.861747\tBest loss: 0.861747\tAccuracy: 71.33%\n",
      "2\tValidation loss: 0.777535\tBest loss: 0.777535\tAccuracy: 72.00%\n",
      "3\tValidation loss: 0.699915\tBest loss: 0.699915\tAccuracy: 75.33%\n",
      "4\tValidation loss: 0.786714\tBest loss: 0.699915\tAccuracy: 78.00%\n",
      "5\tValidation loss: 0.735406\tBest loss: 0.699915\tAccuracy: 76.67%\n",
      "6\tValidation loss: 0.732264\tBest loss: 0.699915\tAccuracy: 78.00%\n",
      "7\tValidation loss: 0.691741\tBest loss: 0.691741\tAccuracy: 76.00%\n",
      "8\tValidation loss: 0.672757\tBest loss: 0.672757\tAccuracy: 80.00%\n",
      "9\tValidation loss: 0.666520\tBest loss: 0.666520\tAccuracy: 80.00%\n",
      "10\tValidation loss: 0.639375\tBest loss: 0.639375\tAccuracy: 81.33%\n",
      "11\tValidation loss: 0.645089\tBest loss: 0.639375\tAccuracy: 82.00%\n",
      "12\tValidation loss: 0.646768\tBest loss: 0.639375\tAccuracy: 80.00%\n",
      "13\tValidation loss: 0.623784\tBest loss: 0.623784\tAccuracy: 82.67%\n",
      "14\tValidation loss: 0.663026\tBest loss: 0.623784\tAccuracy: 80.00%\n",
      "15\tValidation loss: 0.704513\tBest loss: 0.623784\tAccuracy: 79.33%\n",
      "16\tValidation loss: 0.684003\tBest loss: 0.623784\tAccuracy: 79.33%\n",
      "17\tValidation loss: 0.658575\tBest loss: 0.623784\tAccuracy: 82.67%\n",
      "18\tValidation loss: 0.669875\tBest loss: 0.623784\tAccuracy: 79.33%\n",
      "19\tValidation loss: 0.664581\tBest loss: 0.623784\tAccuracy: 78.67%\n",
      "20\tValidation loss: 0.653490\tBest loss: 0.623784\tAccuracy: 80.00%\n",
      "21\tValidation loss: 0.707304\tBest loss: 0.623784\tAccuracy: 79.33%\n",
      "22\tValidation loss: 0.706012\tBest loss: 0.623784\tAccuracy: 80.67%\n",
      "23\tValidation loss: 0.681227\tBest loss: 0.623784\tAccuracy: 78.67%\n",
      "24\tValidation loss: 0.786823\tBest loss: 0.623784\tAccuracy: 78.00%\n",
      "25\tValidation loss: 0.686110\tBest loss: 0.623784\tAccuracy: 79.33%\n",
      "26\tValidation loss: 0.675166\tBest loss: 0.623784\tAccuracy: 82.67%\n",
      "27\tValidation loss: 0.667711\tBest loss: 0.623784\tAccuracy: 82.67%\n",
      "28\tValidation loss: 0.612220\tBest loss: 0.612220\tAccuracy: 83.33%\n",
      "29\tValidation loss: 0.701196\tBest loss: 0.612220\tAccuracy: 78.00%\n",
      "30\tValidation loss: 0.687806\tBest loss: 0.612220\tAccuracy: 81.33%\n",
      "31\tValidation loss: 0.776596\tBest loss: 0.612220\tAccuracy: 79.33%\n",
      "32\tValidation loss: 0.674172\tBest loss: 0.612220\tAccuracy: 80.00%\n",
      "33\tValidation loss: 0.719044\tBest loss: 0.612220\tAccuracy: 83.33%\n",
      "34\tValidation loss: 0.856403\tBest loss: 0.612220\tAccuracy: 74.00%\n",
      "35\tValidation loss: 0.744627\tBest loss: 0.612220\tAccuracy: 80.00%\n",
      "36\tValidation loss: 0.779348\tBest loss: 0.612220\tAccuracy: 78.00%\n",
      "37\tValidation loss: 0.763777\tBest loss: 0.612220\tAccuracy: 78.00%\n",
      "38\tValidation loss: 0.727376\tBest loss: 0.612220\tAccuracy: 78.00%\n",
      "39\tValidation loss: 0.823514\tBest loss: 0.612220\tAccuracy: 78.00%\n",
      "40\tValidation loss: 0.725053\tBest loss: 0.612220\tAccuracy: 80.67%\n",
      "41\tValidation loss: 0.678497\tBest loss: 0.612220\tAccuracy: 80.67%\n",
      "42\tValidation loss: 0.709977\tBest loss: 0.612220\tAccuracy: 80.67%\n",
      "43\tValidation loss: 0.737200\tBest loss: 0.612220\tAccuracy: 77.33%\n",
      "44\tValidation loss: 0.757937\tBest loss: 0.612220\tAccuracy: 77.33%\n",
      "45\tValidation loss: 0.732024\tBest loss: 0.612220\tAccuracy: 80.00%\n",
      "46\tValidation loss: 0.756428\tBest loss: 0.612220\tAccuracy: 80.67%\n",
      "47\tValidation loss: 0.757610\tBest loss: 0.612220\tAccuracy: 78.67%\n",
      "48\tValidation loss: 0.844137\tBest loss: 0.612220\tAccuracy: 80.00%\n",
      "Early stopping!\n",
      "Total training time: 2.3s\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 76.30%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./moj_najlepszy_model_mnis_0_do_4\")\n",
    "    for var in output_layer_vars:\n",
    "        var.initializer.run()\n",
    "\n",
    "    t0 = time.time()\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./moj_model_mnist_5_do_9_piec_zamrozonych\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Wczesne zatrzymywanie!\")\n",
    "                break\n",
    "        print(\"{}\\tF. straty dla zbioru walidacyjnego: {:.6f}\\tNajlepsza wartość straty: {:.6f}\\tDokładność: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Całkowity czas uczenia: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./moj_model_mnist_5_do_9_piec_zamrozonych\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Ostateczna dokładność dla zbioru testowego: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To raczej nie jest wymarzona dokładność, prawda? Oczywiście w przypadku tak małego zestawu uczącego i tylko jednej dostrajanej warstwy nie powinniśmy spodziewać się cudów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ćwiczenie: Spróbuj zbuforować zamrożone warstwy, po czym wytrenuj ponownie model: czy jest on teraz dużo szybszy?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zacznijmy od \"uchwycenia\" wyjścia ostatniej zamrożonej warstwy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"ukryta5_wyj:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyuczmy teraz model wykorzystując niemal taki sam kod, jak wcześniej. Różnica polega na tym, że obliczamy wyjście wierzchniej zamrożonej warstwy już na początku (zarówno za pomocą zestawy uczącego, jak i walidacyjnego), po czym buforujemy wynik. W ten sposób proces uczenia trwa od 1,5 do 3 razy szybciej w tym przykładzie (może to się jednak bardzo różnić w zależności od Twojego systemu): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4\n",
      "0\tValidation loss: 1.109053\tBest loss: 1.109053\tAccuracy: 60.67%\n",
      "1\tValidation loss: 0.813156\tBest loss: 0.813156\tAccuracy: 72.00%\n",
      "2\tValidation loss: 0.755930\tBest loss: 0.755930\tAccuracy: 76.67%\n",
      "3\tValidation loss: 0.744004\tBest loss: 0.744004\tAccuracy: 74.67%\n",
      "4\tValidation loss: 0.685080\tBest loss: 0.685080\tAccuracy: 78.00%\n",
      "5\tValidation loss: 0.702316\tBest loss: 0.685080\tAccuracy: 78.00%\n",
      "6\tValidation loss: 0.646487\tBest loss: 0.646487\tAccuracy: 80.00%\n",
      "7\tValidation loss: 0.686437\tBest loss: 0.646487\tAccuracy: 79.33%\n",
      "8\tValidation loss: 0.750047\tBest loss: 0.646487\tAccuracy: 79.33%\n",
      "9\tValidation loss: 0.688554\tBest loss: 0.646487\tAccuracy: 79.33%\n",
      "10\tValidation loss: 0.785184\tBest loss: 0.646487\tAccuracy: 78.67%\n",
      "11\tValidation loss: 0.634506\tBest loss: 0.634506\tAccuracy: 80.67%\n",
      "12\tValidation loss: 0.656797\tBest loss: 0.634506\tAccuracy: 81.33%\n",
      "13\tValidation loss: 0.645497\tBest loss: 0.634506\tAccuracy: 81.33%\n",
      "14\tValidation loss: 0.618038\tBest loss: 0.618038\tAccuracy: 83.33%\n",
      "15\tValidation loss: 0.641752\tBest loss: 0.618038\tAccuracy: 78.67%\n",
      "16\tValidation loss: 0.645671\tBest loss: 0.618038\tAccuracy: 80.67%\n",
      "17\tValidation loss: 0.654640\tBest loss: 0.618038\tAccuracy: 82.00%\n",
      "18\tValidation loss: 0.670569\tBest loss: 0.618038\tAccuracy: 79.33%\n",
      "19\tValidation loss: 0.670985\tBest loss: 0.618038\tAccuracy: 82.00%\n",
      "20\tValidation loss: 0.659538\tBest loss: 0.618038\tAccuracy: 82.67%\n",
      "21\tValidation loss: 0.622648\tBest loss: 0.618038\tAccuracy: 83.33%\n",
      "22\tValidation loss: 0.736155\tBest loss: 0.618038\tAccuracy: 79.33%\n",
      "23\tValidation loss: 0.739367\tBest loss: 0.618038\tAccuracy: 76.67%\n",
      "24\tValidation loss: 0.699710\tBest loss: 0.618038\tAccuracy: 78.00%\n",
      "25\tValidation loss: 0.709630\tBest loss: 0.618038\tAccuracy: 81.33%\n",
      "26\tValidation loss: 0.692474\tBest loss: 0.618038\tAccuracy: 79.33%\n",
      "27\tValidation loss: 0.807931\tBest loss: 0.618038\tAccuracy: 77.33%\n",
      "28\tValidation loss: 0.676134\tBest loss: 0.618038\tAccuracy: 82.00%\n",
      "29\tValidation loss: 0.738905\tBest loss: 0.618038\tAccuracy: 79.33%\n",
      "30\tValidation loss: 0.664826\tBest loss: 0.618038\tAccuracy: 81.33%\n",
      "31\tValidation loss: 0.694714\tBest loss: 0.618038\tAccuracy: 80.00%\n",
      "32\tValidation loss: 0.739238\tBest loss: 0.618038\tAccuracy: 83.33%\n",
      "33\tValidation loss: 0.697210\tBest loss: 0.618038\tAccuracy: 80.00%\n",
      "34\tValidation loss: 0.817373\tBest loss: 0.618038\tAccuracy: 79.33%\n",
      "Early stopping!\n",
      "Total training time: 0.9s\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 76.51%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./moj_najlepszy_model_mnist_0_do_4\")\n",
    "    for var in output_layer_vars:\n",
    "        var.initializer.run()\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    hidden5_train = hidden5_out.eval(feed_dict={X: X_train2, y: y_train2})\n",
    "    hidden5_valid = hidden5_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            h5_batch, y_batch = hidden5_train[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={hidden5_out: h5_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={hidden5_out: hidden5_valid, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./moj_model_mnist_5_do_9_piec_zamrozonych\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Wczesne zatrzymywanie!\")\n",
    "                break\n",
    "        print(\"{}\\tF. straty dla zestawu walidacyjnego: {:.6f}\\tNajlepsza wartość straty: {:.6f}\\tDokładność: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Całkowity czas uczenia: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./moj_model_mnist_5_do_9_piec_zamrozonych\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Ostateczna dokładność dla zestawu testowego: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ćwiczenie: Spróbuj ponownie wytrenować model, tym korzystając tylko z czterech warstw ukrytych. Czy możesz osiągnąć w ten sposób większą precyzję?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytajmy ponownie najlepszy model, tym razem jednak stworzymy nową warstwę wyjściową z funkcja aktywacji softmax, którą umieścimy nad czwartą warstwą ukrytą:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_outputs = 5\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"./moj_najlepszy_model_mnist_0_do_4.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden4_out = tf.get_default_graph().get_tensor_by_name(\"ukryta4_wyj:0\")\n",
    "logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"nowe_logity\")\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"dokladnosc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A teraz stwórzmy operację uczenia. Chcemy zamrozić wszystkie warstwy oprócz nowoutworzonej warstwy wyjściowej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"nowe_logity\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "four_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I po raz kolejny wytrenujemy model za pomocą dobrze już nam znanego kodu. Uwaga: moglibyśmy, oczywiście, napisać funkcję raz i wykorzystywać ją wiele razy zamiast bez przerwy kopiować niemal niezmieniany kod, my jednak ciągle w jakiś sposób go modyfikujemy, dlatego funkcja ta wymagałaby wielu argumentów i instrukcji `if`, a do tego musiałaby znajdować się na samym początku notatnika, co nie miałoby dużego sensu z punktu widzenia Czytelnika. Po prostu wprowadzałoby to niepotrzebny zamęt, dlatego lepiej zostańmy przy metodzie kopiowania i wklejania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4\n",
      "0\tValidation loss: 0.923460\tBest loss: 0.923460\tAccuracy: 69.33%\n",
      "1\tValidation loss: 0.796192\tBest loss: 0.796192\tAccuracy: 77.33%\n",
      "2\tValidation loss: 0.812068\tBest loss: 0.796192\tAccuracy: 78.67%\n",
      "3\tValidation loss: 0.697938\tBest loss: 0.697938\tAccuracy: 80.67%\n",
      "4\tValidation loss: 0.877122\tBest loss: 0.697938\tAccuracy: 74.67%\n",
      "5\tValidation loss: 0.708524\tBest loss: 0.697938\tAccuracy: 81.33%\n",
      "6\tValidation loss: 0.689500\tBest loss: 0.689500\tAccuracy: 84.00%\n",
      "7\tValidation loss: 0.758315\tBest loss: 0.689500\tAccuracy: 81.33%\n",
      "8\tValidation loss: 0.711138\tBest loss: 0.689500\tAccuracy: 78.67%\n",
      "9\tValidation loss: 0.687304\tBest loss: 0.687304\tAccuracy: 81.33%\n",
      "10\tValidation loss: 0.639222\tBest loss: 0.639222\tAccuracy: 81.33%\n",
      "11\tValidation loss: 0.716750\tBest loss: 0.639222\tAccuracy: 82.67%\n",
      "12\tValidation loss: 0.693442\tBest loss: 0.639222\tAccuracy: 80.67%\n",
      "13\tValidation loss: 0.727682\tBest loss: 0.639222\tAccuracy: 84.00%\n",
      "14\tValidation loss: 0.637289\tBest loss: 0.637289\tAccuracy: 84.67%\n",
      "15\tValidation loss: 0.741304\tBest loss: 0.637289\tAccuracy: 83.33%\n",
      "16\tValidation loss: 0.651895\tBest loss: 0.637289\tAccuracy: 82.67%\n",
      "17\tValidation loss: 0.641192\tBest loss: 0.637289\tAccuracy: 80.67%\n",
      "18\tValidation loss: 0.690386\tBest loss: 0.637289\tAccuracy: 80.67%\n",
      "19\tValidation loss: 0.648541\tBest loss: 0.637289\tAccuracy: 82.67%\n",
      "20\tValidation loss: 0.779663\tBest loss: 0.637289\tAccuracy: 83.33%\n",
      "21\tValidation loss: 0.768834\tBest loss: 0.637289\tAccuracy: 82.67%\n",
      "22\tValidation loss: 0.706279\tBest loss: 0.637289\tAccuracy: 82.67%\n",
      "23\tValidation loss: 0.745840\tBest loss: 0.637289\tAccuracy: 82.00%\n",
      "24\tValidation loss: 0.740068\tBest loss: 0.637289\tAccuracy: 83.33%\n",
      "25\tValidation loss: 0.604927\tBest loss: 0.604927\tAccuracy: 84.67%\n",
      "26\tValidation loss: 0.635410\tBest loss: 0.604927\tAccuracy: 82.00%\n",
      "27\tValidation loss: 0.776003\tBest loss: 0.604927\tAccuracy: 82.67%\n",
      "28\tValidation loss: 0.621502\tBest loss: 0.604927\tAccuracy: 82.00%\n",
      "29\tValidation loss: 0.695963\tBest loss: 0.604927\tAccuracy: 83.33%\n",
      "30\tValidation loss: 0.668194\tBest loss: 0.604927\tAccuracy: 84.67%\n",
      "31\tValidation loss: 0.768975\tBest loss: 0.604927\tAccuracy: 82.67%\n",
      "32\tValidation loss: 0.594731\tBest loss: 0.594731\tAccuracy: 84.00%\n",
      "33\tValidation loss: 0.665088\tBest loss: 0.594731\tAccuracy: 84.00%\n",
      "34\tValidation loss: 0.716284\tBest loss: 0.594731\tAccuracy: 81.33%\n",
      "35\tValidation loss: 0.782680\tBest loss: 0.594731\tAccuracy: 84.00%\n",
      "36\tValidation loss: 0.816441\tBest loss: 0.594731\tAccuracy: 84.00%\n",
      "37\tValidation loss: 0.749341\tBest loss: 0.594731\tAccuracy: 84.00%\n",
      "38\tValidation loss: 0.728754\tBest loss: 0.594731\tAccuracy: 82.00%\n",
      "39\tValidation loss: 0.838166\tBest loss: 0.594731\tAccuracy: 84.00%\n",
      "40\tValidation loss: 0.714871\tBest loss: 0.594731\tAccuracy: 84.00%\n",
      "41\tValidation loss: 0.765463\tBest loss: 0.594731\tAccuracy: 84.67%\n",
      "42\tValidation loss: 0.744043\tBest loss: 0.594731\tAccuracy: 82.00%\n",
      "43\tValidation loss: 0.726922\tBest loss: 0.594731\tAccuracy: 83.33%\n",
      "44\tValidation loss: 0.641118\tBest loss: 0.594731\tAccuracy: 82.67%\n",
      "45\tValidation loss: 0.657861\tBest loss: 0.594731\tAccuracy: 84.00%\n",
      "46\tValidation loss: 0.803642\tBest loss: 0.594731\tAccuracy: 86.00%\n",
      "47\tValidation loss: 0.754644\tBest loss: 0.594731\tAccuracy: 84.67%\n",
      "48\tValidation loss: 0.865141\tBest loss: 0.594731\tAccuracy: 84.00%\n",
      "49\tValidation loss: 0.709169\tBest loss: 0.594731\tAccuracy: 84.67%\n",
      "50\tValidation loss: 0.723139\tBest loss: 0.594731\tAccuracy: 84.00%\n",
      "51\tValidation loss: 0.745109\tBest loss: 0.594731\tAccuracy: 84.67%\n",
      "52\tValidation loss: 0.803908\tBest loss: 0.594731\tAccuracy: 82.67%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_four_frozen\n",
      "Final test accuracy: 80.17%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./moj_najlepszy_model_mnist_0_do_4\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = four_frozen_saver.save(sess, \"./moj_model_mnist_5_do_9_cztery_zamrozone\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Wczesne zatrzymywanie!\")\n",
    "                break\n",
    "        print(\"{}\\tF. straty dla zestawu walidacyjnego: {:.6f}\\tNajlepsza wartość straty: {:.6f}\\tDokładność: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    four_frozen_saver.restore(sess, \"./moj_model_mnist_5_do_9_cztery_zamrozone\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Ostateczna dokładność dla zestawu testowego: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ciągle nieidealnie, ale już znacznie lepiej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ćwiczenie: Odmroź teraz dwie górne warstwy ukryte i kontynuuj proces uczenia: czy w ten sposób jeszcze bardziej wzrasta wydajność modelu?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "unfrozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"ukryta[34]|nowe_logity\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam3\")\n",
    "training_op = optimizer.minimize(loss, var_list=unfrozen_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "two_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_four_frozen\n",
      "0\tValidation loss: 0.880485\tBest loss: 0.880485\tAccuracy: 86.00%\n",
      "1\tValidation loss: 1.388974\tBest loss: 0.880485\tAccuracy: 81.33%\n",
      "2\tValidation loss: 0.741543\tBest loss: 0.741543\tAccuracy: 86.67%\n",
      "3\tValidation loss: 1.030772\tBest loss: 0.741543\tAccuracy: 84.00%\n",
      "4\tValidation loss: 0.699438\tBest loss: 0.699438\tAccuracy: 87.33%\n",
      "5\tValidation loss: 0.743930\tBest loss: 0.699438\tAccuracy: 89.33%\n",
      "6\tValidation loss: 1.711346\tBest loss: 0.699438\tAccuracy: 82.67%\n",
      "7\tValidation loss: 1.437762\tBest loss: 0.699438\tAccuracy: 82.00%\n",
      "8\tValidation loss: 0.829231\tBest loss: 0.699438\tAccuracy: 86.67%\n",
      "9\tValidation loss: 1.033920\tBest loss: 0.699438\tAccuracy: 86.67%\n",
      "10\tValidation loss: 1.055709\tBest loss: 0.699438\tAccuracy: 87.33%\n",
      "11\tValidation loss: 0.971796\tBest loss: 0.699438\tAccuracy: 88.00%\n",
      "12\tValidation loss: 0.801815\tBest loss: 0.699438\tAccuracy: 86.00%\n",
      "13\tValidation loss: 0.726146\tBest loss: 0.699438\tAccuracy: 89.33%\n",
      "14\tValidation loss: 0.757217\tBest loss: 0.699438\tAccuracy: 88.67%\n",
      "15\tValidation loss: 0.791842\tBest loss: 0.699438\tAccuracy: 90.00%\n",
      "16\tValidation loss: 0.732507\tBest loss: 0.699438\tAccuracy: 90.67%\n",
      "17\tValidation loss: 0.737297\tBest loss: 0.699438\tAccuracy: 90.67%\n",
      "18\tValidation loss: 0.746715\tBest loss: 0.699438\tAccuracy: 90.00%\n",
      "19\tValidation loss: 0.747751\tBest loss: 0.699438\tAccuracy: 90.00%\n",
      "20\tValidation loss: 0.749325\tBest loss: 0.699438\tAccuracy: 90.00%\n",
      "21\tValidation loss: 0.751899\tBest loss: 0.699438\tAccuracy: 90.00%\n",
      "22\tValidation loss: 0.754314\tBest loss: 0.699438\tAccuracy: 90.00%\n",
      "23\tValidation loss: 0.757840\tBest loss: 0.699438\tAccuracy: 90.00%\n",
      "24\tValidation loss: 0.761543\tBest loss: 0.699438\tAccuracy: 90.00%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_two_frozen\n",
      "Final test accuracy: 84.37%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    four_frozen_saver.restore(sess, \"./moj_model_mnist_5_do_9_cztery_zamrozone\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = two_frozen_saver.save(sess, \"./moj_model_mnist_5_do_9_dwie_zamrozone\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Wczesne zatrzymywanie!\")\n",
    "                break\n",
    "        print(\"{}\\tF. straty dla zestawu walidacyjnego: {:.6f}\\tNajlepsza wartość straty: {:.6f}\\tDokładność: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    two_frozen_saver.restore(sess, \"./moj_model_mnist_5_do_9_dwie_zamrozone\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Ostateczna dokładność dla zestawu testowego: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy, jaką uzyskamy dokładność po odmrożeniu wszystkich warstw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam4\")\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "no_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_two_frozen\n",
      "0\tValidation loss: 0.846005\tBest loss: 0.846005\tAccuracy: 83.33%\n",
      "1\tValidation loss: 0.694439\tBest loss: 0.694439\tAccuracy: 91.33%\n",
      "2\tValidation loss: 1.201433\tBest loss: 0.694439\tAccuracy: 85.33%\n",
      "3\tValidation loss: 1.975297\tBest loss: 0.694439\tAccuracy: 85.33%\n",
      "4\tValidation loss: 0.692805\tBest loss: 0.692805\tAccuracy: 95.33%\n",
      "5\tValidation loss: 1.090217\tBest loss: 0.692805\tAccuracy: 91.33%\n",
      "6\tValidation loss: 1.924300\tBest loss: 0.692805\tAccuracy: 90.67%\n",
      "7\tValidation loss: 4.019310\tBest loss: 0.692805\tAccuracy: 87.33%\n",
      "8\tValidation loss: 4.150792\tBest loss: 0.692805\tAccuracy: 78.00%\n",
      "9\tValidation loss: 4.522708\tBest loss: 0.692805\tAccuracy: 75.33%\n",
      "10\tValidation loss: 1.163385\tBest loss: 0.692805\tAccuracy: 90.00%\n",
      "11\tValidation loss: 0.655868\tBest loss: 0.655868\tAccuracy: 92.67%\n",
      "12\tValidation loss: 0.943888\tBest loss: 0.655868\tAccuracy: 92.67%\n",
      "13\tValidation loss: 0.529996\tBest loss: 0.529996\tAccuracy: 92.67%\n",
      "14\tValidation loss: 0.610578\tBest loss: 0.529996\tAccuracy: 94.67%\n",
      "15\tValidation loss: 3.899716\tBest loss: 0.529996\tAccuracy: 88.00%\n",
      "16\tValidation loss: 18.285717\tBest loss: 0.529996\tAccuracy: 86.67%\n",
      "17\tValidation loss: 23.169626\tBest loss: 0.529996\tAccuracy: 78.00%\n",
      "18\tValidation loss: 17.309252\tBest loss: 0.529996\tAccuracy: 90.00%\n",
      "19\tValidation loss: 44.261902\tBest loss: 0.529996\tAccuracy: 80.00%\n",
      "20\tValidation loss: 52.460327\tBest loss: 0.529996\tAccuracy: 80.00%\n",
      "21\tValidation loss: 26.318949\tBest loss: 0.529996\tAccuracy: 83.33%\n",
      "22\tValidation loss: 32.857723\tBest loss: 0.529996\tAccuracy: 90.67%\n",
      "23\tValidation loss: 53.359497\tBest loss: 0.529996\tAccuracy: 88.00%\n",
      "24\tValidation loss: 57.823742\tBest loss: 0.529996\tAccuracy: 88.00%\n",
      "25\tValidation loss: 37.154972\tBest loss: 0.529996\tAccuracy: 92.67%\n",
      "26\tValidation loss: 41.386772\tBest loss: 0.529996\tAccuracy: 90.00%\n",
      "27\tValidation loss: 43.486767\tBest loss: 0.529996\tAccuracy: 90.00%\n",
      "28\tValidation loss: 42.776855\tBest loss: 0.529996\tAccuracy: 88.67%\n",
      "29\tValidation loss: 43.368839\tBest loss: 0.529996\tAccuracy: 90.67%\n",
      "30\tValidation loss: 43.440975\tBest loss: 0.529996\tAccuracy: 90.00%\n",
      "31\tValidation loss: 42.889927\tBest loss: 0.529996\tAccuracy: 91.33%\n",
      "32\tValidation loss: 42.806690\tBest loss: 0.529996\tAccuracy: 90.67%\n",
      "33\tValidation loss: 42.784145\tBest loss: 0.529996\tAccuracy: 90.67%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_no_frozen\n",
      "Final test accuracy: 90.60%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    two_frozen_saver.restore(sess, \"./moj_model_mnist_5_do_9_dwie_zamrozone\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = no_frozen_saver.save(sess, \"./moj_model_mnist_5_do_9_brak_zamrozonych\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Wczesne zatrzymywanie!\")\n",
    "                break\n",
    "        print(\"{}\\tF. straty dla zestawu walidacyjnego: {:.6f}\\tNajlepsza wartość straty: {:.6f}\\tDokładność: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    no_frozen_saver.restore(sess, \"./moj_model_mnist_5_do_9_brak_zamrozonych\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Ostateczna dokładność dla zestawu testowego: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównajmy wynik z wytrenowaną od podstaw siecią GSN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.803557\tBest loss: 0.803557\tAccuracy: 71.33%\n",
      "1\tValidation loss: 0.966741\tBest loss: 0.803557\tAccuracy: 85.33%\n",
      "2\tValidation loss: 1.158972\tBest loss: 0.803557\tAccuracy: 78.00%\n",
      "3\tValidation loss: 0.615960\tBest loss: 0.615960\tAccuracy: 88.00%\n",
      "4\tValidation loss: 0.612626\tBest loss: 0.612626\tAccuracy: 92.00%\n",
      "5\tValidation loss: 0.686420\tBest loss: 0.612626\tAccuracy: 89.33%\n",
      "6\tValidation loss: 0.805281\tBest loss: 0.612626\tAccuracy: 89.33%\n",
      "7\tValidation loss: 0.753108\tBest loss: 0.612626\tAccuracy: 88.67%\n",
      "8\tValidation loss: 1.051471\tBest loss: 0.612626\tAccuracy: 86.00%\n",
      "9\tValidation loss: 0.487089\tBest loss: 0.487089\tAccuracy: 93.33%\n",
      "10\tValidation loss: 1.191093\tBest loss: 0.487089\tAccuracy: 85.33%\n",
      "11\tValidation loss: 0.878905\tBest loss: 0.487089\tAccuracy: 88.67%\n",
      "12\tValidation loss: 0.768841\tBest loss: 0.487089\tAccuracy: 91.33%\n",
      "13\tValidation loss: 1.153907\tBest loss: 0.487089\tAccuracy: 90.67%\n",
      "14\tValidation loss: 0.985427\tBest loss: 0.487089\tAccuracy: 89.33%\n",
      "15\tValidation loss: 1.221879\tBest loss: 0.487089\tAccuracy: 85.33%\n",
      "16\tValidation loss: 0.961743\tBest loss: 0.487089\tAccuracy: 88.67%\n",
      "17\tValidation loss: 3.116057\tBest loss: 0.487089\tAccuracy: 84.00%\n",
      "18\tValidation loss: 0.686387\tBest loss: 0.487089\tAccuracy: 84.00%\n",
      "19\tValidation loss: 0.929801\tBest loss: 0.487089\tAccuracy: 88.00%\n",
      "20\tValidation loss: 1.137579\tBest loss: 0.487089\tAccuracy: 92.00%\n",
      "21\tValidation loss: 0.987261\tBest loss: 0.487089\tAccuracy: 91.33%\n",
      "22\tValidation loss: 2.030677\tBest loss: 0.487089\tAccuracy: 91.33%\n",
      "23\tValidation loss: 1.094184\tBest loss: 0.487089\tAccuracy: 92.00%\n",
      "24\tValidation loss: 1.332256\tBest loss: 0.487089\tAccuracy: 82.67%\n",
      "25\tValidation loss: 1.128633\tBest loss: 0.487089\tAccuracy: 85.33%\n",
      "26\tValidation loss: 0.866569\tBest loss: 0.487089\tAccuracy: 90.67%\n",
      "27\tValidation loss: 1.088500\tBest loss: 0.487089\tAccuracy: 89.33%\n",
      "28\tValidation loss: 1.146113\tBest loss: 0.487089\tAccuracy: 89.33%\n",
      "29\tValidation loss: 1.163180\tBest loss: 0.487089\tAccuracy: 89.33%\n",
      "30\tValidation loss: 1.154797\tBest loss: 0.487089\tAccuracy: 89.33%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function elu at 0x7fd9e8a620d0>,\n",
       "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x7fd9d5e628c8>,\n",
       "       learning_rate=0.01, n_hidden_layers=4, n_neurons=100,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_5_to_9 = DNNClassifier(n_hidden_layers=4, random_state=42)\n",
    "dnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=1000, X_valid=X_valid2, y_valid=y_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90413495165603786"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_5_to_9.predict(X_test2)\n",
    "accuracy_score(y_test2, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buu. Cóż za rozczarowanie! ;) Uczenie transferowe niewiele pomogło (jeśli w ogóle) w tym zadaniu. Przynajmniej spróbowaliśmy... Na szczęście kolejne ćwiczenie przyniesie lepsze rezultaty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\tUczenie wstępne za pomocą dodatkowego zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym ćwiczeniu stworzymy sieć GSN porównującą dwa obrazy cyfr z zestawu MNIST i prognozującą, czy reprezentują one tę samą cyfrę lub nie. Następnie ponownie wykorzystamy nższe warstwy tej sieci do wytrenowania klasyfikatora obrazów MNIST przy użyciu bardziej niewielkiej ilości danych uczących."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.\n",
    "Ćwiczenie: _Rozpocznij od stworzenia dwóch sieci GSN (nazwijmy je sieciami A i B); obydwie będą podobne do stworzonej w poprzednich ćwiczeniach, ale pozbawione warstwy wyjściowej: każda z sieci powinna mieć 5 warstw ukrytych, po 100 neuronów w każdej z nich, do tego inicjację He'ego i funkcję aktywacji ELU. Teraz na samej górze obydwu sieci neuronowych dodaj jeszcze jedną warstwę ukrytą zawierającą 10 jednostek. W tym celu należy użyć funkcji `concat()` z parametrem `axis=1` w celu połączenia wyników obydwu sieci dla każdej próbki, a następnie przesłać wynik do warstwy ukrytej. Na koniec dodaj jednoneuronową warstwę wyjściową wykorzystującą logistyczną funkcję aktywacji._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ostrzeżenie**! We wcześniejszych wersjach książka zawierała błąd w opisie ćwiczenia: brakowało instrukcji dodania górnej warstwy ukrytej. Bez spełnienia tego warunku sieć neuronowa zazwyczaj nie jest w stanie rozpocząć procesu uczenia. Właściciele najnowszego wydania książki mogą spać spokojnie, gdyż błąd ten został usunięty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moglibyśmy mieć dwa wejściowe węzły zastępcze - `X1` i `X2` - jeden przeznaczony dla obrazów dostarczanych do pierwszej sieci GSN, a drugi zawierający próbki dostarczane do drugiej sieci neuronowej. Metoda ta działałaby prawidłowo. Możemy jednak również wykorzystać jeden wejściowy węzeł zastępczy przechowujący obydwa zbiory obrazów (każdy rząd zawierałby parę obrazów), a następnie używać funkcji `tf.unstack()` do rozdzielania tego tensora na dwa oddzielne tensory, np.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28 # zbiór MNIST\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 2, n_inputs), name=\"X\")\n",
    "X1, X2 = tf.unstack(X, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potrzebujemy również węzła zastępczego dla etykiet. Każda etykieta będzie miała wartość 0, jeśli obrazy będą przedstawiać różne cyfry, a 1, gdy będą reprezentować tę samą cyfrę:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozdzielmy teraz te dane pomiędzy dwie osobne sieci GSN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn1 = dnn(X1, name=\"GSN_A\")\n",
    "dnn2 = dnn(X2, name=\"GSN_B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I połączmy ich wyjścia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn_outputs = tf.concat([dnn1, dnn2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każda z tych sieci generuje na wyjściu 100 pobudzeń (na każdą próbkę), zatem mają one postać `[None, 100]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(100)])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(100)])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiste jest zatem, że połączone wyjścia mają postać `[None, 200]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(200)])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wstawmy teraz dodatkową warstwę ukrytą zawierającą jedynie 10 neuronów, a także jednoneuronową warstwę wyjściową:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden = tf.layers.dense(dnn_outputs, units=10, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "logits = tf.layers.dense(hidden, units=1, kernel_initializer=he_init)\n",
    "y_proba = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cała sieć przewiduje wartość `1`, jeżeli `y_proba >= 0.5` (tzn. sieć prognozuje, że obrazy reprezentują tę samą cyfrę), w przeciwnym wypadku `0`. My będziemy obliczać `logits >= 0`, co jest równoznacznym, ale znacznie szybszym rozwiązaniem: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodajmy teraz funkcję kosztu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_as_float = tf.cast(y, tf.float32)\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A teraz możemy stworzyć operację uczenia za pomocą optymalizatora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.95\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chcemy zmierzyć dokładność naszego klasyfikatora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_correct = tf.equal(y_pred, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tradycyjnie nie możemy zapomnieć o węzłach `init` `saver`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.\n",
    "_Ćwiczenie: Podziel zestaw danych MNIST na dwa podzbiory: pierwszy z nich powinien zawierać 55 000 obrazów, a drugi — 5 000. Stwórz funkcję generującą grupę uczącą, w której każdą próbkę będzie stanowiła para obrazów z pierwszego podzbiorów. Połowę próbek uczących powinny stanowić pary obrazów należących do tej samej klasy, natomiast w drugiej połowie niech będą wymieszane obrazy należące do różnych klas. Etykieta próbek stanowiących pary jednoklasowe powinna mieć wartość 0, a jeżeli pary są dwuklasowe — 1._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zestaw danych MNIST zwrócony przez funkcję `input_data()` jest już podzielony na trzy podzbiory: uczący (55 000 próbek), walidacyjny (5 000 próbek) i testowy (10 000 próbek). Wykorzystajmy pierwszy z wymienionych zestawów do wygenerowania zbioru uczącego składającego się z par obrazów, natomiast drugi zbiór przyda nam się w drugiej fazie ćwiczenia (uczeniu standardowego klasyfikatora MNIST). Trzeci zestaw posłuży nam w obydwu fazach jako zbiór testowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1 = mnist.train.images\n",
    "y_train1 = mnist.train.labels\n",
    "\n",
    "X_train2 = mnist.validation.images\n",
    "y_train2 = mnist.validation.labels\n",
    "\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napiszmy funkcję generującą pary obrazów: połowa par będzie przedstawiała tę samą cyfrę, a druga połowa zawierać będzie różne cyfry. Istnieje wiele sposobów implementacji tego mechanizmu. W naszym przykładzie najpierw decydujemy, ile \"takich samych\" (tj. przedstawiających tę samą cyfrę) i \"różnych\" (zawierających różne cyfry) zamierzamy wygenerować. Moglibyśmy skorzystać po prostu z działania `batch_size // 2`, chcemy jednak rozwiązać potencjalny problem elementów nieparzystych (po prawdzie, mógłby być zabójczy!). Następnie wygenerujemy losowe pary i dobierzemy odpowiednią liczbę \"takich samych\" par, po czym zrobimy to samo z \"różnymi\" parami. Na koniec przetasujemy grupę i zwrócimy ją:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(images, labels, batch_size):\n",
    "    size1 = batch_size // 2\n",
    "    size2 = batch_size - size1\n",
    "    if size1 != size2 and np.random.rand() > 0.5:\n",
    "        size1, size2 = size2, size1\n",
    "    X = []\n",
    "    y = []\n",
    "    while len(X) < size1:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if rnd_idx1 != rnd_idx2 and labels[rnd_idx1] == labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([1])\n",
    "    while len(X) < batch_size:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if labels[rnd_idx1] != labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([0])\n",
    "    rnd_indices = np.random.permutation(batch_size)\n",
    "    return np.array(X)[rnd_indices], np.array(y)[rnd_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy, czy powyższy kod działa i wygenerujemy mini-grupę składającą się z pięciu par obrazów: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każdy rząd w tensorze `X_batch` zawiera parę obrazów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 2, 784), dtype('float32'))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch.shape, X_batch.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się tym parom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANcAAAGiCAYAAAB05VNzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9d2dda2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3, 3 * batch_size))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_batch[:,0].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_batch[:,1].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A teraz sprawdźmy etykiety (0 oznacza \"różne\", a 1 - \"takie same\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Znakomicie!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.\n",
    "_Ćwiczenie: wytrenuj sieć wobec zbioru uczącego. Dla każdej pary obrazów możesz jednocześnie przesyłać po jednym do sieci A i B. Cała sieć będzie stopniowo uczyła się rozpoznawać, czy pary obrazów należą do tej samej klasy._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygenerujmy zbiór testowy składający się z wielu par obrazów pochodzących z zestawu testowego MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test1, y_test1 = generate_batch(X_test, y_test, batch_size=len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz zaś wytrenujmy model. Ten etap nie jest jakiś wyjątkowy oprócz faktu, że potrzebujemy całkiem dużej wartości parametru `batch_size`, gdyż w przeciwnym razie model nie nauczy się niczego i będzie uzyskiwać dokładność rzędu 50%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss: 0.492426\n",
      "0 Test accuracy: 0.7861\n",
      "1 Train loss: 0.334813\n",
      "2 Train loss: 0.290434\n",
      "3 Train loss: 0.253434\n",
      "4 Train loss: 0.217843\n",
      "5 Train loss: 0.17127\n",
      "5 Test accuracy: 0.9185\n",
      "6 Train loss: 0.207128\n",
      "7 Train loss: 0.172275\n",
      "8 Train loss: 0.166783\n",
      "9 Train loss: 0.161094\n",
      "10 Train loss: 0.125131\n",
      "10 Test accuracy: 0.9425\n",
      "11 Train loss: 0.159824\n",
      "12 Train loss: 0.124752\n",
      "13 Train loss: 0.112234\n",
      "14 Train loss: 0.114502\n",
      "15 Train loss: 0.0950093\n",
      "15 Test accuracy: 0.9532\n",
      "16 Train loss: 0.119296\n",
      "17 Train loss: 0.0754429\n",
      "18 Train loss: 0.112295\n",
      "19 Train loss: 0.133708\n",
      "20 Train loss: 0.113547\n",
      "20 Test accuracy: 0.9596\n",
      "21 Train loss: 0.0674082\n",
      "22 Train loss: 0.0936297\n",
      "23 Train loss: 0.0986469\n",
      "24 Train loss: 0.111875\n",
      "25 Train loss: 0.0735623\n",
      "25 Test accuracy: 0.9675\n",
      "26 Train loss: 0.0790324\n",
      "27 Train loss: 0.0487644\n",
      "28 Train loss: 0.0869071\n",
      "29 Train loss: 0.0694422\n",
      "30 Train loss: 0.060089\n",
      "30 Test accuracy: 0.9663\n",
      "31 Train loss: 0.103902\n",
      "32 Train loss: 0.0535952\n",
      "33 Train loss: 0.0310679\n",
      "34 Train loss: 0.0536294\n",
      "35 Train loss: 0.046265\n",
      "35 Test accuracy: 0.9701\n",
      "36 Train loss: 0.0679821\n",
      "37 Train loss: 0.0326656\n",
      "38 Train loss: 0.0357479\n",
      "39 Train loss: 0.0333373\n",
      "40 Train loss: 0.0415115\n",
      "40 Test accuracy: 0.9719\n",
      "41 Train loss: 0.0577977\n",
      "42 Train loss: 0.0342781\n",
      "43 Train loss: 0.0439651\n",
      "44 Train loss: 0.0597254\n",
      "45 Train loss: 0.0588695\n",
      "45 Test accuracy: 0.9721\n",
      "46 Train loss: 0.0556821\n",
      "47 Train loss: 0.063956\n",
      "48 Train loss: 0.0301285\n",
      "49 Train loss: 0.0402678\n",
      "50 Train loss: 0.0489125\n",
      "50 Test accuracy: 0.9751\n",
      "51 Train loss: 0.0394528\n",
      "52 Train loss: 0.0233041\n",
      "53 Train loss: 0.064878\n",
      "54 Train loss: 0.0510189\n",
      "55 Train loss: 0.0312619\n",
      "55 Test accuracy: 0.9742\n",
      "56 Train loss: 0.0244156\n",
      "57 Train loss: 0.0409082\n",
      "58 Train loss: 0.0346896\n",
      "59 Train loss: 0.0455727\n",
      "60 Train loss: 0.0488268\n",
      "60 Test accuracy: 0.9751\n",
      "61 Train loss: 0.0154253\n",
      "62 Train loss: 0.0358874\n",
      "63 Train loss: 0.0290555\n",
      "64 Train loss: 0.0172143\n",
      "65 Train loss: 0.0377991\n",
      "65 Test accuracy: 0.9751\n",
      "66 Train loss: 0.0360786\n",
      "67 Train loss: 0.0240278\n",
      "68 Train loss: 0.0314243\n",
      "69 Train loss: 0.0412082\n",
      "70 Train loss: 0.0439106\n",
      "70 Test accuracy: 0.9763\n",
      "71 Train loss: 0.0169656\n",
      "72 Train loss: 0.0181306\n",
      "73 Train loss: 0.0214228\n",
      "74 Train loss: 0.0418301\n",
      "75 Train loss: 0.0378622\n",
      "75 Test accuracy: 0.9759\n",
      "76 Train loss: 0.0199817\n",
      "77 Train loss: 0.0145837\n",
      "78 Train loss: 0.0199176\n",
      "79 Train loss: 0.0226598\n",
      "80 Train loss: 0.0119815\n",
      "80 Test accuracy: 0.9779\n",
      "81 Train loss: 0.0177832\n",
      "82 Train loss: 0.00981572\n",
      "83 Train loss: 0.0279094\n",
      "84 Train loss: 0.0237818\n",
      "85 Train loss: 0.0157778\n",
      "85 Test accuracy: 0.978\n",
      "86 Train loss: 0.00950592\n",
      "87 Train loss: 0.0226222\n",
      "88 Train loss: 0.0226599\n",
      "89 Train loss: 0.0185005\n",
      "90 Train loss: 0.0118967\n",
      "90 Test accuracy: 0.976\n",
      "91 Train loss: 0.0209059\n",
      "92 Train loss: 0.0181153\n",
      "93 Train loss: 0.0131697\n",
      "94 Train loss: 0.017605\n",
      "95 Train loss: 0.0193861\n",
      "95 Test accuracy: 0.976\n",
      "96 Train loss: 0.0156532\n",
      "97 Train loss: 0.0136041\n",
      "98 Train loss: 0.00743028\n",
      "99 Train loss: 0.0267189\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "            loss_val, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, \"Funkcja straty dla zbioru testowego:\", loss_val)\n",
    "        if epoch % 5 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "            print(epoch, \"Dokładność dla zbioru testowego:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_model_porownywanie_cyfr.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W porządku, uzyskujemy dokładność rzędu 97,6% w tym zadaniu porównywania cyfr. Całkiem nieźle, model ten wie co nieco na temat porównywania odręcznie pisanych cyfr!\n",
    "\n",
    "Sprawdźmy, czy ta wiedza może okazać się przydatna w standardowym zadaniu klasyfikacyjnym zestawu MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.\n",
    "_Ćwiczenie: stwórz teraz nową sieć GSN ponownie wykorzystując i zamrażając warstwy ukryte sieci A, a na szczycie wprowadzając dziesięcioneuronową warstwę wyjściową wykorzystującą funkcję softmax. Wyucz tę sieć wobec drugiego podzbioru danych i sprawdź, czy model jest w stanie osiągnąć wysoką wydajność pomimo faktu, że na każdą klasę przypada tylko 500 obrazów._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy model; nie powinno być to zadanie zbyt skomplikowane. Istnieje wiele metod zamrażania niższych warstw, co zostało opisane w książce. W tym przykładzie wybraliśmy funkcję `tf.stop_gradient()`. Zwróć uwagę, że potrzebujemy jednego obiektu `Saver` do odtwarzania gotowej sieci GSN A, a drugiego do zapisania ostatecznego modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # Zbiór MNIST\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X, name=\"GSN_A\")\n",
    "frozen_outputs = tf.stop_gradient(dnn_outputs)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init)\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GSN_A\")\n",
    "restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas na uczenie! Najpierw inicjujemy wszystkie zmienne (również stanowiące część nowej warstwy wyjściowej), po czym odtwarzamy gotową sieć GSN A. Następnie pozostaje nam wytrenować model na małym zbiorze danych MNIS (zawierającym zaledwie 5 000 obrazów): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_digit_comparison_model.ckpt\n",
      "0 Test accuracy: 0.9269\n",
      "10 Test accuracy: 0.9675\n",
      "20 Test accuracy: 0.9673\n",
      "30 Test accuracy: 0.9673\n",
      "40 Test accuracy: 0.9674\n",
      "50 Test accuracy: 0.9673\n",
      "60 Test accuracy: 0.9673\n",
      "70 Test accuracy: 0.9673\n",
      "80 Test accuracy: 0.9672\n",
      "90 Test accuracy: 0.9673\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./moj_model_porownywanie_cyfr.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Dokładność dla zbioru testowego:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_ostateczny_model_mnist.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokładność rzędu 96,7% to nie jest najlepszy wynik, jaki udało nam się do tej pory uzyskać dla zbioru MNIST, pamiętaj jednak, że korzystamy z niewielkiej liczby próbek uczących (zaledwie 500 obrazów na każdą cyfrę). Porównajmy ten rezultat z głęboką siecią neuronową wyuczoną od podstaw, bez korzytania z uczenia transferowego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # zbiór MNIST\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X, name=\"GSN_A\")\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init)\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"strata\")\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GSN_A\")\n",
    "restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8893\n",
      "10 Test accuracy: 0.9402\n",
      "20 Test accuracy: 0.9479\n",
      "30 Test accuracy: 0.9474\n",
      "40 Test accuracy: 0.9479\n",
      "50 Test accuracy: 0.9475\n",
      "60 Test accuracy: 0.9475\n",
      "70 Test accuracy: 0.9475\n",
      "80 Test accuracy: 0.9476\n",
      "90 Test accuracy: 0.9476\n",
      "100 Test accuracy: 0.9473\n",
      "110 Test accuracy: 0.9472\n",
      "120 Test accuracy: 0.9474\n",
      "130 Test accuracy: 0.9474\n",
      "140 Test accuracy: 0.9475\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 150\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Dokładność dla zbioru testowego:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./moj_ostateczny_model_mnist.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaledwie 94,8% dokładności... Zatem uczenie transferowe pomogło nam zredukować stopę błędu z 5,2% do 3,3% (to zmniejszenie liczby pojawiających się błędów o ponad 36%). Do tego model wykorzystujący uczenie transferowe uzyskał dokładność rzędu 96% w ciągu dziesięciu epok.\n",
    "\n",
    "Podsumowując, uczenie transferowe nie zawsze okazuje się skuteczne (przekonaliśmy się o tym w rozdziale 9.), ale z drugiej strony w odpowiednich sytuacjach potrafi zrobić dużą różnicę. Warto więc ją wypróbować!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
